{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda activate torch-xla-nightly\n",
    "#export XRT_TPU_CONFIG=\"tpu_worker;0;$10.0.101.2:8470\"\n",
    "#git init\n",
    "#git remote add origin https://github.com/nosound2/RSNA-Hemorrhage\n",
    "#git pull origin master\n",
    "#git config remote.origin.push HEAD\n",
    "#gcloud config set compute/zone europe-west4-a\n",
    "#gcloud auth login\n",
    "#gcloud config set project endless-empire-239015\n",
    "#pip install kaggle\n",
    "#mkdir .kaggle\n",
    "#gsutil cp gs://recursion-double-strand/kaggle-keys/kaggle.json ~/.kaggle\n",
    "#chmod 600 /home/zahar_chikishev/.kaggle/kaggle.json\n",
    "#kaggle competitions download rsna-intracranial-hemorrhage-detection -f stage_1_train.csv\n",
    "#sudo apt install unzip\n",
    "#unzip stage_1_train.csv.zip\n",
    "#kaggle kernels output xhlulu/rsna-generate-metadata-csvs -p .\n",
    "#gsutil cp gs://rsna-hemorrhage/yuvals/* .\n",
    "\n",
    "#export XRT_TPU_CONFIG=\"tpu_worker;0;10.0.101.2:8470\"; conda activate torch-xla-nightly; jupyter notebook\n",
    "\n",
    "# 35.204.242.164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 22\n",
    "CLOUD_SINGLE = False\n",
    "MIXUP = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "from matplotlib import patches, patheffects\n",
    "import time\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error,log_loss\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "import pdb\n",
    "\n",
    "import scipy as sp\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "\n",
    "CLOUD = not torch.cuda.is_available()\n",
    "\n",
    "if not CLOUD:\n",
    "    torch.cuda.current_device()\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as D\n",
    "import torch.nn.functional as F\n",
    "import torch.utils as U\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "from torchvision import models as M\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if CLOUD:\n",
    "    PATH = Path('/home/zahar_chikishev')\n",
    "    PATH_WORK = Path('/home/zahar_chikishev/running')\n",
    "else:\n",
    "    PATH = Path('C:/StudioProjects/Hemorrhage')\n",
    "    PATH_WORK = Path('C:/StudioProjects/Hemorrhage/running')\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import seaborn as sn\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "\n",
    "all_ich = ['any','epidural','intraparenchymal','intraventricular','subarachnoid','subdural']\n",
    "class_weights = 6.0*np.array([2,1,1,1,1,1])/7.0\n",
    "\n",
    "if CLOUD:\n",
    "    import torch_xla\n",
    "    import torch_xla.distributed.data_parallel as dp\n",
    "    import torch_xla.utils as xu\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    \n",
    "    from typing import Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLOUD:\n",
    "    device = xm.xla_device()\n",
    "    #device = 'cpu'\n",
    "    MAX_DEVICES = 1 if CLOUD_SINGLE else 8\n",
    "    bs = 100\n",
    "else:\n",
    "    device = 'cuda'\n",
    "    #device = 'cpu'\n",
    "    MAX_DEVICES = 1\n",
    "    bs = 10\n",
    "\n",
    "if CLOUD and (not CLOUD_SINGLE):\n",
    "    devices = xm.get_xla_supported_devices(max_devices=MAX_DEVICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2351\n",
    "\n",
    "def setSeeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "setSeeds(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_cat, cols_float = pickle.load(open(PATH_WORK/'covs','rb'))\n",
    "meta_cols = cols_cat + cols_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    filename = PATH_WORK/'indexes_file.pkl'\n",
    "    all_idx, train_ids, val_ids = pickle.load(open(filename,'rb'))\n",
    "\n",
    "    train_md = pd.read_csv(PATH_WORK/'train_md.csv').sort_values(['SeriesInstanceUID','pos_idx'])\n",
    "    train_md['img_id'] = train_md.SOPInstanceUID.str.split('_').apply(lambda x: x[1])\n",
    "\n",
    "    trn_data = train_md.loc[train_md.img_id.isin(all_idx[train_ids])].reset_index(drop=True)\n",
    "    val_data = train_md.loc[train_md.img_id.isin(all_idx[val_ids])].reset_index(drop=True)\n",
    "\n",
    "    assert len(trn_data.SeriesInstanceUID.unique()) + len(val_data.SeriesInstanceUID.unique()) \\\n",
    "        == len(train_md.SeriesInstanceUID.unique())\n",
    "\n",
    "    assert len(trn_data.PatientID.unique()) + len(val_data.PatientID.unique()) \\\n",
    "        >= len(train_md.PatientID.unique())\n",
    "\n",
    "    ids_df = pd.DataFrame(all_idx, columns = ['img_id'])\n",
    "    ids_df = ids_df.join(train_md[['img_id','SeriesInstanceUID','pos_idx']].set_index('img_id'), on = 'img_id')\n",
    "\n",
    "    assert len(ids_df.SeriesInstanceUID.unique()) == 19530\n",
    "    \n",
    "    pickle.dump((trn_data,val_data,ids_df), open(PATH_WORK/'train.post.processed.1','wb'))\n",
    "else:\n",
    "    trn_data,val_data,ids_df = pickle.load(open(PATH_WORK/'train.post.processed.1','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    test_md = pd.read_csv(PATH_WORK/'test_md.csv').sort_values(['SeriesInstanceUID','pos_idx'])\n",
    "    test_md['img_id'] = test_md.SOPInstanceUID.str.split('_').apply(lambda x: x[1])\n",
    "\n",
    "    filename = PATH_WORK/'test_indexes.pkl'\n",
    "    test_ids = pickle.load(open(filename,'rb'))\n",
    "\n",
    "    test_ids_df = pd.DataFrame(test_ids, columns = ['img_id'])\n",
    "    test_ids_df = test_ids_df.join(test_md[['img_id','SeriesInstanceUID','pos_idx']].set_index('img_id'), on = 'img_id')\n",
    "\n",
    "    assert len(test_ids_df.SeriesInstanceUID.unique()) == 2214\n",
    "    \n",
    "    pickle.dump((test_md,test_ids_df), open(PATH_WORK/'test.post.processed.1','wb'))\n",
    "else:\n",
    "    test_md,test_ids_df = pickle.load(open(PATH_WORK/'test.post.processed.1','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    print(pd.concat([test_md[meta_cols].mean(0),\n",
    "                     trn_data[meta_cols].mean(0),\n",
    "                     val_data[meta_cols].mean(0)], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    filename = PATH_WORK/'model_Densenet161_3_vehrsion_basic_classifier_type_features_train_split_2.pkl'\n",
    "    feats = pickle.load(open(filename,'rb'))\n",
    "\n",
    "    for series_id in tqdm(ids_df.SeriesInstanceUID.unique()):\n",
    "        mask = torch.BoolTensor(ids_df.SeriesInstanceUID.values == series_id)\n",
    "        feats_id = feats[mask]\n",
    "        pickle.dump(feats_id, open(PATH_WORK/'features/densenet161_v3/train/{}'.format(series_id),'wb'))\n",
    "\n",
    "\n",
    "    filename = PATH_WORK/'model_Densenet161_3_vehrsion_basic_classifier_type_features_test_split_2.pkl'\n",
    "    feats = pickle.load(open(filename,'rb'))\n",
    "\n",
    "    for series_id in tqdm(test_ids_df.SeriesInstanceUID.unique()):\n",
    "        mask = torch.BoolTensor(test_ids_df.SeriesInstanceUID.values == series_id)\n",
    "        feats_id = feats[mask]\n",
    "        pickle.dump(feats_id, open(PATH_WORK/'features/densenet161_v3/test/{}'.format(series_id),'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = PATH_WORK/'features/densenet161_v3/train/ID_000a935543'\n",
    "#feats1 = pickle.load(open(path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_black = '006d4432e'\n",
    "\n",
    "path = PATH_WORK/'features/densenet161_v3/train/ID_992b567eb6'\n",
    "black_feats = pickle.load(open(path,'rb'))[41]\n",
    "\n",
    "#black_feats = torch.zeros(black_feats.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSNA_DataSet(D.Dataset):\n",
    "    def __init__(self, metadata, ids_df, mode='train', bs=None):\n",
    "        \n",
    "        super(RSNA_DataSet, self).__init__()\n",
    "        \n",
    "        md = metadata.copy()\n",
    "        md = md.reset_index(drop=True)\n",
    "        series = md.SeriesInstanceUID.unique()\n",
    "        \n",
    "        samples_add = 0\n",
    "        if (mode != 'train') and not DATA_SMALL:\n",
    "            batch_num = -((-len(series))//(bs*MAX_DEVICES))\n",
    "            samples_add = batch_num*bs*MAX_DEVICES - len(series)\n",
    "            print('adding dummy serieses', samples_add)\n",
    "        \n",
    "        #self.records = df.to_records(index=False)\n",
    "        self.mode = mode\n",
    "        self.real = np.concatenate([np.repeat(True,len(series)),np.repeat(False,samples_add)])\n",
    "        self.series = np.concatenate([series, random.sample(list(series),samples_add)])\n",
    "        self.metadata = md\n",
    "        self.ids_df = ids_df\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        series_id = self.series[index]\n",
    "        df = self.metadata.loc[self.metadata.SeriesInstanceUID == series_id].reset_index(drop=True)\n",
    "        \n",
    "        folder = 'test' if self.mode == 'test' else 'train'\n",
    "        path = PATH_WORK/'features/densenet161_v3/{}/{}'.format(folder,series_id)\n",
    "        feats = pickle.load(open(path,'rb'))\n",
    "        ids_df_sub = self.ids_df.loc[self.ids_df.SeriesInstanceUID.values == series_id]\n",
    "        \n",
    "        if feats.shape[0] > len(df):\n",
    "            mask_dup = ~ids_df_sub.img_id.duplicated().values\n",
    "            ids_df_sub = ids_df_sub.loc[mask_dup]\n",
    "            feats = feats[torch.BoolTensor(mask_dup)]\n",
    "        \n",
    "        assert feats.shape[0] == len(df)\n",
    "        assert len(ids_df_sub) == len(df)\n",
    "        assert np.all(ids_df_sub.img_id.isin(df.img_id).values)\n",
    "        order = np.argsort(ids_df_sub.pos_idx.values)\n",
    "        assert np.all(ids_df_sub.img_id.values[order] == df.img_id.values)\n",
    "        feats = feats[torch.LongTensor(order)]\n",
    "        \n",
    "        feats = torch.cat([feats, torch.Tensor(df[meta_cols].values)], dim=1)\n",
    "        target = torch.Tensor(df[all_ich].values)\n",
    "        \n",
    "        PAD = 4+9\n",
    "        \n",
    "        offset = np.random.randint(0, 61 - feats.shape[0])\n",
    "        #offset = 0\n",
    "        top_pad = PAD + offset\n",
    "        if top_pad > 0:\n",
    "            dummy_row = torch.cat([black_feats, torch.Tensor(df.head(1)[meta_cols].values).squeeze()])\n",
    "            #dummy_row = torch.cat([black_feats, torch.zeros(len(meta_cols))])\n",
    "            feats = torch.cat([dummy_row.repeat(top_pad,1), feats], dim=0)\n",
    "            if offset > 0:\n",
    "                target = torch.cat([torch.zeros((offset, len(all_ich))), target], dim=0)\n",
    "        bot_pad = 60 - len(df) - offset + PAD\n",
    "        if bot_pad > 0:\n",
    "            dummy_row = torch.cat([black_feats, torch.Tensor(df.tail(1)[meta_cols].values).squeeze()])\n",
    "            #dummy_row = torch.cat([black_feats, torch.zeros(len(meta_cols))])\n",
    "            feats = torch.cat([feats, dummy_row.repeat(bot_pad,1)], dim=0)\n",
    "            if (60 - len(df) - offset) > 0:\n",
    "                target = torch.cat([target, torch.zeros((60 - len(df) - offset, len(all_ich)))], dim=0)\n",
    "        \n",
    "        assert feats.shape[0] == (60 + 2*PAD)\n",
    "        assert target.shape[0] == 60\n",
    "        \n",
    "        feats = feats.transpose(1,0)\n",
    "        \n",
    "        idx = index\n",
    "        if not self.real[index]: idx = -1\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            return feats, target\n",
    "        else:\n",
    "            return feats, target, idx, offset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.series) if not DATA_SMALL else int(0.01*len(self.series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCurrentBatch(fold=0):\n",
    "    sel_batch = None\n",
    "    for filename in os.listdir(PATH_WORK/'models'):\n",
    "        splits = filename.split('.')\n",
    "        if int(splits[2][1]) != fold: continue\n",
    "        if int(splits[3][1:]) != VERSION: continue\n",
    "        if sel_batch is None:\n",
    "            sel_batch = int(splits[1][1:])\n",
    "        else:\n",
    "            sel_batch = max(sel_batch, int(splits[1][1:]))\n",
    "    return sel_batch\n",
    "\n",
    "def modelFileName(fold=0, batch = 1, return_last = False, return_next = False):\n",
    "    sel_batch = batch\n",
    "    if return_last or return_next:\n",
    "        sel_batch = getCurrentBatch(fold)\n",
    "        if return_last and sel_batch is None:\n",
    "            return None\n",
    "        if return_next:\n",
    "            if sel_batch is None: sel_batch = 1\n",
    "            else: sel_batch += 1\n",
    "    \n",
    "    return 'model.b{}.f{}.v{}'.format(sel_batch, fold, VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCEWithLogitsLoss(nn.Module):\n",
    "    def __init__(self, weight=None):\n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "    \n",
    "    def forward(self, input, target, batch_weights = None):\n",
    "        loss = (torch.log(1+torch.exp(input)) - target*input)*self.weight\n",
    "        if batch_weights is not None:\n",
    "            loss = batch_weights*loss\n",
    "        return loss.mean()\n",
    "        \n",
    "        #return F.binary_cross_entropy_with_logits(input.squeeze(), target,\n",
    "        #                                          self.weight,\n",
    "        #                                          pos_weight=self.pos_weight,\n",
    "        #                                          reduction=self.reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatProduct(nn.Module):\n",
    "    def __init__(self, in_feature, out_feature):\n",
    "        super(FeatProduct, self).__init__()\n",
    "        self.in_feature = in_feature\n",
    "        self.out_feature = out_feature\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_feature, in_feature))\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_feature))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        nn.init.uniform_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = F.linear(x, self.weight) + self.bias\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_drop_lin(n_in:int, n_out:int, bn:bool=True, p:float=0., actn=None):\n",
    "    \"Sequence of batchnorm (if `bn`), dropout (with `p`) and linear (`n_in`,`n_out`) layers followed by `actn`.\"\n",
    "    layers = [nn.BatchNorm1d(n_in)] if bn else []\n",
    "    if p != 0: layers.append(nn.Dropout(p))\n",
    "    layers.append(nn.Linear(n_in, n_out))\n",
    "    if actn is not None: layers.append(actn)\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularModel(nn.Module):\n",
    "    \"Basic model for tabular data.\"\n",
    "    def __init__(self, n_cont:int, out_sz:int, layers, ps=None,\n",
    "                 emb_drop:float=0., use_bn:bool=True, bn_final:bool=False, feat_sz=2208, fc_drop_p=0.3):\n",
    "        super().__init__()\n",
    "        self.bn_cont = nn.BatchNorm1d(feat_sz + n_cont)\n",
    "        self.n_cont = n_cont\n",
    "        sizes = self.get_sizes(layers, out_sz)\n",
    "        actns = [nn.ReLU(inplace=True) for _ in range(len(sizes)-2)] + [None]\n",
    "        layers = []\n",
    "        for i,(n_in,n_out,dp,act) in enumerate(zip(sizes[:-1],sizes[1:],[0.]+ps,actns)):\n",
    "            layers += bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act)\n",
    "        if bn_final: layers.append(nn.BatchNorm1d(sizes[-1]))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.feat_product = FeatProduct(feat_sz + n_cont, 20)\n",
    "        self.fc_drop = nn.Dropout(p=fc_drop_p)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2D_1 = nn.Conv2d(1,32,(feat_sz + n_cont,1))\n",
    "        self.conv2D_2 = nn.Conv2d(1,32,(feat_sz + n_cont,5))#,padding=(0,2)\n",
    "        self.bn_cont1 = nn.BatchNorm1d(64)\n",
    "        self.conv1D_1 = nn.Conv1d(64,32,3)#,padding=1\n",
    "        self.conv1D_3 = nn.Conv1d(64,32,5,dilation=5)\n",
    "        self.conv1D_2 = nn.Conv1d(64,6,3)#,padding=1\n",
    "        self.bn_cont2 = nn.BatchNorm1d(64)\n",
    "        self.bn_cont3 = nn.BatchNorm1d(6)\n",
    "\n",
    "    def get_sizes(self, layers, out_sz):\n",
    "        return [1200] + layers + [out_sz]\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        x = self.bn_cont(x) # bs,2208,60\n",
    "        x = self.fc_drop(x)\n",
    "        x = x.reshape(x.shape[0],1,x.shape[1],x.shape[2]) # bs,1,2208,60\n",
    "        x = torch.cat([self.conv2D_1(x[:,:,:,2:(-2)]).squeeze(), \n",
    "                       self.conv2D_2(x).squeeze()], dim=1) # bs,64,60\n",
    "        x = self.relu(x)\n",
    "        x = self.bn_cont1(x)\n",
    "        x = self.fc_drop(x)\n",
    "        #x = self.conv1D_1(x)\n",
    "        x = torch.cat([self.conv1D_1(x[:,:,9:(-9)]), \n",
    "                       self.conv1D_3(x)], dim=1) # bs,64,60\n",
    "        x = self.relu(x)\n",
    "        x = self.bn_cont2(x)\n",
    "        x = self.fc_drop(x)\n",
    "        x = self.conv1D_2(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.bn_cont3(x) # bs,6,60\n",
    "        #x = self.fc_drop(x)\n",
    "        #x = self.feat_product(x)\n",
    "        #x = x.reshape(x.shape[0],-1)\n",
    "        #x = self.layers(x)\n",
    "        #x = x.reshape(x.shape[0],60,6)\n",
    "        x = x.transpose(1,2) # bs,60,6\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_fn(model, loader, device, context = None):\n",
    "    \n",
    "    if CLOUD and (not CLOUD_SINGLE):\n",
    "        tlen = len(loader._loader._loader)\n",
    "        OUT_LOSS = 1000\n",
    "        OUT_TIME = 4\n",
    "        generator = loader\n",
    "        device_num = int(str(device)[-1])\n",
    "        dataset = loader._loader._loader.dataset\n",
    "    else:\n",
    "        tlen = len(loader)\n",
    "        OUT_LOSS = 10\n",
    "        OUT_TIME = 1\n",
    "        generator = enumerate(loader)\n",
    "        device_num = 1\n",
    "        dataset = loader.dataset\n",
    "    \n",
    "    #print('Start training {}'.format(device), 'batches', tlen)\n",
    "    \n",
    "    criterion = BCEWithLogitsLoss(weight = torch.Tensor(class_weights).to(device))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(0.9, 0.99))\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    if CLOUD:\n",
    "        tracker = xm.RateTracker()\n",
    "\n",
    "    tloss = 0\n",
    "    tloss_count = 0\n",
    "    \n",
    "    st = time.time()\n",
    "    mixup_collected = False\n",
    "    for i, (x, y) in generator:\n",
    "        if (not CLOUD) or CLOUD_SINGLE:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "        \n",
    "        if MIXUP:\n",
    "            if mixup_collected:\n",
    "                lambd = np.random.beta(0.4, 0.4, y.size(0))\n",
    "                lambd = torch.Tensor(lambd).to(device)[:,None,None]\n",
    "                #shuffle = torch.randperm(y.size(0)).to(device)\n",
    "                x = lambd * x + (1-lambd) * x_mix #x[shuffle]\n",
    "                mixup_collected = False\n",
    "            else:\n",
    "                x_mix = x\n",
    "                y_mix = y\n",
    "                mixup_collected = True\n",
    "                continue\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        \n",
    "        if MIXUP:\n",
    "            loss = criterion(output, y, lambd) + criterion(output, y_mix, 1-lambd) #y[shuffle]\n",
    "            del x_mix, y_mix\n",
    "        else:\n",
    "            loss = criterion(output, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        tloss += len(y)*loss.cpu().detach().item()\n",
    "        tloss_count += len(y)\n",
    "        \n",
    "        if CLOUD or CLOUD_SINGLE:\n",
    "            xm.optimizer_step(optimizer)\n",
    "            if CLOUD_SINGLE:\n",
    "                xm.mark_step()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        \n",
    "        if CLOUD:\n",
    "            tracker.add(len(y))\n",
    "        \n",
    "        st_passed = time.time() - st\n",
    "        if (i+1)%OUT_TIME == 0 and device_num == 1:\n",
    "            #print(torch_xla._XLAC._xla_metrics_report())\n",
    "            print('Batch {} device: {} time passed: {:.3f} time per batch: {:.3f}'\n",
    "                .format(i+1, device, st_passed, st_passed/(i+1)))\n",
    "        \n",
    "        del loss, output, y, x\n",
    "    \n",
    "    return tloss, tloss_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def val_loop_fn(model, loader, device, context = None):\n",
    "    \n",
    "    if CLOUD and (not CLOUD_SINGLE):\n",
    "        tlen = len(loader._loader._loader)\n",
    "        OUT_LOSS = 1000\n",
    "        OUT_TIME = 1\n",
    "        generator = loader\n",
    "        device_num = int(str(device)[-1])\n",
    "    else:\n",
    "        tlen = len(loader)\n",
    "        OUT_LOSS = 1000\n",
    "        OUT_TIME = 10\n",
    "        generator = enumerate(loader)\n",
    "        device_num = 1\n",
    "    \n",
    "    #print('Start validating {}'.format(device), 'batches', tlen)\n",
    "    \n",
    "    st = time.time()\n",
    "    model.eval()\n",
    "    \n",
    "    results = []\n",
    "    indices = []\n",
    "    offsets = []\n",
    "    \n",
    "    for i, (x, y, idx, offset) in generator:\n",
    "        \n",
    "        if (not CLOUD) or CLOUD_SINGLE:\n",
    "            x = x.to(device)\n",
    "        \n",
    "        output = torch.sigmoid(model(x))\n",
    "        \n",
    "        mask = (idx >= 0)\n",
    "        results.append(output[mask].cpu().detach().numpy())\n",
    "        indices.append(idx[mask].cpu().detach().numpy())\n",
    "        offsets.append(offset[mask].cpu().detach().numpy())\n",
    "        \n",
    "        st_passed = time.time() - st\n",
    "        if (i+1)%OUT_TIME == 0 and device_num == 1:\n",
    "            print('Batch {} device: {} time passed: {:.3f} time per batch: {:.3f}'\n",
    "                  .format(i+1, device, st_passed, st_passed/(i+1)))\n",
    "        \n",
    "        del output, y, x, idx, offset\n",
    "    \n",
    "    results = np.concatenate(results)\n",
    "    indices = np.concatenate(indices)\n",
    "    offsets = np.concatenate(offsets)\n",
    "    \n",
    "    return results, indices, offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_loop_fn(model, loader, device, context = None):\n",
    "    \n",
    "    if CLOUD and (not CLOUD_SINGLE):\n",
    "        tlen = len(loader._loader._loader)\n",
    "        OUT_LOSS = 1000\n",
    "        OUT_TIME = 100\n",
    "        generator = loader\n",
    "        device_num = int(str(device)[-1])\n",
    "    else:\n",
    "        tlen = len(loader)\n",
    "        OUT_LOSS = 1000\n",
    "        OUT_TIME = 10\n",
    "        generator = enumerate(loader)\n",
    "        device_num = 1\n",
    "    \n",
    "    #print('Start testing {}'.format(device), 'batches', tlen)\n",
    "    \n",
    "    st = time.time()\n",
    "    model.eval()\n",
    "    \n",
    "    results = []\n",
    "    indices = []\n",
    "    offsets = []\n",
    "    \n",
    "    for i, (x, y, idx, offset) in generator:\n",
    "        \n",
    "        if (not CLOUD) or CLOUD_SINGLE:\n",
    "            x = x.to(device)\n",
    "        \n",
    "        output = torch.sigmoid(model(x))\n",
    "        \n",
    "        mask = (idx >= 0)\n",
    "        results.append(output[mask].cpu().detach().numpy())\n",
    "        indices.append(idx[mask].cpu().detach().numpy())\n",
    "        offsets.append(offset[mask].cpu().detach().numpy())\n",
    "        \n",
    "        st_passed = time.time() - st\n",
    "        if (i+1)%OUT_TIME == 0 and device_num == 1:\n",
    "            print('B{} -> time passed: {:.3f} time per batch: {:.3f}'.format(i+1, st_passed, st_passed/(i+1)))\n",
    "        \n",
    "        del output, x, y, idx, offset\n",
    "    \n",
    "    return np.concatenate(results), np.concatenate(indices), np.concatenate(offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(weight=None, load_model=True, epochs=1, bs=100):\n",
    "    \n",
    "    cur_epoch = getCurrentBatch()\n",
    "    if cur_epoch is None: cur_epoch = 0\n",
    "    print('completed epochs:', cur_epoch, 'starting now:', epochs)\n",
    "    \n",
    "    setSeeds(SEED + cur_epoch)\n",
    "    \n",
    "    trn_ds = RSNA_DataSet(trn_data, ids_df, mode='train', bs=bs)\n",
    "    loader = D.DataLoader(trn_ds, num_workers=16 if CLOUD else 0, batch_size=bs, shuffle=True, drop_last=True)\n",
    "    val_ds = RSNA_DataSet(val_data, ids_df, mode='valid', bs=bs)\n",
    "    loader_val = D.DataLoader(val_ds, num_workers=16 if CLOUD else 0, batch_size=bs, shuffle=True)\n",
    "    #tst_ds = RSNA_DataSet(test_md, test_ids_df, mode='test')\n",
    "    print('dataset train:', len(trn_ds), 'valid:', len(val_ds), 'loader train:', len(loader), 'valid:', len(loader_val))\n",
    "    \n",
    "    model = TabularModel(n_cont = len(meta_cols), out_sz=360, layers=[500,200], ps=[0.5,0.5], bn_final=True)\n",
    "    \n",
    "    model_file_name = modelFileName(return_last=True)\n",
    "    if model_file_name is not None:\n",
    "        print('loading model', model_file_name)\n",
    "        state_dict = torch.load(PATH_WORK/'models'/model_file_name)\n",
    "        model.load_state_dict(state_dict)\n",
    "    else:\n",
    "        print('starting from scratch')\n",
    "    \n",
    "    if (not CLOUD) or CLOUD_SINGLE:\n",
    "        model = model.to(device)\n",
    "    else:\n",
    "        model_parallel = dp.DataParallel(model, device_ids=devices)\n",
    "    \n",
    "    for i in range(cur_epoch+1, cur_epoch+epochs+1):\n",
    "        st = time.time()\n",
    "\n",
    "        if CLOUD and (not CLOUD_SINGLE):\n",
    "            results = model_parallel(train_loop_fn, loader)\n",
    "            tloss, tloss_count = np.stack(results).sum(0)\n",
    "            state_dict = model_parallel._models[0].state_dict()\n",
    "        else:\n",
    "            tloss, tloss_count = train_loop_fn(model, loader, device)\n",
    "            state_dict = model.state_dict()\n",
    "        \n",
    "        state_dict = {k:v.to('cpu') for k,v in state_dict.items()}\n",
    "        tr_ll = tloss / tloss_count\n",
    "        \n",
    "        train_time = time.time()-st\n",
    "        \n",
    "        model_file_name = modelFileName(return_next=True)\n",
    "        if not DATA_SMALL:\n",
    "            torch.save(state_dict, PATH_WORK/'models'/model_file_name)\n",
    "        \n",
    "        st = time.time()\n",
    "        if CLOUD and (not CLOUD_SINGLE):\n",
    "            results = model_parallel(val_loop_fn, loader_val)\n",
    "            predictions = np.concatenate([results[i][0] for i in range(MAX_DEVICES)])\n",
    "            indices = np.concatenate([results[i][1] for i in range(MAX_DEVICES)])\n",
    "            offsets = np.concatenate([results[i][2] for i in range(MAX_DEVICES)])\n",
    "        else:\n",
    "            predictions, indices, offsets = val_loop_fn(model, loader_val, device)\n",
    "        \n",
    "        loc_data = val_data.copy()\n",
    "        if DATA_SMALL:\n",
    "            val_sz = int(0.01*len(val_data.SeriesInstanceUID.unique()))\n",
    "            val_series = val_data.SeriesInstanceUID.unique()[:val_sz]\n",
    "            loc_data = loc_data.loc[val_data.SeriesInstanceUID.isin(val_series)]\n",
    "        \n",
    "        predictions = predictions[np.argsort(indices)]\n",
    "        offsets = offsets[np.argsort(indices)]\n",
    "        assert len(predictions) == len(loc_data.SeriesInstanceUID.unique())\n",
    "        assert len(predictions) == len(offsets)\n",
    "        assert np.all(indices[np.argsort(indices)] == np.array(range(len(predictions))))\n",
    "        \n",
    "        valid_time = time.time()-st\n",
    "        \n",
    "        val_results = np.zeros((len(loc_data),6))\n",
    "        for k, series in enumerate(loc_data.SeriesInstanceUID.unique()):\n",
    "            mask = loc_data.SeriesInstanceUID == series\n",
    "            assert (offsets[k] + mask.sum()) <= 60\n",
    "            val_results[mask] = predictions[k,offsets[k]:(offsets[k] + mask.sum())]\n",
    "        \n",
    "        lls = [log_loss(loc_data[all_ich[k]].values, val_results[:,k], eps=1e-8, labels=[0,1]) for k in range(6)]\n",
    "        ll = (class_weights * np.array(lls)).mean()\n",
    "        cor = np.corrcoef(loc_data.loc[:,all_ich].values.reshape(-1), val_results.reshape(-1))[0,1]\n",
    "\n",
    "        print('epoch {}, train ll: {:.4f}, val ll: {:.4f}, cor: {:.4f}'.format(i, tr_ll, ll, cor))\n",
    "        valid_time = time.time()-st\n",
    "\n",
    "        epoch_stats = pd.DataFrame([[i, 0, tr_ll, ll, cor, lls[0], lls[1], lls[2], lls[3], lls[4], lls[5],\n",
    "                                     len(trn_ds), len(val_ds), bs, train_time, valid_time,\n",
    "                                     learning_rate, weight_decay]], \n",
    "                                   columns = \n",
    "                                    ['epoch','fold','train_loss','val_loss','cor',\n",
    "                                     'any','epidural','intraparenchymal','intraventricular','subarachnoid','subdural',\n",
    "                                     'train_sz','val_sz','bs','train_time','valid_time','lr','wd'\n",
    "                                     ])\n",
    "\n",
    "        stats_filename = PATH_WORK/'stats.f{}.v{}'.format(0,VERSION)\n",
    "        if stats_filename.is_file():\n",
    "            epoch_stats = pd.concat([pd.read_csv(stats_filename), epoch_stats], sort=False)\n",
    "        #if not DATA_SMALL:\n",
    "        epoch_stats.to_csv(stats_filename, index=False)\n",
    "    \n",
    "    return model, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch 22 device: xla:1 time passed: 277.972 time per batch: 12.635 - 16 cores / 8 workers\n",
    "#Batch 22 device: xla:1 time passed: 209.280 time per batch: 9.513  - 16 cores / 16 workers\n",
    "#Batch 22 device: xla:1 time passed: 213.209 time per batch: 9.691  - 16 cores / 32 workers\n",
    "#Batch 22 device: xla:1 time passed: 275.780 time per batch: 12.535 - 16 cores / 8 workers\n",
    "#Batch 22 device: xla:1 time passed: 208.826 time per batch: 9.492  - 16 cores / 16 workers\n",
    "#Batch 22 device: xla:1 time passed: 245.750 time per batch: 11.170 - 16 cores / 12 workers\n",
    "#Batch 22 device: xla:1 time passed: 374.876 time per batch: 17.040 - 16 cores / 8 workers\n",
    "#Batch 22 device: xla:1 time passed: 400.221 time per batch: 18.192 - 8 cores / 8 workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train ll: 0.0354, val ll: 0.0577, cor: 0.8462, LB 0.065"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-cycle\n",
    "# copy latest model to GS code\n",
    "# add performance tracking, search for bottlenecks\n",
    "# smaller bs\n",
    "# keep the data in memory\n",
    "# improve black image meta data\n",
    "# freeze bias approach?\n",
    "\n",
    "# Yuval: zoom in, squish, perspective wraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epochs: 0 starting now: 3\n",
      "dataset train: 175 valid: 19 loader train: 17 valid: 2\n",
      "starting from scratch\n",
      "Batch 1 device: cuda time passed: 1.747 time per batch: 1.747\n",
      "Batch 2 device: cuda time passed: 3.327 time per batch: 1.663\n",
      "Batch 3 device: cuda time passed: 4.775 time per batch: 1.592\n",
      "Batch 4 device: cuda time passed: 6.222 time per batch: 1.555\n",
      "Batch 5 device: cuda time passed: 7.549 time per batch: 1.510\n",
      "Batch 6 device: cuda time passed: 8.904 time per batch: 1.484\n",
      "Batch 7 device: cuda time passed: 10.279 time per batch: 1.468\n",
      "Batch 8 device: cuda time passed: 11.578 time per batch: 1.447\n",
      "Batch 9 device: cuda time passed: 12.858 time per batch: 1.429\n",
      "Batch 10 device: cuda time passed: 14.057 time per batch: 1.406\n",
      "Batch 11 device: cuda time passed: 15.340 time per batch: 1.395\n",
      "Batch 12 device: cuda time passed: 16.755 time per batch: 1.396\n",
      "Batch 13 device: cuda time passed: 18.210 time per batch: 1.401\n",
      "Batch 14 device: cuda time passed: 19.777 time per batch: 1.413\n",
      "Batch 15 device: cuda time passed: 21.309 time per batch: 1.421\n",
      "Batch 16 device: cuda time passed: 22.682 time per batch: 1.418\n",
      "Batch 17 device: cuda time passed: 24.022 time per batch: 1.413\n",
      "epoch 1, train ll: 0.6728, val ll: 0.6407, cor: 0.5285\n",
      "Batch 1 device: cuda time passed: 1.445 time per batch: 1.445\n",
      "Batch 2 device: cuda time passed: 2.829 time per batch: 1.414\n",
      "Batch 3 device: cuda time passed: 4.099 time per batch: 1.366\n",
      "Batch 4 device: cuda time passed: 5.414 time per batch: 1.353\n",
      "Batch 5 device: cuda time passed: 6.716 time per batch: 1.343\n",
      "Batch 6 device: cuda time passed: 8.026 time per batch: 1.338\n",
      "Batch 7 device: cuda time passed: 9.273 time per batch: 1.325\n",
      "Batch 8 device: cuda time passed: 10.746 time per batch: 1.343\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-f2c217fe1c08>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0005\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mweight_decay\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1e-4\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_one\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-56-6d21a1e94144>\u001b[0m in \u001b[0;36mtrain_one\u001b[1;34m(weight, load_model, epochs, bs)\u001b[0m\n\u001b[0;32m     37\u001b[0m             \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_parallel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_models\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m             \u001b[0mtloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtloss_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_loop_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m             \u001b[0mstate_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-53-352f94753085>\u001b[0m in \u001b[0;36mtrain_loop_fn\u001b[1;34m(model, loader, device, context)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mtloss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m         \u001b[0mtloss_count\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DATA_SMALL = True\n",
    "learning_rate = 0.0005\n",
    "weight_decay = 1e-4\n",
    "model, predictions = train_one(epochs=3, bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>fold</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>cor</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "      <th>train_sz</th>\n",
       "      <th>val_sz</th>\n",
       "      <th>bs</th>\n",
       "      <th>train_time</th>\n",
       "      <th>valid_time</th>\n",
       "      <th>lr</th>\n",
       "      <th>wd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.245414</td>\n",
       "      <td>0.368845</td>\n",
       "      <td>0.457550</td>\n",
       "      <td>0.514073</td>\n",
       "      <td>0.044114</td>\n",
       "      <td>0.229571</td>\n",
       "      <td>0.092236</td>\n",
       "      <td>0.201881</td>\n",
       "      <td>0.985966</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>160.251015</td>\n",
       "      <td>31.356178</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.111711</td>\n",
       "      <td>0.425409</td>\n",
       "      <td>0.517751</td>\n",
       "      <td>0.835668</td>\n",
       "      <td>0.052879</td>\n",
       "      <td>0.300094</td>\n",
       "      <td>0.288142</td>\n",
       "      <td>0.345089</td>\n",
       "      <td>0.320320</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.015386</td>\n",
       "      <td>29.556167</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.092080</td>\n",
       "      <td>0.172839</td>\n",
       "      <td>0.584343</td>\n",
       "      <td>0.221234</td>\n",
       "      <td>0.019638</td>\n",
       "      <td>0.128909</td>\n",
       "      <td>0.129850</td>\n",
       "      <td>0.149150</td>\n",
       "      <td>0.339859</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>154.879500</td>\n",
       "      <td>27.140476</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.113210</td>\n",
       "      <td>0.566921</td>\n",
       "      <td>0.309114</td>\n",
       "      <td>1.084074</td>\n",
       "      <td>0.027198</td>\n",
       "      <td>0.492600</td>\n",
       "      <td>0.448893</td>\n",
       "      <td>0.417427</td>\n",
       "      <td>0.414179</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>156.678311</td>\n",
       "      <td>28.908540</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142060</td>\n",
       "      <td>0.122733</td>\n",
       "      <td>0.687616</td>\n",
       "      <td>0.178393</td>\n",
       "      <td>0.048400</td>\n",
       "      <td>0.098498</td>\n",
       "      <td>0.110666</td>\n",
       "      <td>0.111207</td>\n",
       "      <td>0.133571</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>154.641192</td>\n",
       "      <td>26.106112</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.165164</td>\n",
       "      <td>0.171820</td>\n",
       "      <td>0.681821</td>\n",
       "      <td>0.330669</td>\n",
       "      <td>0.017591</td>\n",
       "      <td>0.091962</td>\n",
       "      <td>0.113049</td>\n",
       "      <td>0.157081</td>\n",
       "      <td>0.161722</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>154.669446</td>\n",
       "      <td>25.977148</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.108488</td>\n",
       "      <td>0.175178</td>\n",
       "      <td>0.442986</td>\n",
       "      <td>0.314731</td>\n",
       "      <td>0.015936</td>\n",
       "      <td>0.149549</td>\n",
       "      <td>0.089253</td>\n",
       "      <td>0.148018</td>\n",
       "      <td>0.194026</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.439858</td>\n",
       "      <td>25.644034</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046603</td>\n",
       "      <td>0.110731</td>\n",
       "      <td>0.735936</td>\n",
       "      <td>0.190573</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.076381</td>\n",
       "      <td>0.056579</td>\n",
       "      <td>0.118125</td>\n",
       "      <td>0.127090</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.412679</td>\n",
       "      <td>26.197765</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.043568</td>\n",
       "      <td>0.075700</td>\n",
       "      <td>0.808988</td>\n",
       "      <td>0.121208</td>\n",
       "      <td>0.015132</td>\n",
       "      <td>0.055995</td>\n",
       "      <td>0.040890</td>\n",
       "      <td>0.080520</td>\n",
       "      <td>0.094944</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>154.755002</td>\n",
       "      <td>26.382353</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.041196</td>\n",
       "      <td>0.074663</td>\n",
       "      <td>0.810396</td>\n",
       "      <td>0.121195</td>\n",
       "      <td>0.014108</td>\n",
       "      <td>0.056875</td>\n",
       "      <td>0.035701</td>\n",
       "      <td>0.075549</td>\n",
       "      <td>0.098022</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>154.682014</td>\n",
       "      <td>26.325155</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038925</td>\n",
       "      <td>0.062498</td>\n",
       "      <td>0.835334</td>\n",
       "      <td>0.101864</td>\n",
       "      <td>0.013212</td>\n",
       "      <td>0.046009</td>\n",
       "      <td>0.030019</td>\n",
       "      <td>0.062721</td>\n",
       "      <td>0.081797</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.851308</td>\n",
       "      <td>26.180528</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038245</td>\n",
       "      <td>0.060230</td>\n",
       "      <td>0.840434</td>\n",
       "      <td>0.098347</td>\n",
       "      <td>0.012866</td>\n",
       "      <td>0.044839</td>\n",
       "      <td>0.029394</td>\n",
       "      <td>0.059918</td>\n",
       "      <td>0.077898</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>154.850655</td>\n",
       "      <td>26.117462</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037875</td>\n",
       "      <td>0.059834</td>\n",
       "      <td>0.841174</td>\n",
       "      <td>0.098087</td>\n",
       "      <td>0.012779</td>\n",
       "      <td>0.044849</td>\n",
       "      <td>0.029316</td>\n",
       "      <td>0.059304</td>\n",
       "      <td>0.076416</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>384.280921</td>\n",
       "      <td>51.336847</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037421</td>\n",
       "      <td>0.059395</td>\n",
       "      <td>0.842651</td>\n",
       "      <td>0.097142</td>\n",
       "      <td>0.012808</td>\n",
       "      <td>0.044485</td>\n",
       "      <td>0.029056</td>\n",
       "      <td>0.058803</td>\n",
       "      <td>0.076331</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.777497</td>\n",
       "      <td>29.887910</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037143</td>\n",
       "      <td>0.058922</td>\n",
       "      <td>0.844032</td>\n",
       "      <td>0.096749</td>\n",
       "      <td>0.012895</td>\n",
       "      <td>0.043866</td>\n",
       "      <td>0.028514</td>\n",
       "      <td>0.058257</td>\n",
       "      <td>0.075425</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.482482</td>\n",
       "      <td>25.653671</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.063199</td>\n",
       "      <td>0.062034</td>\n",
       "      <td>0.837447</td>\n",
       "      <td>0.101087</td>\n",
       "      <td>0.012745</td>\n",
       "      <td>0.047637</td>\n",
       "      <td>0.032370</td>\n",
       "      <td>0.061437</td>\n",
       "      <td>0.077872</td>\n",
       "      <td>17577</td>\n",
       "      <td>2000</td>\n",
       "      <td>100</td>\n",
       "      <td>169.736084</td>\n",
       "      <td>29.725267</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.061176</td>\n",
       "      <td>0.060760</td>\n",
       "      <td>0.838443</td>\n",
       "      <td>0.099033</td>\n",
       "      <td>0.012716</td>\n",
       "      <td>0.045007</td>\n",
       "      <td>0.033106</td>\n",
       "      <td>0.058764</td>\n",
       "      <td>0.077659</td>\n",
       "      <td>17577</td>\n",
       "      <td>2000</td>\n",
       "      <td>100</td>\n",
       "      <td>158.700440</td>\n",
       "      <td>24.708474</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.060958</td>\n",
       "      <td>0.062885</td>\n",
       "      <td>0.841032</td>\n",
       "      <td>0.104076</td>\n",
       "      <td>0.012513</td>\n",
       "      <td>0.046202</td>\n",
       "      <td>0.032725</td>\n",
       "      <td>0.061478</td>\n",
       "      <td>0.079124</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>153.454062</td>\n",
       "      <td>28.075624</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0.060142</td>\n",
       "      <td>0.062561</td>\n",
       "      <td>0.840940</td>\n",
       "      <td>0.103543</td>\n",
       "      <td>0.012825</td>\n",
       "      <td>0.046356</td>\n",
       "      <td>0.033186</td>\n",
       "      <td>0.060847</td>\n",
       "      <td>0.077625</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>154.643816</td>\n",
       "      <td>26.342926</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.060683</td>\n",
       "      <td>0.063580</td>\n",
       "      <td>0.840973</td>\n",
       "      <td>0.106396</td>\n",
       "      <td>0.013263</td>\n",
       "      <td>0.045586</td>\n",
       "      <td>0.031670</td>\n",
       "      <td>0.060354</td>\n",
       "      <td>0.081400</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>154.788163</td>\n",
       "      <td>26.137916</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.060733</td>\n",
       "      <td>0.060299</td>\n",
       "      <td>0.842699</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.012535</td>\n",
       "      <td>0.044882</td>\n",
       "      <td>0.030581</td>\n",
       "      <td>0.057817</td>\n",
       "      <td>0.077322</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>154.736595</td>\n",
       "      <td>29.023877</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0.060171</td>\n",
       "      <td>0.062357</td>\n",
       "      <td>0.839801</td>\n",
       "      <td>0.102981</td>\n",
       "      <td>0.012760</td>\n",
       "      <td>0.045732</td>\n",
       "      <td>0.033905</td>\n",
       "      <td>0.060712</td>\n",
       "      <td>0.077428</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>154.789902</td>\n",
       "      <td>26.088672</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0.060118</td>\n",
       "      <td>0.060522</td>\n",
       "      <td>0.841712</td>\n",
       "      <td>0.099508</td>\n",
       "      <td>0.012758</td>\n",
       "      <td>0.044703</td>\n",
       "      <td>0.030725</td>\n",
       "      <td>0.060239</td>\n",
       "      <td>0.076210</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.973188</td>\n",
       "      <td>26.107710</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0.060056</td>\n",
       "      <td>0.060459</td>\n",
       "      <td>0.843309</td>\n",
       "      <td>0.099756</td>\n",
       "      <td>0.012853</td>\n",
       "      <td>0.045013</td>\n",
       "      <td>0.030690</td>\n",
       "      <td>0.059043</td>\n",
       "      <td>0.076100</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>154.747613</td>\n",
       "      <td>26.102819</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.060182</td>\n",
       "      <td>0.060651</td>\n",
       "      <td>0.843138</td>\n",
       "      <td>0.100404</td>\n",
       "      <td>0.012770</td>\n",
       "      <td>0.045022</td>\n",
       "      <td>0.030943</td>\n",
       "      <td>0.058453</td>\n",
       "      <td>0.076564</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.069272</td>\n",
       "      <td>28.767607</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0.058318</td>\n",
       "      <td>0.059742</td>\n",
       "      <td>0.843963</td>\n",
       "      <td>0.098442</td>\n",
       "      <td>0.012746</td>\n",
       "      <td>0.044305</td>\n",
       "      <td>0.030169</td>\n",
       "      <td>0.058106</td>\n",
       "      <td>0.075987</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.616217</td>\n",
       "      <td>26.113737</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0.058986</td>\n",
       "      <td>0.059739</td>\n",
       "      <td>0.843590</td>\n",
       "      <td>0.098584</td>\n",
       "      <td>0.012919</td>\n",
       "      <td>0.044461</td>\n",
       "      <td>0.029744</td>\n",
       "      <td>0.058144</td>\n",
       "      <td>0.075737</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>154.627858</td>\n",
       "      <td>26.179941</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0.059261</td>\n",
       "      <td>0.059380</td>\n",
       "      <td>0.844253</td>\n",
       "      <td>0.097980</td>\n",
       "      <td>0.012621</td>\n",
       "      <td>0.044263</td>\n",
       "      <td>0.029754</td>\n",
       "      <td>0.057572</td>\n",
       "      <td>0.075489</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.360457</td>\n",
       "      <td>26.128672</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0.059228</td>\n",
       "      <td>0.059909</td>\n",
       "      <td>0.842914</td>\n",
       "      <td>0.098742</td>\n",
       "      <td>0.012802</td>\n",
       "      <td>0.044614</td>\n",
       "      <td>0.029972</td>\n",
       "      <td>0.058185</td>\n",
       "      <td>0.076303</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.530201</td>\n",
       "      <td>27.551405</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037146</td>\n",
       "      <td>0.059396</td>\n",
       "      <td>0.843380</td>\n",
       "      <td>0.098268</td>\n",
       "      <td>0.012643</td>\n",
       "      <td>0.044284</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.057151</td>\n",
       "      <td>0.076586</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>154.486103</td>\n",
       "      <td>26.230717</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036045</td>\n",
       "      <td>0.057565</td>\n",
       "      <td>0.846713</td>\n",
       "      <td>0.094500</td>\n",
       "      <td>0.012647</td>\n",
       "      <td>0.043381</td>\n",
       "      <td>0.027538</td>\n",
       "      <td>0.056336</td>\n",
       "      <td>0.074055</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.204885</td>\n",
       "      <td>26.453111</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035795</td>\n",
       "      <td>0.057748</td>\n",
       "      <td>0.846156</td>\n",
       "      <td>0.094808</td>\n",
       "      <td>0.013045</td>\n",
       "      <td>0.043539</td>\n",
       "      <td>0.027406</td>\n",
       "      <td>0.056319</td>\n",
       "      <td>0.074310</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.244213</td>\n",
       "      <td>27.856316</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035666</td>\n",
       "      <td>0.057728</td>\n",
       "      <td>0.846278</td>\n",
       "      <td>0.095015</td>\n",
       "      <td>0.012587</td>\n",
       "      <td>0.043213</td>\n",
       "      <td>0.027224</td>\n",
       "      <td>0.056258</td>\n",
       "      <td>0.074786</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.400090</td>\n",
       "      <td>26.334645</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035420</td>\n",
       "      <td>0.057729</td>\n",
       "      <td>0.846230</td>\n",
       "      <td>0.094790</td>\n",
       "      <td>0.012786</td>\n",
       "      <td>0.043387</td>\n",
       "      <td>0.027166</td>\n",
       "      <td>0.056169</td>\n",
       "      <td>0.075013</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.614933</td>\n",
       "      <td>26.507466</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  fold  train_loss  val_loss       cor       any  epidural  \\\n",
       "0       1     0    0.245414  0.368845  0.457550  0.514073  0.044114   \n",
       "1       2     0    0.111711  0.425409  0.517751  0.835668  0.052879   \n",
       "2       3     0    0.092080  0.172839  0.584343  0.221234  0.019638   \n",
       "3       4     0    0.113210  0.566921  0.309114  1.084074  0.027198   \n",
       "4       5     0    0.142060  0.122733  0.687616  0.178393  0.048400   \n",
       "5       6     0    0.165164  0.171820  0.681821  0.330669  0.017591   \n",
       "6       7     0    0.108488  0.175178  0.442986  0.314731  0.015936   \n",
       "7       8     0    0.046603  0.110731  0.735936  0.190573  0.015797   \n",
       "8       9     0    0.043568  0.075700  0.808988  0.121208  0.015132   \n",
       "9      10     0    0.041196  0.074663  0.810396  0.121195  0.014108   \n",
       "10     11     0    0.038925  0.062498  0.835334  0.101864  0.013212   \n",
       "11     12     0    0.038245  0.060230  0.840434  0.098347  0.012866   \n",
       "12     13     0    0.037875  0.059834  0.841174  0.098087  0.012779   \n",
       "13     14     0    0.037421  0.059395  0.842651  0.097142  0.012808   \n",
       "14     15     0    0.037143  0.058922  0.844032  0.096749  0.012895   \n",
       "15     16     0    0.063199  0.062034  0.837447  0.101087  0.012745   \n",
       "16     17     0    0.061176  0.060760  0.838443  0.099033  0.012716   \n",
       "17     18     0    0.060958  0.062885  0.841032  0.104076  0.012513   \n",
       "18     19     0    0.060142  0.062561  0.840940  0.103543  0.012825   \n",
       "19     20     0    0.060683  0.063580  0.840973  0.106396  0.013263   \n",
       "20     21     0    0.060733  0.060299  0.842699  0.099477  0.012535   \n",
       "21     22     0    0.060171  0.062357  0.839801  0.102981  0.012760   \n",
       "22     23     0    0.060118  0.060522  0.841712  0.099508  0.012758   \n",
       "23     24     0    0.060056  0.060459  0.843309  0.099756  0.012853   \n",
       "24     25     0    0.060182  0.060651  0.843138  0.100404  0.012770   \n",
       "25     26     0    0.058318  0.059742  0.843963  0.098442  0.012746   \n",
       "26     27     0    0.058986  0.059739  0.843590  0.098584  0.012919   \n",
       "27     28     0    0.059261  0.059380  0.844253  0.097980  0.012621   \n",
       "28     29     0    0.059228  0.059909  0.842914  0.098742  0.012802   \n",
       "29     30     0    0.037146  0.059396  0.843380  0.098268  0.012643   \n",
       "30     31     0    0.036045  0.057565  0.846713  0.094500  0.012647   \n",
       "31     32     0    0.035795  0.057748  0.846156  0.094808  0.013045   \n",
       "32     33     0    0.035666  0.057728  0.846278  0.095015  0.012587   \n",
       "33     34     0    0.035420  0.057729  0.846230  0.094790  0.012786   \n",
       "\n",
       "    intraparenchymal  intraventricular  subarachnoid  subdural  train_sz  \\\n",
       "0           0.229571          0.092236      0.201881  0.985966     17577   \n",
       "1           0.300094          0.288142      0.345089  0.320320     17577   \n",
       "2           0.128909          0.129850      0.149150  0.339859     17577   \n",
       "3           0.492600          0.448893      0.417427  0.414179     17577   \n",
       "4           0.098498          0.110666      0.111207  0.133571     17577   \n",
       "5           0.091962          0.113049      0.157081  0.161722     17577   \n",
       "6           0.149549          0.089253      0.148018  0.194026     17577   \n",
       "7           0.076381          0.056579      0.118125  0.127090     17577   \n",
       "8           0.055995          0.040890      0.080520  0.094944     17577   \n",
       "9           0.056875          0.035701      0.075549  0.098022     17577   \n",
       "10          0.046009          0.030019      0.062721  0.081797     17577   \n",
       "11          0.044839          0.029394      0.059918  0.077898     17577   \n",
       "12          0.044849          0.029316      0.059304  0.076416     17577   \n",
       "13          0.044485          0.029056      0.058803  0.076331     17577   \n",
       "14          0.043866          0.028514      0.058257  0.075425     17577   \n",
       "15          0.047637          0.032370      0.061437  0.077872     17577   \n",
       "16          0.045007          0.033106      0.058764  0.077659     17577   \n",
       "17          0.046202          0.032725      0.061478  0.079124     17577   \n",
       "18          0.046356          0.033186      0.060847  0.077625     17577   \n",
       "19          0.045586          0.031670      0.060354  0.081400     17577   \n",
       "20          0.044882          0.030581      0.057817  0.077322     17577   \n",
       "21          0.045732          0.033905      0.060712  0.077428     17577   \n",
       "22          0.044703          0.030725      0.060239  0.076210     17577   \n",
       "23          0.045013          0.030690      0.059043  0.076100     17577   \n",
       "24          0.045022          0.030943      0.058453  0.076564     17577   \n",
       "25          0.044305          0.030169      0.058106  0.075987     17577   \n",
       "26          0.044461          0.029744      0.058144  0.075737     17577   \n",
       "27          0.044263          0.029754      0.057572  0.075489     17577   \n",
       "28          0.044614          0.029972      0.058185  0.076303     17577   \n",
       "29          0.044284          0.028572      0.057151  0.076586     17577   \n",
       "30          0.043381          0.027538      0.056336  0.074055     17577   \n",
       "31          0.043539          0.027406      0.056319  0.074310     17577   \n",
       "32          0.043213          0.027224      0.056258  0.074786     17577   \n",
       "33          0.043387          0.027166      0.056169  0.075013     17577   \n",
       "\n",
       "    val_sz   bs  train_time  valid_time      lr      wd  \n",
       "0     2400  100  160.251015   31.356178  0.1000  0.0001  \n",
       "1     2400  100  155.015386   29.556167  0.1000  0.0001  \n",
       "2     2400  100  154.879500   27.140476  0.1000  0.0001  \n",
       "3     2400  100  156.678311   28.908540  0.1000  0.0001  \n",
       "4     2400  100  154.641192   26.106112  0.1000  0.0001  \n",
       "5     2400  100  154.669446   25.977148  0.1000  0.0001  \n",
       "6     2400  100  155.439858   25.644034  0.1000  0.0001  \n",
       "7     2400  100  155.412679   26.197765  0.0100  0.0001  \n",
       "8     2400  100  154.755002   26.382353  0.0100  0.0001  \n",
       "9     2400  100  154.682014   26.325155  0.0100  0.0001  \n",
       "10    2400  100  155.851308   26.180528  0.0020  0.0001  \n",
       "11    2400  100  154.850655   26.117462  0.0020  0.0001  \n",
       "12    2400  100  384.280921   51.336847  0.0020  0.0001  \n",
       "13    2400  100  155.777497   29.887910  0.0020  0.0001  \n",
       "14    2400  100  155.482482   25.653671  0.0020  0.0001  \n",
       "15    2000  100  169.736084   29.725267  0.0020  0.0001  \n",
       "16    2000  100  158.700440   24.708474  0.0020  0.0001  \n",
       "17    2400  100  153.454062   28.075624  0.0020  0.0001  \n",
       "18    2400  100  154.643816   26.342926  0.0020  0.0001  \n",
       "19    2400  100  154.788163   26.137916  0.0020  0.0001  \n",
       "20    2400  100  154.736595   29.023877  0.0020  0.0001  \n",
       "21    2400  100  154.789902   26.088672  0.0020  0.0001  \n",
       "22    2400  100  155.973188   26.107710  0.0020  0.0001  \n",
       "23    2400  100  154.747613   26.102819  0.0005  0.0001  \n",
       "24    2400  100  155.069272   28.767607  0.0005  0.0001  \n",
       "25    2400  100  155.616217   26.113737  0.0005  0.0001  \n",
       "26    2400  100  154.627858   26.179941  0.0005  0.0001  \n",
       "27    2400  100  155.360457   26.128672  0.0005  0.0001  \n",
       "28    2400  100  155.530201   27.551405  0.0005  0.0001  \n",
       "29    2400  100  154.486103   26.230717  0.0005  0.0001  \n",
       "30    2400  100  155.204885   26.453111  0.0005  0.0001  \n",
       "31    2400  100  155.244213   27.856316  0.0005  0.0001  \n",
       "32    2400  100  155.400090   26.334645  0.0002  0.0001  \n",
       "33    2400  100  155.614933   26.507466  0.0002  0.0001  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(PATH_WORK/'stats.f{}.v{}'.format(0,VERSION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0739044 , 0.00253213, 0.02575298, 0.01846603, 0.0246274 ,\n",
       "       0.03151604], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.mean((0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f43015e64a8>]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(predictions.mean(0)[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_one(bs = 100):\n",
    "    st = time.time()\n",
    "\n",
    "    cur_epoch = getCurrentBatch()\n",
    "    if cur_epoch is None: cur_epoch = 0\n",
    "    print('completed epochs:', cur_epoch)\n",
    "\n",
    "    model = TabularModel(n_cont = len(meta_cols), out_sz=360, layers=[500,200], ps=[0.5,0.5], bn_final=True)\n",
    "    \n",
    "    model_file_name = modelFileName(return_last=True)\n",
    "    if model_file_name is not None:\n",
    "        print('loading model', model_file_name)\n",
    "        state_dict = torch.load(PATH_WORK/'models'/model_file_name)\n",
    "        model.load_state_dict(state_dict)\n",
    "    \n",
    "    if (not CLOUD) or CLOUD_SINGLE:\n",
    "        model = model.to(device)\n",
    "    else:\n",
    "        model_parallel = dp.DataParallel(model, device_ids=devices)\n",
    "\n",
    "    setSeeds(SEED + cur_epoch)\n",
    "\n",
    "    tst_ds = RSNA_DataSet(test_md, test_ids_df, mode='test', bs=bs)\n",
    "    loader_tst = D.DataLoader(tst_ds, num_workers=8 if CLOUD else 0, batch_size=bs, shuffle=False)\n",
    "    print('dataset test:', len(tst_ds), 'loader test:', len(loader_tst))\n",
    "\n",
    "    if CLOUD and (not CLOUD_SINGLE):\n",
    "        results = model_parallel(test_loop_fn, loader_tst)\n",
    "        predictions = np.concatenate([results[i][0] for i in range(MAX_DEVICES)])\n",
    "        indices = np.concatenate([results[i][1] for i in range(MAX_DEVICES)])\n",
    "        offsets = np.concatenate([results[i][2] for i in range(MAX_DEVICES)])\n",
    "    else:\n",
    "        predictions, indices, offsets = test_loop_fn(model, loader_tst, device)\n",
    "\n",
    "    predictions = predictions[np.argsort(indices)]\n",
    "    offsets = offsets[np.argsort(indices)]\n",
    "    assert len(predictions) == len(test_md.SeriesInstanceUID.unique())\n",
    "    assert np.all(indices[np.argsort(indices)] == np.array(range(len(predictions))))\n",
    "    \n",
    "    print('test processing time:', time.time() - st)\n",
    "    \n",
    "    return predictions, offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epochs: 34\n",
      "loading model model.b34.f0.v21\n",
      "adding dummy serieses 186\n",
      "dataset test: 2400 loader test: 24\n",
      "test processing time: 30.99569296836853\n"
     ]
    }
   ],
   "source": [
    "DATA_SMALL = False\n",
    "predictions, offsets = inference_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07529006, 0.00286574, 0.02590009, 0.01699982, 0.02646616,\n",
       "       0.03229785], dtype=float32)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.mean((0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "for i, series_id in enumerate(test_md.SeriesInstanceUID.unique()):\n",
    "    df = test_md.loc[test_md.SeriesInstanceUID == series_id]\n",
    "    id_column = [a + '_' + b for a in df.SOPInstanceUID for b in all_ich]\n",
    "    assert (offsets[i] + len(df)) <= 60\n",
    "    data_sub = pd.DataFrame({'ID':np.array(id_column), \n",
    "                             'Label':predictions[i,offsets[i]:(offsets[i] + len(df))].reshape(-1)})\n",
    "    sub = pd.concat([sub,data_sub], axis=0, sort=False)\n",
    "\n",
    "sub = sub.reset_index(drop=True)\n",
    "\n",
    "assert len(sub) == 6*len(test_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.127288818359375"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.loc[range(0,len(sub),6), 'Label'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sub = pd.read_csv(PATH/'submission_061.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13475628267250275"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_sub.loc[range(0,len(sub),6), 'Label'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(PATH/'sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9829018879124783"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(sub.sort_values('ID').reset_index(drop=True).loc[range(0,len(sub),6), 'Label'], \n",
    "            best_sub.sort_values('ID').reset_index(drop=True).loc[range(0,len(sub),6), 'Label'])[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 16.9M/16.9M [00:05<00:00, 3.39MB/s]\n",
      "Successfully submitted to RSNA Intracranial Hemorrhage Detection"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit rsna-intracranial-hemorrhage-detection -f ~/sub.csv -m \"TPU, black im padding, more train\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
