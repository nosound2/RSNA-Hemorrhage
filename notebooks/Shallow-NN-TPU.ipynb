{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda activate torch-xla-nightly\n",
    "#export XRT_TPU_CONFIG=\"tpu_worker;0;$10.0.101.2:8470\"\n",
    "#git init\n",
    "#git remote add origin https://github.com/nosound2/RSNA-Hemorrhage\n",
    "#git pull origin master\n",
    "#git config remote.origin.push HEAD\n",
    "#gcloud config set compute/zone europe-west4-a\n",
    "#gcloud auth login\n",
    "#gcloud config set project endless-empire-239015\n",
    "#pip install kaggle\n",
    "#mkdir .kaggle\n",
    "#gsutil cp gs://recursion-double-strand/kaggle-keys/kaggle.json ~/.kaggle\n",
    "#chmod 600 /home/zahar_chikishev/.kaggle/kaggle.json\n",
    "#kaggle competitions download rsna-intracranial-hemorrhage-detection -f stage_1_train.csv\n",
    "#sudo apt install unzip\n",
    "#unzip stage_1_train.csv.zip\n",
    "#kaggle kernels output xhlulu/rsna-generate-metadata-csvs -p .\n",
    "#gsutil cp gs://rsna-hemorrhage/yuvals/* .\n",
    "\n",
    "#export XRT_TPU_CONFIG=\"tpu_worker;0;10.0.101.2:8470\"; conda activate torch-xla-nightly; jupyter notebook\n",
    "\n",
    "# 35.204.242.164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 76\n",
    "CLOUD_SINGLE = False\n",
    "MIXUP = False\n",
    "DATA_SET = 2\n",
    "NO_BLACK_LOSS = True\n",
    "DATA_SMALL = False\n",
    "\n",
    "if DATA_SET == 1:\n",
    "    dataset_name = 'Densenet161'\n",
    "    feat_sz = 2208\n",
    "elif DATA_SET == 2:\n",
    "    dataset_name = 'Densenet169'\n",
    "    filename_add = '_3'\n",
    "    filename_add2 = '_v3'\n",
    "    feat_sz = 208\n",
    "elif DATA_SET == 3:\n",
    "    dataset_name = 'Densenet201'\n",
    "    filename_add = '_3'\n",
    "    filename_add2 = '_v3'\n",
    "    feat_sz = 240\n",
    "elif DATA_SET == 4:\n",
    "    dataset_name = 'se_resnext101_32x4d'\n",
    "    filename_add = ''\n",
    "    filename_add2 = ''\n",
    "    feat_sz = 256\n",
    "elif DATA_SET == 5:\n",
    "    dataset_name = 'Densenet161'\n",
    "    filename_add = '_3'\n",
    "    filename_add2 = '_v4'\n",
    "    feat_sz = 552\n",
    "else: assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "from matplotlib import patches, patheffects\n",
    "import time\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error,log_loss,roc_auc_score\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "import pdb\n",
    "\n",
    "import scipy as sp\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "\n",
    "CLOUD = not torch.cuda.is_available()\n",
    "\n",
    "if not CLOUD:\n",
    "    torch.cuda.current_device()\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as D\n",
    "import torch.nn.functional as F\n",
    "import torch.utils as U\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "from torchvision import models as M\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if CLOUD:\n",
    "    PATH = Path('/home/zahar_chikishev')\n",
    "    PATH_WORK = Path('/home/zahar_chikishev/running')\n",
    "else:\n",
    "    PATH = Path('C:/StudioProjects/Hemorrhage')\n",
    "    PATH_WORK = Path('C:/StudioProjects/Hemorrhage/running')\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import seaborn as sn\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "\n",
    "all_ich = ['any','epidural','intraparenchymal','intraventricular','subarachnoid','subdural']\n",
    "class_weights = 6.0*np.array([2,1,1,1,1,1])/7.0\n",
    "\n",
    "if CLOUD:\n",
    "    import torch_xla\n",
    "    import torch_xla.distributed.data_parallel as dp\n",
    "    import torch_xla.utils as xu\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    \n",
    "    from typing import Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_black = '006d4432e'\n",
    "all_black = '00bd6c59c'\n",
    "\n",
    "if CLOUD:\n",
    "    device = xm.xla_device()\n",
    "    #device = 'cpu'\n",
    "    MAX_DEVICES = 1 if CLOUD_SINGLE else 8\n",
    "    bs = 32\n",
    "else:\n",
    "    device = 'cuda'\n",
    "    #device = 'cpu'\n",
    "    MAX_DEVICES = 1\n",
    "    bs = 16\n",
    "\n",
    "if CLOUD and (not CLOUD_SINGLE):\n",
    "    devices = xm.get_xla_supported_devices(max_devices=MAX_DEVICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2351\n",
    "\n",
    "def setSeeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "setSeeds(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_le,cols_float,cols_bool = pickle.load(open(PATH_WORK/'covs','rb'))\n",
    "meta_cols = cols_bool + cols_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#meta_cols = ['ImagePositionPatient_1','pos_rel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_SET == 1:\n",
    "    if False:\n",
    "        filename = PATH_WORK/'indexes_file.pkl'\n",
    "        all_idx, train_ids, val_ids = pickle.load(open(filename,'rb'))\n",
    "\n",
    "        train_md = pd.read_csv(PATH_WORK/'train_md.csv').sort_values(['SeriesInstanceUID','pos_idx1'])\n",
    "        train_md['img_id'] = train_md.SOPInstanceUID.str.split('_').apply(lambda x: x[1])\n",
    "\n",
    "        ids_df = pd.DataFrame(all_idx, columns = ['img_id'])\n",
    "        ids_df = ids_df.join(train_md.set_index('img_id'), on = 'img_id')\n",
    "        \n",
    "        assert len(ids_df.SeriesInstanceUID.unique()) == 19530\n",
    "        \n",
    "        trn_data = ids_df.loc[ids_df.img_id.isin(all_idx[train_ids])].reset_index(drop=True)\n",
    "        val_data = ids_df.loc[ids_df.img_id.isin(all_idx[val_ids])].reset_index(drop=True)\n",
    "\n",
    "        assert len(trn_data.SeriesInstanceUID.unique()) + len(val_data.SeriesInstanceUID.unique()) \\\n",
    "            == len(train_md.SeriesInstanceUID.unique())\n",
    "\n",
    "        assert len(trn_data.PatientID.unique()) + len(val_data.PatientID.unique()) \\\n",
    "            >= len(train_md.PatientID.unique())\n",
    "\n",
    "        pickle.dump((trn_data,val_data), open(PATH_WORK/'train.post.processed.1','wb'))\n",
    "    else:\n",
    "        trn_data,val_data = pickle.load(open(PATH_WORK/'train.post.processed.1','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_SET == 1:\n",
    "    if False:\n",
    "        test_md = pd.read_csv(PATH_WORK/'test_md.csv').sort_values(['SeriesInstanceUID','pos_idx1'])\n",
    "        test_md['img_id'] = test_md.SOPInstanceUID.str.split('_').apply(lambda x: x[1])\n",
    "\n",
    "        filename = PATH_WORK/'test_indexes.pkl'\n",
    "        test_ids = pickle.load(open(filename,'rb'))\n",
    "\n",
    "        test_ids_df = pd.DataFrame(test_ids, columns = ['img_id'])\n",
    "        test_md = test_ids_df.join(test_md.set_index('img_id'), on = 'img_id')\n",
    "\n",
    "        assert len(test_md.SeriesInstanceUID.unique()) == 2214\n",
    "\n",
    "        pickle.dump(test_md, open(PATH_WORK/'test.post.processed.1','wb'))\n",
    "    else:\n",
    "        test_md = pickle.load(open(PATH_WORK/'test.post.processed.1','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    print(pd.concat([test_md[meta_cols].mean(0),\n",
    "                     trn_data[meta_cols].mean(0),\n",
    "                     val_data[meta_cols].mean(0)], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. \n",
    "model_Densenet161_3_vehrsion_basic_classifier_type_features_train_split_2.pkl\n",
    "10/7/19, 4:14:13 PM UTC+3\t\n",
    "model_Densenet161_3_vehrsion_basic_classifier_type_features_test_split_2.pkl\n",
    "10/7/19, 5:05:17 PM UTC+3\t\n",
    "indexes_file.pkl\n",
    "10/7/19, 5:34:42 PM UTC+3\t\n",
    "test_indexes.pkl \n",
    "10/9/19, 6:36:35 PM UTC+3\t\n",
    "\n",
    "2. \n",
    "model_Densenet169_3_vehrsion_basic_classifier_wso_type_features_test_tta_split_2.pkl\n",
    "10/10/19, 6:31:59 PM UTC+3\t\n",
    "model_Densenet169_3_vehrsion_basic_classifier_wso_type_features_train_tta_split_2.pkl\n",
    "10/10/19, 5:56:16 PM UTC+3\t\n",
    "\n",
    "model_Densenet161_3_vehrsion_basic_classifier_wso2_type_features_test_tta_split_2.pkl\n",
    "10/10/19, 3:07:20 PM UTC+3\t\n",
    "model_Densenet161_3_vehrsion_basic_classifier_wso2_type_features_train_tta_split_2.pkl\n",
    "10/10/19, 1:36:36 PM UTC+3\t\n",
    "\n",
    "4.\n",
    "model_Densenet161_3_version_classifier_splits_type_features_test_split_0.pkl\n",
    "10/14/19, 12:53:08 PM UTC+3\t\n",
    "model_Densenet161_3_version_classifier_splits_type_features_test_split_1.pkl\n",
    "10/14/19, 2:09:27 PM UTC+3\t\n",
    "model_Densenet161_3_version_classifier_splits_type_features_test_split_2.pkl\n",
    "10/14/19, 3:17:16 PM UTC+3\t\n",
    "\n",
    "5.\n",
    "train_dedup.csv\n",
    "10/14/19, 11:39:18 AM UTC+3\t\n",
    "model_Densenet169_3_version_classifier_splits_type_features_train_tta_split_2.pkl\n",
    "10/13/19, 2:43:34 PM UTC+3\t\n",
    "model_Densenet169_3_version_classifier_splits_type_features_train_tta_split_0.pkl\n",
    "10/14/19, 6:09:28 PM UTC+3\t\n",
    "model_Densenet169_3_version_classifier_splits_type_features_train_tta_split_1.pkl\n",
    "10/14/19, 8:09:27 PM UTC+3\t\n",
    "PID_splits.pkl\n",
    "10/14/19, 11:34:06 AM UTC+3\t\n",
    "model_Densenet169_3_version_classifier_splits_type_features_test_split_0.pkl\n",
    "10/13/19, 3:34:21 PM UTC+3\t\n",
    "model_Densenet169_3_version_classifier_splits_type_features_test_split_1.pkl\n",
    "10/13/19, 4:36:25 PM UTC+3\t\n",
    "model_Densenet169_3_version_classifier_splits_type_features_test_split_2.pkl\n",
    "10/13/19, 4:05:32 PM UTC+3\t\n",
    "\n",
    "\n",
    "I finished uploading all densenet 169 folds for test and train.\n",
    "(Be aware, fold 0 gives worse scores then 2).\n",
    "I uploaded the train dataset I used (without duplicates) it is called 'train_dedup'\n",
    "Also loaded the splits data into PID_split.\n",
    "It is a tuple. The first variable is a numpy array with the unique PIDs in train_df.\n",
    "The 2nd variable is a double list, with the indices of the train and validation for the 3 splits. \n",
    "The indices are for the PID numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_SET >= 2:\n",
    "    if False:\n",
    "        train_dedup = pd.read_csv(PATH_WORK/'yuval'/'train_dedup.csv')\n",
    "        pids, folding = pickle.load(open(PATH_WORK/'yuval'/'PID_splits.pkl','rb'))\n",
    "\n",
    "        assert len(pids) == 17079\n",
    "        assert len(np.unique(pids)) == 17079\n",
    "\n",
    "        for fol in folding:\n",
    "            assert len(fol[0]) + len(fol[1]) == 17079\n",
    "\n",
    "        assert len(folding[0][1]) + len(folding[1][1]) + len(folding[2][1]) == 17079\n",
    "\n",
    "        assert len(train_dedup.PID.unique()) == 17079\n",
    "\n",
    "        train_dedup['fold'] = np.nan\n",
    "\n",
    "        for fold in range(3):\n",
    "            train_dedup.loc[train_dedup.PID.isin(pids[folding[fold][1]]),'fold'] = fold\n",
    "\n",
    "        assert train_dedup.fold.isnull().sum() == 0\n",
    "\n",
    "        train_md = pd.read_csv(PATH_WORK/'train_md.csv').sort_values(['SeriesInstanceUID','pos_idx1'])\n",
    "        train_md['img_id'] = train_md.SOPInstanceUID.str.split('_').apply(lambda x: x[1])\n",
    "\n",
    "        ids_df = train_dedup[['fold','PatientID']]\n",
    "        ids_df.columns = ['fold','img_id']\n",
    "\n",
    "        ids_df = ids_df.join(train_md.set_index('img_id'), on = 'img_id')\n",
    "\n",
    "        pickle.dump(ids_df, open(PATH_WORK/'features/{}{}/train/train.ids.df'.format(dataset_name, filename_add2),'wb'))\n",
    "\n",
    "        #test_md = pickle.load(open(PATH_WORK/'test.post.processed.1','rb'))\n",
    "        test_md = pd.read_csv(PATH_WORK/'test_md.csv').sort_values(['SeriesInstanceUID','pos_idx1'])\n",
    "        test_md['img_id'] = test_md.SOPInstanceUID.str.split('_').apply(lambda x: x[1])\n",
    "        \n",
    "        filename = PATH_WORK/'test_indexes.pkl'\n",
    "        test_ids = pickle.load(open(filename,'rb'))\n",
    "\n",
    "        test_ids_df = pd.DataFrame(test_ids, columns = ['img_id'])\n",
    "        test_md = test_ids_df.join(test_md.set_index('img_id'), on = 'img_id')\n",
    "        \n",
    "        assert len(test_md.SeriesInstanceUID.unique()) == 2214\n",
    "        \n",
    "        pickle.dump(test_md, open(PATH_WORK/'features/{}{}/test/test.ids.df'.format(dataset_name,filename_add2),'wb'))\n",
    "\n",
    "        for fold in range(3):\n",
    "            filename = PATH_WORK/'yuval'/\\\n",
    "                'model_{}{}_version_classifier_splits_type_features_train_tta_split_{}.pkl'\\\n",
    "                .format(dataset_name, filename_add, fold)\n",
    "            feats = pickle.load(open(filename,'rb'))\n",
    "            print('feats size', feats.shape)\n",
    "            assert len(feats) == 4*len(ids_df)\n",
    "            means = feats.mean(0,keepdim=True)\n",
    "            stds = feats.std(0,keepdim=True)\n",
    "            \n",
    "            feats = feats - means\n",
    "            feats = torch.where(stds > 0, feats/stds, feats)\n",
    "\n",
    "            for i in range(4):\n",
    "                feats_sub1 = feats[torch.BoolTensor(np.arange(len(feats))%4 == i)]\n",
    "                feats_sub2 = feats_sub1[torch.BoolTensor(train_dedup.fold != fold)]\n",
    "                pickle.dump(feats_sub2, open(PATH_WORK/'features/{}{}/train/train.f{}.a{}'\n",
    "                                             .format(dataset_name,filename_add2,fold,i),'wb'))\n",
    "\n",
    "                feats_sub2 = feats_sub1[torch.BoolTensor(train_dedup.fold == fold)]\n",
    "                pickle.dump(feats_sub2, open(PATH_WORK/'features/{}{}/train/valid.f{}.a{}'\n",
    "                                             .format(dataset_name,filename_add2,fold,i),'wb'))\n",
    "\n",
    "                if i==0:\n",
    "                    black_feats = feats_sub1[torch.BoolTensor(ids_df.img_id == all_black)].squeeze()\n",
    "                    pickle.dump(black_feats, open(PATH_WORK/'features/{}{}/train/black.f{}'\n",
    "                                                  .format(dataset_name,filename_add2,fold),'wb'))\n",
    "\n",
    "            filename = PATH_WORK/'yuval'/\\\n",
    "                'model_{}{}_version_classifier_splits_type_features_test_split_{}.pkl'\\\n",
    "                .format(dataset_name,filename_add,fold)\n",
    "            feats = pickle.load(open(filename,'rb'))\n",
    "            \n",
    "            feats = feats - means\n",
    "            feats = torch.where(stds > 0, feats/stds, feats)\n",
    "            \n",
    "            for i in range(8):\n",
    "                feats_sub = feats[torch.BoolTensor(np.arange(len(feats))%8 == i)]\n",
    "                pickle.dump(feats_sub, open(PATH_WORK/'features/{}{}/test/test.f{}.a{}'\n",
    "                                            .format(dataset_name,filename_add2,fold,i),'wb'))\n",
    "                assert len(feats_sub) == len(test_md)\n",
    "    else:\n",
    "        ids_df = pickle.load(open(PATH_WORK/'features/{}{}/train/train.ids.df'.format(dataset_name,filename_add2),'rb'))\n",
    "        test_md = pickle.load(open(PATH_WORK/'features/{}{}/test/test.ids.df'.format(dataset_name,filename_add2),'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_SET == 1:\n",
    "    if False:\n",
    "        filename = PATH_WORK/'model_Densenet161_3_vehrsion_basic_classifier_type_features_train_split_2.pkl'\n",
    "        feats = pickle.load(open(filename,'rb'))\n",
    "\n",
    "        for series_id in tqdm(ids_df.SeriesInstanceUID.unique()):\n",
    "            mask = torch.BoolTensor(ids_df.SeriesInstanceUID.values == series_id)\n",
    "            feats_id = feats[mask]\n",
    "            pickle.dump(feats_id, open(PATH_WORK/'features/densenet161_v3/train/{}'.format(series_id),'wb'))\n",
    "\n",
    "\n",
    "        filename = PATH_WORK/'model_Densenet161_3_vehrsion_basic_classifier_type_features_test_split_2.pkl'\n",
    "        feats = pickle.load(open(filename,'rb'))\n",
    "\n",
    "        for series_id in tqdm(test_md.SeriesInstanceUID.unique()):\n",
    "            mask = torch.BoolTensor(test_md.SeriesInstanceUID.values == series_id)\n",
    "            feats_id = feats[mask]\n",
    "            pickle.dump(feats_id, open(PATH_WORK/'features/densenet161_v3/test/{}'.format(series_id),'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = PATH_WORK/'features/densenet161_v3/train/ID_000a935543'\n",
    "#feats1 = pickle.load(open(path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ff = pickle.load(open(PATH_WORK/'features/{}{}/{}/{}.f{}.a{}'\\\n",
    "#                .format(dataset_name,filename_add2,'train','train',0,0),'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_SET == 1:\n",
    "    path = PATH_WORK/'features/densenet161_v3/train/ID_992b567eb6'\n",
    "    black_feats = pickle.load(open(path,'rb'))[41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSNA_DataSet(D.Dataset):\n",
    "    def __init__(self, metadata, mode='train', bs=None, dataset=DATA_SET, fold=0):\n",
    "        \n",
    "        super(RSNA_DataSet, self).__init__()\n",
    "        \n",
    "        if dataset == 1:\n",
    "            md = metadata.copy()\n",
    "            md = md.reset_index(drop=True)\n",
    "        else:\n",
    "            if mode == 'train':\n",
    "                md = metadata.loc[metadata.fold != fold].copy().reset_index(drop=True)\n",
    "            elif mode == 'valid':\n",
    "                md = metadata.loc[metadata.fold == fold].copy().reset_index(drop=True)\n",
    "            else:\n",
    "                md = metadata.copy().reset_index(drop=True)\n",
    "        \n",
    "        series = np.sort(md.SeriesInstanceUID.unique())\n",
    "        md = md.set_index('SeriesInstanceUID', drop=True)\n",
    "        \n",
    "        samples_add = 0\n",
    "        if (mode != 'train') and not DATA_SMALL:\n",
    "            batch_num = -((-len(series))//(bs*MAX_DEVICES))\n",
    "            samples_add = batch_num*bs*MAX_DEVICES - len(series)\n",
    "            print('adding dummy serieses', samples_add)\n",
    "        \n",
    "        #self.records = df.to_records(index=False)\n",
    "        self.mode = mode\n",
    "        self.real = np.concatenate([np.repeat(True,len(series)),np.repeat(False,samples_add)])\n",
    "        self.series = np.concatenate([series, random.sample(list(series),samples_add)])\n",
    "        self.metadata = md\n",
    "        self.dataset = dataset\n",
    "        self.fold = fold\n",
    "        \n",
    "        print('DataSet', dataset, mode, 'size', len(self.series), 'fold', fold)\n",
    "        \n",
    "        if self.dataset >= 2:\n",
    "            path = PATH_WORK/'features/{}{}/train/black.f{}'.format(dataset_name, filename_add2, fold)\n",
    "            self.black_feats = pickle.load(open(path,'rb')).squeeze()\n",
    "        \n",
    "        elif self.dataset == 1:\n",
    "            self.black_feats = black_feats\n",
    "    \n",
    "    def setFeats(self, anum):\n",
    "        def getAPath(an):\n",
    "            return PATH_WORK/'features/{}{}/{}/{}.f{}.a{}'\\\n",
    "                .format(dataset_name,filename_add2,folder,self.mode,self.fold,an)\n",
    "        \n",
    "        if self.dataset == 1: return\n",
    "        print('setFeats, augmentation', anum)\n",
    "        self.anum = anum\n",
    "        folder = 'test' if self.mode == 'test' else 'train'\n",
    "        sz = len(self.metadata)\n",
    "        if anum == -1:\n",
    "            max_a = 8 if self.mode == 'test' else 4\n",
    "            feats2 = torch.stack([pickle.load(open(getAPath(an),'rb')) for an in range(max_a)])\n",
    "            feats = feats2[torch.randint(max_a,(sz,)), torch.arange(sz)].squeeze()\n",
    "        else:\n",
    "            feats = pickle.load(open(getAPath(anum),'rb'))\n",
    "        \n",
    "        self.feats = feats\n",
    "        assert len(feats) == sz\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        series_id = self.series[index]\n",
    "        #df = self.metadata.loc[self.metadata.SeriesInstanceUID == series_id].reset_index(drop=True)\n",
    "        df = self.metadata.loc[series_id].reset_index(drop=True)\n",
    "        \n",
    "        if self.dataset == 1:\n",
    "            folder = 'test' if self.mode == 'test' else 'train'\n",
    "            path = PATH_WORK/'features/densenet161_v3/{}/{}'.format(folder,series_id)\n",
    "            feats = pickle.load(open(path,'rb'))\n",
    "            \n",
    "            if feats.shape[0] > len(df.img_id.unique()):\n",
    "                mask_dup = ~df.img_id.duplicated().values\n",
    "                df = df.loc[mask_dup]\n",
    "                feats = feats[torch.BoolTensor(mask_dup)]\n",
    "            \n",
    "            assert feats.shape[0] == len(df)\n",
    "        elif self.dataset >= 2:\n",
    "            feats = self.feats[torch.BoolTensor(self.metadata.index.values == series_id)]\n",
    "        else: assert False\n",
    "        \n",
    "        order = np.argsort(df.pos_idx1.values)\n",
    "        df = df.sort_values(['pos_idx1'])\n",
    "        feats = feats[torch.LongTensor(order)]\n",
    "        \n",
    "        non_black = torch.ones(len(feats))\n",
    "        feats = torch.cat([feats, torch.Tensor(df[meta_cols].values)], dim=1)\n",
    "        feats_le = torch.LongTensor(df[cols_le].values)\n",
    "        \n",
    "        target = torch.Tensor(df[all_ich].values)\n",
    "        \n",
    "        #PAD = 4+9+1\n",
    "        PAD = 8\n",
    "        \n",
    "        offset = np.random.randint(0, 61 - feats.shape[0])\n",
    "        #offset = 0\n",
    "        top_pad = PAD + offset\n",
    "        if top_pad > 0:\n",
    "            dummy_row = torch.cat([self.black_feats, torch.Tensor(df.head(1)[meta_cols].values).squeeze()])\n",
    "            feats = torch.cat([dummy_row.repeat(top_pad,1), feats], dim=0)\n",
    "            feats_le = torch.cat([torch.LongTensor(df.head(1)[cols_le].values).squeeze().repeat(top_pad,1), feats_le])\n",
    "            if offset > 0:\n",
    "                non_black = torch.cat([0.01 + torch.zeros(offset), non_black])\n",
    "                target = torch.cat([torch.zeros((offset, len(all_ich))), target], dim=0)\n",
    "        bot_pad = 60 - len(df) - offset + PAD\n",
    "        if bot_pad > 0:\n",
    "            dummy_row = torch.cat([self.black_feats, torch.Tensor(df.tail(1)[meta_cols].values).squeeze()])\n",
    "            feats = torch.cat([feats, dummy_row.repeat(bot_pad,1)], dim=0)\n",
    "            feats_le = torch.cat([feats_le, torch.LongTensor(df.tail(1)[cols_le].values).squeeze().repeat(bot_pad,1)])\n",
    "            if (60 - len(df) - offset) > 0:\n",
    "                non_black = torch.cat([non_black, 0.01 + torch.zeros(60 - len(df) - offset)])\n",
    "                target = torch.cat([target, torch.zeros((60 - len(df) - offset, len(all_ich)))], dim=0)\n",
    "        \n",
    "        assert feats_le.shape[0] == (60 + 2*PAD)\n",
    "        assert feats.shape[0] == (60 + 2*PAD)\n",
    "        assert target.shape[0] == 60\n",
    "        \n",
    "        #feats = feats.transpose(1,0)\n",
    "        \n",
    "        idx = index\n",
    "        if not self.real[index]: idx = -1\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            return feats, feats_le, target, non_black\n",
    "        else:\n",
    "            return feats, feats_le, target, idx, offset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.series) if not DATA_SMALL else int(0.01*len(self.series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCurrentBatch(fold=0):\n",
    "    sel_batch = None\n",
    "    for filename in os.listdir(PATH_WORK/'models'):\n",
    "        splits = filename.split('.')\n",
    "        if int(splits[2][1]) != fold: continue\n",
    "        if int(splits[3][1:]) != VERSION: continue\n",
    "        if sel_batch is None:\n",
    "            sel_batch = int(splits[1][1:])\n",
    "        else:\n",
    "            sel_batch = max(sel_batch, int(splits[1][1:]))\n",
    "    return sel_batch\n",
    "\n",
    "def modelFileName(fold=0, batch = 1, return_last = False, return_next = False):\n",
    "    sel_batch = batch\n",
    "    if return_last or return_next:\n",
    "        sel_batch = getCurrentBatch(fold)\n",
    "        if return_last and sel_batch is None:\n",
    "            return None\n",
    "        if return_next:\n",
    "            if sel_batch is None: sel_batch = 1\n",
    "            else: sel_batch += 1\n",
    "    \n",
    "    return 'model.b{}.f{}.v{}'.format(sel_batch, fold, VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCEWithLogitsLoss(nn.Module):\n",
    "    def __init__(self, weight=None):\n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "    \n",
    "    def forward(self, input, target, batch_weights = None):\n",
    "        loss = (torch.log(1+torch.exp(input)) - target*input)*self.weight\n",
    "        if batch_weights is not None:\n",
    "            loss = batch_weights*loss\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_drop_lin(n_in:int, n_out:int, bn:bool=True, p:float=0., actn=None):\n",
    "    \"Sequence of batchnorm (if `bn`), dropout (with `p`) and linear (`n_in`,`n_out`) layers followed by `actn`.\"\n",
    "    layers = [nn.BatchNorm1d(n_in)] if bn else []\n",
    "    if p != 0: layers.append(nn.Dropout(p))\n",
    "    layers.append(nn.Linear(n_in, n_out))\n",
    "    if actn is not None: layers.append(actn)\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def noop(x): return x\n",
    "act_fun = nn.ReLU(inplace=True)\n",
    "\n",
    "def conv_layer(ni, nf, ks=3, act=True):\n",
    "    bn = nn.BatchNorm1d(nf)\n",
    "    layers = [nn.Conv1d(ni, nf, ks), bn]\n",
    "    if act: layers.append(act_fun)\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, ni, nh):\n",
    "        super().__init__()\n",
    "        layers  = [conv_layer(ni, nh, 1),\n",
    "                   conv_layer(nh, nh, 5, act=False)]\n",
    "        self.convs = nn.Sequential(*layers)\n",
    "        self.idconv = noop if (ni == nh) else conv_layer(ni, nh, 1, act=False)\n",
    "    \n",
    "    def forward(self, x): return act_fun(self.convs(x) + self.idconv(x[:,:,2:-2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetModel(nn.Module):\n",
    "    def __init__(self, n_cont:int, feat_sz=2208):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.le_sz = 9\n",
    "        le_in_sizes = np.array([5,5,7,4,4,11,4,6,3])\n",
    "        le_out_sizes = np.array([3,3,4,2,2,6,2,4,2])\n",
    "        le_out_sz = le_out_sizes.sum()\n",
    "        self.embeddings = nn.ModuleList([embedding(le_in_sizes[i], le_out_sizes[i]) for i in range(self.le_sz)])\n",
    "\n",
    "        self.feat_sz = feat_sz\n",
    "        \n",
    "        self.n_cont = n_cont\n",
    "        \n",
    "        self.conv2D = nn.Conv2d(1,64,(feat_sz + n_cont + le_out_sz,1))\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        \n",
    "        self.res1 = ResBlock(64,64)\n",
    "        self.res2 = ResBlock(64,64)\n",
    "        self.res3 = ResBlock(64,64)\n",
    "        self.res4 = ResBlock(64,64)\n",
    "        \n",
    "        self.conv1D = nn.Conv1d(128,6,1)\n",
    "    \n",
    "    def forward(self, x, x_le, x_le_mix = None, lambd = None) -> torch.Tensor:\n",
    "        x_le = [e(x_le[:,:,i]) for i,e in enumerate(self.embeddings)]\n",
    "        x_le = torch.cat(x_le, 2)\n",
    "        \n",
    "        x = torch.cat([x, x_le], 2)\n",
    "        x = x.transpose(1,2)\n",
    "        \n",
    "        #x = torch.cat([x[:,:self.feat_sz],self.bn_cont(x[:,self.feat_sz:])], dim=1)\n",
    "        x = x.reshape(x.shape[0],1,x.shape[1],x.shape[2])\n",
    "        \n",
    "        x = self.conv2D(x).squeeze()\n",
    "        x = self.bn1(x)\n",
    "        x = act_fun(x)\n",
    "        \n",
    "        x2 = self.res1(x)\n",
    "        x2 = self.res2(x2)\n",
    "        x2 = self.res3(x2)\n",
    "        x2 = self.res4(x2)\n",
    "        \n",
    "        x = torch.cat([x[:,:,8:-8], x2], 1)\n",
    "        x = self.conv1D(x)\n",
    "        x = x.transpose(1,2)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trunc_normal_(x, mean:float=0., std:float=1.):\n",
    "    \"Truncated normal initialization.\"\n",
    "    # From https://discuss.pytorch.org/t/implementing-truncated-normal-initializer/4778/12\n",
    "    return x.normal_().fmod_(2).mul_(std).add_(mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding(ni:int,nf:int) -> nn.Module:\n",
    "    \"Create an embedding layer.\"\n",
    "    emb = nn.Embedding(ni, nf)\n",
    "    # See https://arxiv.org/abs/1711.09160\n",
    "    with torch.no_grad(): trunc_normal_(emb.weight, std=0.01)\n",
    "    return emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularModel(nn.Module):\n",
    "    \"Basic model for tabular data.\"\n",
    "    def __init__(self, n_cont:int, feat_sz=2208, fc_drop_p=0.3):\n",
    "        super().__init__()\n",
    "        self.le_sz = 9\n",
    "        le_in_sizes = np.array([5,5,7,4,4,11,4,6,3])\n",
    "        le_out_sizes = np.array([3,3,4,2,2,6,2,4,2])\n",
    "        le_out_sz = le_out_sizes.sum()\n",
    "        self.embeddings = nn.ModuleList([embedding(le_in_sizes[i], le_out_sizes[i]) for i in range(self.le_sz)])\n",
    "        #self.bn_cont = nn.BatchNorm1d(feat_sz + n_cont)\n",
    "        \n",
    "        self.feat_sz = feat_sz\n",
    "        self.bn_cont = nn.BatchNorm1d(n_cont + le_out_sz)\n",
    "        self.n_cont = n_cont\n",
    "        self.fc_drop = nn.Dropout(p=fc_drop_p)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        scale = 4\n",
    "        \n",
    "        self.conv2D_1 = nn.Conv2d(1,16*scale,(feat_sz + n_cont + le_out_sz,1))\n",
    "        self.conv2D_2 = nn.Conv2d(1,16*scale,(feat_sz + n_cont + le_out_sz,5))\n",
    "        self.bn_cont1 = nn.BatchNorm1d(32*scale)\n",
    "        self.conv1D_1 = nn.Conv1d(32*scale,16*scale,3)\n",
    "        self.conv1D_3 = nn.Conv1d(32*scale,16*scale,5,dilation=5)\n",
    "        self.conv1D_2 = nn.Conv1d(32*scale,6,3)\n",
    "        self.bn_cont2 = nn.BatchNorm1d(32*scale)\n",
    "        self.bn_cont3 = nn.BatchNorm1d(6)\n",
    "\n",
    "        self.conv1D_4 = nn.Conv1d(32*scale,32*scale,3)\n",
    "        self.bn_cont4 = nn.BatchNorm1d(32*scale)\n",
    "\n",
    "    def forward(self, x, x_le, x_le_mix = None, lambd = None) -> torch.Tensor:\n",
    "        x_le = [e(x_le[:,:,i]) for i,e in enumerate(self.embeddings)]\n",
    "        x_le = torch.cat(x_le, 2)\n",
    "        \n",
    "        if MIXUP and x_le_mix is not None:\n",
    "            x_le_mix = [e(x_le_mix[:,:,i]) for i,e in enumerate(self.embeddings)]\n",
    "            x_le_mix = torch.cat(x_le_mix, 2)\n",
    "            x_le = lambd * x_le + (1-lambd) * x_le_mix\n",
    "        \n",
    "        #assert torch.isnan(x_le).any().cpu() == False\n",
    "        x = torch.cat([x, x_le], 2)\n",
    "        x = x.transpose(1,2)\n",
    "        \n",
    "        #x = torch.cat([x[:,:self.feat_sz],self.bn_cont(x[:,self.feat_sz:])], dim=1)\n",
    "        \n",
    "        x = x.reshape(x.shape[0],1,x.shape[1],x.shape[2])\n",
    "        x = self.fc_drop(x)\n",
    "        x = torch.cat([self.conv2D_1(x[:,:,:,2:(-2)]).squeeze(), \n",
    "                       self.conv2D_2(x).squeeze()], dim=1)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn_cont1(x)\n",
    "        x = self.fc_drop(x)\n",
    "        \n",
    "        x = torch.cat([self.conv1D_1(x[:,:,9:(-9)]),\n",
    "                       self.conv1D_3(x)], dim=1)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn_cont2(x)\n",
    "        x = self.fc_drop(x)\n",
    "        \n",
    "        x = self.conv1D_4(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.bn_cont4(x)\n",
    "        \n",
    "        x = self.conv1D_2(x)\n",
    "        x = x.transpose(1,2)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_fn(model, loader, device, context = None):\n",
    "    \n",
    "    if CLOUD and (not CLOUD_SINGLE):\n",
    "        tlen = len(loader._loader._loader)\n",
    "        OUT_LOSS = 1000\n",
    "        OUT_TIME = 4\n",
    "        generator = loader\n",
    "        device_num = int(str(device)[-1])\n",
    "        dataset = loader._loader._loader.dataset\n",
    "    else:\n",
    "        tlen = len(loader)\n",
    "        OUT_LOSS = 10\n",
    "        OUT_TIME = 1\n",
    "        generator = enumerate(loader)\n",
    "        device_num = 1\n",
    "        dataset = loader.dataset\n",
    "    \n",
    "    #print('Start training {}'.format(device), 'batches', tlen)\n",
    "    \n",
    "    criterion = BCEWithLogitsLoss(weight = torch.Tensor(class_weights).to(device))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(0.9, 0.99))\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    if CLOUD:\n",
    "        tracker = xm.RateTracker()\n",
    "\n",
    "    tloss = 0\n",
    "    tloss_count = 0\n",
    "    \n",
    "    st = time.time()\n",
    "    mixup_collected = False\n",
    "    x_le_mix = None\n",
    "    lambd = None\n",
    "    for i, (x, x_le, y, non_black) in generator:\n",
    "        if (not CLOUD) or CLOUD_SINGLE:\n",
    "            x = x.to(device)\n",
    "            x_le = x_le.to(device)\n",
    "            y = y.to(device)\n",
    "            non_black = non_black.to(device)\n",
    "        \n",
    "        if MIXUP:\n",
    "            if mixup_collected:\n",
    "                lambd = np.random.beta(0.4, 0.4, y.size(0))\n",
    "                lambd = torch.Tensor(lambd).to(device)[:,None,None]\n",
    "                #shuffle = torch.randperm(y.size(0)).to(device)\n",
    "                x = lambd * x + (1-lambd) * x_mix #x[shuffle]\n",
    "                #x_le = lambd * x_le + (1-lambd) * x_le_mix #x[shuffle]\n",
    "                mixup_collected = False\n",
    "            else:\n",
    "                x_mix = x\n",
    "                x_le_mix = x_le\n",
    "                y_mix = y\n",
    "                mixup_collected = True\n",
    "                continue\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(x, x_le, x_le_mix, lambd)\n",
    "        \n",
    "        if MIXUP:\n",
    "            if NO_BLACK_LOSS:\n",
    "                loss = criterion(output, y, lambd*non_black[:,:,None]) \\\n",
    "                     + criterion(output, y_mix, (1-lambd)*non_black[:,:,None])\n",
    "            else:\n",
    "                loss = criterion(output, y, lambd) + criterion(output, y_mix, 1-lambd)\n",
    "            del x_mix, y_mix\n",
    "        else:\n",
    "            if NO_BLACK_LOSS:\n",
    "                loss = criterion(output, y, non_black[:,:,None])\n",
    "            else:\n",
    "                loss = criterion(output, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        tloss += len(y)*loss.cpu().detach().item()\n",
    "        tloss_count += len(y)\n",
    "        \n",
    "        if CLOUD or CLOUD_SINGLE:\n",
    "            xm.optimizer_step(optimizer)\n",
    "            if CLOUD_SINGLE:\n",
    "                xm.mark_step()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        \n",
    "        if CLOUD:\n",
    "            tracker.add(len(y))\n",
    "        \n",
    "        st_passed = time.time() - st\n",
    "        if (i+1)%OUT_TIME == 0 and device_num == 1:\n",
    "            #print(torch_xla._XLAC._xla_metrics_report())\n",
    "            print('Batch {} device: {} time passed: {:.3f} time per batch: {:.3f}'\n",
    "                .format(i+1, device, st_passed, st_passed/(i+1)))\n",
    "        \n",
    "        del loss, output, y, x, x_le\n",
    "    \n",
    "    return tloss, tloss_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def val_loop_fn(model, loader, device, context = None):\n",
    "    \n",
    "    if CLOUD and (not CLOUD_SINGLE):\n",
    "        tlen = len(loader._loader._loader)\n",
    "        OUT_LOSS = 1000\n",
    "        OUT_TIME = 4\n",
    "        generator = loader\n",
    "        device_num = int(str(device)[-1])\n",
    "    else:\n",
    "        tlen = len(loader)\n",
    "        OUT_LOSS = 1000\n",
    "        OUT_TIME = 10\n",
    "        generator = enumerate(loader)\n",
    "        device_num = 1\n",
    "    \n",
    "    #print('Start validating {}'.format(device), 'batches', tlen)\n",
    "    \n",
    "    st = time.time()\n",
    "    model.eval()\n",
    "    \n",
    "    results = []\n",
    "    indices = []\n",
    "    offsets = []\n",
    "    \n",
    "    for i, (x, x_le, y, idx, offset) in generator:\n",
    "        \n",
    "        if (not CLOUD) or CLOUD_SINGLE:\n",
    "            x = x.to(device)\n",
    "            x_le = x_le.to(device)\n",
    "        \n",
    "        output = model(x, x_le)\n",
    "        assert torch.isnan(output).any().cpu() == False\n",
    "        output = torch.sigmoid(output)\n",
    "        assert torch.isnan(output).any().cpu() == False\n",
    "        \n",
    "        mask = (idx >= 0)\n",
    "        results.append(output[mask].cpu().detach().numpy())\n",
    "        indices.append(idx[mask].cpu().detach().numpy())\n",
    "        offsets.append(offset[mask].cpu().detach().numpy())\n",
    "        \n",
    "        st_passed = time.time() - st\n",
    "        if (i+1)%OUT_TIME == 0 and device_num == 1:\n",
    "            print('Batch {} device: {} time passed: {:.3f} time per batch: {:.3f}'\n",
    "                  .format(i+1, device, st_passed, st_passed/(i+1)))\n",
    "        \n",
    "        del output, y, x, x_le, idx, offset\n",
    "    \n",
    "    results = np.concatenate(results)\n",
    "    indices = np.concatenate(indices)\n",
    "    offsets = np.concatenate(offsets)\n",
    "    \n",
    "    return results, indices, offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_loop_fn(model, loader, device, context = None):\n",
    "    \n",
    "    if CLOUD and (not CLOUD_SINGLE):\n",
    "        tlen = len(loader._loader._loader)\n",
    "        OUT_LOSS = 1000\n",
    "        OUT_TIME = 100\n",
    "        generator = loader\n",
    "        device_num = int(str(device)[-1])\n",
    "    else:\n",
    "        tlen = len(loader)\n",
    "        OUT_LOSS = 1000\n",
    "        OUT_TIME = 10\n",
    "        generator = enumerate(loader)\n",
    "        device_num = 1\n",
    "    \n",
    "    #print('Start testing {}'.format(device), 'batches', tlen)\n",
    "    \n",
    "    st = time.time()\n",
    "    model.eval()\n",
    "    \n",
    "    results = []\n",
    "    indices = []\n",
    "    offsets = []\n",
    "    \n",
    "    for i, (x, x_le, y, idx, offset) in generator:\n",
    "        \n",
    "        if (not CLOUD) or CLOUD_SINGLE:\n",
    "            x = x.to(device)\n",
    "            x_le = x_le.to(device)\n",
    "        \n",
    "        output = torch.sigmoid(model(x, x_le))\n",
    "        \n",
    "        mask = (idx >= 0)\n",
    "        results.append(output[mask].cpu().detach().numpy())\n",
    "        indices.append(idx[mask].cpu().detach().numpy())\n",
    "        offsets.append(offset[mask].cpu().detach().numpy())\n",
    "        \n",
    "        st_passed = time.time() - st\n",
    "        if (i+1)%OUT_TIME == 0 and device_num == 1:\n",
    "            print('B{} -> time passed: {:.3f} time per batch: {:.3f}'.format(i+1, st_passed, st_passed/(i+1)))\n",
    "        \n",
    "        del output, x, y, idx, offset\n",
    "    \n",
    "    return np.concatenate(results), np.concatenate(indices), np.concatenate(offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(weight=None, load_model=True, epochs=1, bs=100, fold=0):\n",
    "    \n",
    "    st0 = time.time()\n",
    "    \n",
    "    cur_epoch = getCurrentBatch(fold=fold)\n",
    "    if cur_epoch is None: cur_epoch = 0\n",
    "    print('completed epochs:', cur_epoch, 'starting now:', epochs)\n",
    "    \n",
    "    setSeeds(SEED + cur_epoch)\n",
    "    \n",
    "    if DATA_SET == 1:\n",
    "        trn_ds = RSNA_DataSet(trn_data, mode='train', bs=bs, fold=fold)\n",
    "        val_ds = RSNA_DataSet(val_data, mode='valid', bs=bs, fold=fold)\n",
    "    elif DATA_SET >= 2:\n",
    "        trn_ds = RSNA_DataSet(ids_df, mode='train', bs=bs, fold=fold)\n",
    "        val_ds = RSNA_DataSet(ids_df, mode='valid', bs=bs, fold=fold)\n",
    "    else: assert False\n",
    "    val_ds.setFeats(0)\n",
    "    \n",
    "    loader = D.DataLoader(trn_ds, num_workers=16 if (CLOUD and not CLOUD_SINGLE) else 0, batch_size=bs, \n",
    "                          shuffle=True, drop_last=True)\n",
    "    loader_val = D.DataLoader(val_ds, num_workers=16 if (CLOUD and not CLOUD_SINGLE) else 0, batch_size=bs, \n",
    "                              shuffle=True)\n",
    "    print('dataset train:', len(trn_ds), 'valid:', len(val_ds), 'loader train:', len(loader), 'valid:', len(loader_val))\n",
    "    \n",
    "    #model = TabularModel(n_cont = len(meta_cols), feat_sz=feat_sz, fc_drop_p=0)\n",
    "    model = ResNetModel(n_cont = len(meta_cols), feat_sz=feat_sz)\n",
    "    \n",
    "    model_file_name = modelFileName(return_last=True, fold=fold)\n",
    "    if model_file_name is not None:\n",
    "        print('loading model', model_file_name)\n",
    "        state_dict = torch.load(PATH_WORK/'models'/model_file_name)\n",
    "        model.load_state_dict(state_dict)\n",
    "    else:\n",
    "        print('starting from scratch')\n",
    "    \n",
    "    if (not CLOUD) or CLOUD_SINGLE:\n",
    "        model = model.to(device)\n",
    "    else:\n",
    "        model_parallel = dp.DataParallel(model, device_ids=devices)\n",
    "        \n",
    "    loc_data = val_ds.metadata.copy()\n",
    "\n",
    "    if DATA_SMALL:\n",
    "        #val_sz = int(0.01*len(loc_data.index.unique()))\n",
    "        val_sz = len(val_ds)\n",
    "        val_series = val_ds.series[:val_sz]\n",
    "        loc_data = loc_data.loc[loc_data.index.isin(val_series)]\n",
    "    \n",
    "    series_counts = loc_data.index.value_counts()\n",
    "    \n",
    "    loc_data['orig_idx'] = np.arange(len(loc_data))\n",
    "    loc_data = loc_data.sort_values(['SeriesInstanceUID','pos_idx1'])\n",
    "    loc_data['my_order'] = np.arange(len(loc_data))\n",
    "    loc_data = loc_data.sort_values(['orig_idx'])\n",
    "    \n",
    "    for i in range(cur_epoch+1, cur_epoch+epochs+1):\n",
    "        st = time.time()\n",
    "        \n",
    "        #trn_ds.setFeats((i-1) % 4)\n",
    "        trn_ds.setFeats(-1)\n",
    "        \n",
    "        if CLOUD and (not CLOUD_SINGLE):\n",
    "            results = model_parallel(train_loop_fn, loader)\n",
    "            tloss, tloss_count = np.stack(results).sum(0)\n",
    "            state_dict = model_parallel._models[0].state_dict()\n",
    "        else:\n",
    "            tloss, tloss_count = train_loop_fn(model, loader, device)\n",
    "            state_dict = model.state_dict()\n",
    "        \n",
    "        state_dict = {k:v.to('cpu') for k,v in state_dict.items()}\n",
    "        tr_ll = tloss / tloss_count\n",
    "        \n",
    "        train_time = time.time()-st\n",
    "        \n",
    "        model_file_name = modelFileName(return_next=True, fold=fold)\n",
    "        if not DATA_SMALL:\n",
    "            torch.save(state_dict, PATH_WORK/'models'/model_file_name)\n",
    "        \n",
    "        st = time.time()\n",
    "        if CLOUD and (not CLOUD_SINGLE):\n",
    "            results = model_parallel(val_loop_fn, loader_val)\n",
    "            predictions = np.concatenate([results[i][0] for i in range(MAX_DEVICES)])\n",
    "            indices = np.concatenate([results[i][1] for i in range(MAX_DEVICES)])\n",
    "            offsets = np.concatenate([results[i][2] for i in range(MAX_DEVICES)])\n",
    "        else:\n",
    "            predictions, indices, offsets = val_loop_fn(model, loader_val, device)\n",
    "        \n",
    "        predictions = predictions[np.argsort(indices)]\n",
    "        offsets = offsets[np.argsort(indices)]\n",
    "        assert len(predictions) == len(loc_data.index.unique())\n",
    "        assert len(predictions) == len(offsets)\n",
    "        assert np.all(indices[np.argsort(indices)] == np.array(range(len(predictions))))\n",
    "        \n",
    "        #val_results = np.zeros((len(loc_data),6))\n",
    "        val_results = []\n",
    "        for k, series in enumerate(np.sort(loc_data.index.unique())):\n",
    "            cnt = series_counts[series]\n",
    "            #mask = loc_data.SeriesInstanceUID == series\n",
    "            assert (offsets[k] + cnt) <= 60\n",
    "            #val_results[mask] = predictions[k,offsets[k]:(offsets[k] + cnt)]\n",
    "            val_results.append(predictions[k,offsets[k]:(offsets[k] + cnt)])\n",
    "        \n",
    "        val_results = np.concatenate(val_results)\n",
    "        assert np.isnan(val_results).sum() == 0\n",
    "        val_results = val_results[loc_data.my_order]\n",
    "        assert np.isnan(val_results).sum() == 0\n",
    "        assert len(val_results) == len(loc_data)\n",
    "        \n",
    "        lls = [log_loss(loc_data[all_ich[k]].values, val_results[:,k], eps=1e-7, labels=[0,1])\\\n",
    "               for k in range(6)]\n",
    "        ll = (class_weights * np.array(lls)).mean()\n",
    "        cor = np.corrcoef(loc_data.loc[:,all_ich].values.reshape(-1), val_results.reshape(-1))[0,1]\n",
    "        auc = roc_auc_score(loc_data.loc[:,all_ich].values.reshape(-1), val_results.reshape(-1))\n",
    "\n",
    "        print('ver {}, epoch {}, fold {}, train ll: {:.4f}, val ll: {:.4f}, cor: {:.4f}, auc: {:.4f}, lr: {}'\n",
    "              .format(VERSION, i, fold, tr_ll, ll, cor, auc, learning_rate))\n",
    "        valid_time = time.time()-st\n",
    "\n",
    "        epoch_stats = pd.DataFrame([[i, 0, tr_ll, ll, cor, lls[0], lls[1], lls[2], lls[3], lls[4], lls[5],\n",
    "                                     len(trn_ds), len(val_ds), bs, train_time, valid_time,\n",
    "                                     learning_rate, weight_decay]], \n",
    "                                   columns = \n",
    "                                    ['epoch','fold','train_loss','val_loss','cor',\n",
    "                                     'any','epidural','intraparenchymal','intraventricular','subarachnoid','subdural',\n",
    "                                     'train_sz','val_sz','bs','train_time','valid_time','lr','wd'\n",
    "                                     ])\n",
    "\n",
    "        stats_filename = PATH_WORK/'stats.f{}.v{}'.format(fold,VERSION)\n",
    "        if stats_filename.is_file():\n",
    "            epoch_stats = pd.concat([pd.read_csv(stats_filename), epoch_stats], sort=False)\n",
    "        #if not DATA_SMALL:\n",
    "        epoch_stats.to_csv(stats_filename, index=False)\n",
    "    \n",
    "    print('total running time', time.time() - st0)\n",
    "    \n",
    "    return model, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch 22 device: xla:1 time passed: 277.972 time per batch: 12.635 - 16 cores / 8 workers\n",
    "#Batch 22 device: xla:1 time passed: 209.280 time per batch: 9.513  - 16 cores / 16 workers\n",
    "#Batch 22 device: xla:1 time passed: 213.209 time per batch: 9.691  - 16 cores / 32 workers\n",
    "#Batch 22 device: xla:1 time passed: 275.780 time per batch: 12.535 - 16 cores / 8 workers\n",
    "#Batch 22 device: xla:1 time passed: 208.826 time per batch: 9.492  - 16 cores / 16 workers\n",
    "#Batch 22 device: xla:1 time passed: 245.750 time per batch: 11.170 - 16 cores / 12 workers\n",
    "#Batch 22 device: xla:1 time passed: 374.876 time per batch: 17.040 - 16 cores / 8 workers\n",
    "#Batch 22 device: xla:1 time passed: 400.221 time per batch: 18.192 - 8 cores / 8 workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIXUP\n",
    "# ver 76, epoch 55, fold 0, train ll: 0.0461, val ll: 0.0656, cor: 0.8393, auc: 0.9868, lr: 5e-06\n",
    "#--- dataset 4\n",
    "\n",
    "\n",
    "# ver 75, epoch 25, fold 0, train ll: 0.0284, val ll: 0.0634, cor: 0.8430, auc: 0.9883, lr: 5e-06\n",
    "# ver 75, epoch 25, fold 1, train ll: 0.0291, val ll: 0.0622, cor: 0.8418, auc: 0.9881, lr: 5e-06\n",
    "# ver 75, epoch 25, fold 2, train ll: 0.0287, val ll: 0.0600, cor: 0.8442, auc: 0.9896, lr: 5e-06\n",
    "#--- dataset 4\n",
    "\n",
    "# ver 74, epoch 25, fold 0, train ll: 0.0285, val ll: 0.0637, cor: 0.8425, auc: 0.9880, lr: 5e-06\n",
    "# ver 74, epoch 25, fold 1, train ll: 0.0285, val ll: 0.0630, cor: 0.8404, auc: 0.9879, lr: 5e-06\n",
    "# ver 74, epoch 25, fold 2, train ll: 0.0287, val ll: 0.0605, cor: 0.8430, auc: 0.9893, lr: 5e-06\n",
    "#--- dataset 3\n",
    "\n",
    "# ver 73, epoch 25, fold 0, train ll: 0.0282, val ll: 0.0637, cor: 0.8431, auc: 0.9879, lr: 5e-06\n",
    "# ver 73, epoch 25, fold 1, train ll: 0.0291, val ll: 0.0631, cor: 0.8396, auc: 0.9878, lr: 5e-06\n",
    "# ver 73, epoch 25, fold 2, train ll: 0.0288, val ll: 0.0607, cor: 0.8431, auc: 0.9891, lr: 5e-06\n",
    "#--- dataset 2\n",
    "\n",
    "# ResNet, concat\n",
    "# 5x1e-3, 10x2e-4, 7x2e-5, 3x5e-6\n",
    "# ver 72, epoch 25, fold 0, train ll: 0.0280, val ll: 0.0639, cor: 0.8426, auc: 0.9878, lr: 5e-06\n",
    "# ver 72, epoch 25, fold 1, train ll: 0.0281, val ll: 0.0634, cor: 0.8381, auc: 0.9879, lr: 5e-06\n",
    "# ver 72, epoch 25, fold 2, train ll: 0.0284, val ll: 0.0608, cor: 0.8429, auc: 0.9890, lr: 5e-06\n",
    "\n",
    "\n",
    "# ResNet, PAD=8\n",
    "# ver 71, epoch 20, fold 0, train ll: 0.0302, val ll: 0.0647, cor: 0.8414, auc: 0.9870, lr: 5e-06\n",
    "\n",
    "# TabularModel, feats norm, scale = 4\n",
    "# ver 70, epoch 20, fold 0, train ll: 0.0295, val ll: 0.0654, cor: 0.8417, auc: 0.9867, lr: 5e-06\n",
    "# TabularModel, feats norm, scale = 8\n",
    "# ver 69, epoch 20, fold 0, train ll: 0.0287, val ll: 0.0651, cor: 0.8417, auc: 0.9868, lr: 5e-06\n",
    "\n",
    "# ResNet, feats norm\n",
    "# 3x1e-3, 7x2e-4, 7x2e-5, 3x5e-6\n",
    "# ver 68, epoch 20, fold 0, train ll: 0.0307, val ll: 0.0648, cor: 0.8410, auc: 0.9871, lr: 5e-06\n",
    "\n",
    "# ResNet try2, PAD=3\n",
    "# ver 67, epoch 29, fold 0, train ll: 0.0360, val ll: 0.0711, cor: 0.8217, auc: 0.9842, lr: 5e-06\n",
    "# ver 67, epoch 36, fold 0, train ll: 0.0345, val ll: 0.0701, cor: 0.8279, auc: 0.9838, lr: 2e-05\n",
    "\n",
    "# scale=2, reduced by 2\n",
    "# 3x1e-3, 7x2e-4, 7x2e-5, 3x5e-6\n",
    "# ver 66, epoch 20, fold 1, train ll: 0.0327, val ll: 0.0659, cor: 0.8372, auc: 0.9864, lr: 5e-06\n",
    "# ver 66, epoch 20, fold 2, train ll: 0.0330, val ll: 0.0635, cor: 0.8415, auc: 0.9879, lr: 5e-06\n",
    "\n",
    "# 3x1e-3, 7x2e-4, 7x2e-5, 3x5e-6\n",
    "# ver 64, epoch 17, fold 0, train ll: 0.0298, val ll: 0.0647, cor: 0.8431, auc: 0.9871, lr: 2e-05\n",
    "# ver 64, epoch 20, fold 0, train ll: 0.0297, val ll: 0.0646, cor: 0.8433, auc: 0.9869, lr: 5e-06\n",
    "# ver 65, epoch 17, fold 1, train ll: 0.0303, val ll: 0.0648, cor: 0.8379, auc: 0.9870, lr: 2e-05\n",
    "# ver 65, epoch 20, fold 1, train ll: 0.0301, val ll: 0.0647, cor: 0.8375, auc: 0.9870, lr: 5e-06\n",
    "# ver 65, epoch 17, fold 2, train ll: 0.0303, val ll: 0.0625, cor: 0.8417, auc: 0.9881, lr: 2e-05\n",
    "# ver 65, epoch 20, fold 2, train ll: 0.0303, val ll: 0.0625, cor: 0.8416, auc: 0.9882, lr: 5e-06\n",
    "\n",
    "#--- dataset 5\n",
    "\n",
    "# Mixup\n",
    "# 5x5e-6 more\n",
    "# ver 61, epoch 49, fold 1, train ll: 0.0471, val ll: 0.0639, cor: 0.8392, auc: 0.9867, lr: 5e-06\n",
    "# ver 61, epoch 49, fold 2, train ll: 0.0466, val ll: 0.0618, cor: 0.8408, auc: 0.9887, lr: 5e-06\n",
    "# 6x1e-3, 14x2e-4, 24x2e-5\n",
    "# ver 63, epoch 45, fold 0, train ll: 0.0467, val ll: 0.0641, cor: 0.8426, auc: 0.9872, lr: 2e-05\n",
    "# ver 61, epoch 44, fold 1, train ll: 0.0474, val ll: 0.0642, cor: 0.8390, auc: 0.9867, lr: 2e-05\n",
    "# ver 61, epoch 44, fold 2, train ll: 0.0464, val ll: 0.0619, cor: 0.8405, auc: 0.9888, lr: 2e-05\n",
    "\n",
    "# 5x2e-5, 5x5e-6 more\n",
    "# ver 61, epoch 32, fold 0, train ll: 0.0282, val ll: 0.0647, cor: 0.8418, auc: 0.9876, lr: 5e-06\n",
    "# 3x1e-3, 7x2e-4, 2x2e-5\n",
    "# ver 61, epoch 20, fold 0, train ll: 0.0292, val ll: 0.0650, cor: 0.8418, auc: 0.9876, lr: 2e-05\n",
    "\n",
    "# ll eps change, black 0.1\n",
    "# 10x2e-4, 10x2e-5\n",
    "\n",
    "# ver 53, epoch 28, fold 0, train ll: 0.0290, val ll: 0.0647, cor: 0.8424, auc: 0.9869, lr: 2e-05\n",
    "# ver 53, epoch 28, fold 1, train ll: 0.0293, val ll: 0.0635, cor: 0.8408, auc: 0.9871, lr: 2e-05\n",
    "# ver 53, epoch 28, fold 2, train ll: 0.0291, val ll: 0.0616, cor: 0.8422, auc: 0.9890, lr: 2e-05\n",
    "\n",
    "# ver 53, epoch 21, fold 0, train ll: 0.0297, val ll: 0.0652, cor: 0.8415, auc: 0.9857, lr: 0.0002\n",
    "# ver 53, epoch 21, fold 1, train ll: 0.0301, val ll: 0.0643, cor: 0.8394, auc: 0.9864, lr: 0.0002\n",
    "# ver 53, epoch 21, fold 2, train ll: 0.0296, val ll: 0.0621, cor: 0.8417, auc: 0.9888, lr: 0.0002\n",
    "\n",
    "#--- dataset 4\n",
    "\n",
    "# 7x0.00002 more\n",
    "# ver 52, epoch 28, fold 0, train ll: 0.0297, val ll: 0.0646, cor: 0.8422, auc: 0.9868, lr: 2e-05\n",
    "# ver 52, epoch 28, fold 1, train ll: 0.0297, val ll: 0.0638, cor: 0.8395, auc: 0.9871, lr: 2e-05\n",
    "# ver 52, epoch 28, fold 2, train ll: 0.0297, val ll: 0.0622, cor: 0.8407, auc: 0.9883, lr: 2e-05\n",
    "# 7x0.002, 7x0.001, 7x0.0002\n",
    "# ver 52, epoch 21, fold 0, train ll: 0.0302, val ll: 0.0653, cor: 0.8409, auc: 0.9864, lr: 0.0002\n",
    "# ver 52, epoch 21, fold 1, train ll: 0.0305, val ll: 0.0648, cor: 0.8379, auc: 0.9862, lr: 0.0002\n",
    "# ver 52, epoch 21, fold 2, train ll: 0.0305, val ll: 0.0622, cor: 0.8400, auc: 0.9883, lr: 0.0002\n",
    "\n",
    "#--- dataset 3\n",
    "\n",
    "# 7x0.00002 more\n",
    "# ver 51, epoch 28, fold 0, train ll: 0.0293, val ll: 0.0649, cor: 0.8417, auc: 0.9868, lr: 2e-05\n",
    "# ver 51, epoch 32, fold 1, train ll: 0.0298, val ll: 0.0646, cor: 0.8379, auc: 0.9868, lr: 2e-05\n",
    "# ver 51, epoch 28, fold 2, train ll: 0.0297, val ll: 0.0621, cor: 0.8411, auc: 0.9882, lr: 2e-05\n",
    "\n",
    "# 7x0.002, 7x0.001, 7x0.0002\n",
    "# new metadata\n",
    "# ver 51, epoch 21, fold 0, train ll: 0.0299, val ll: 0.0651, cor: 0.8411, auc: 0.9869, lr: 0.0002\n",
    "# ver 51, epoch 21, fold 1, train ll: 0.0302, val ll: 0.0647, cor: 0.8374, auc: 0.9874, lr: 0.0002\n",
    "# ver 51, epoch 21, fold 2, train ll: 0.0302, val ll: 0.0625, cor: 0.8400, auc: 0.9881, lr: 0.0002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-cycle\n",
    "# copy latest model to GS code\n",
    "# improve black image meta data\n",
    "# freeze bias approach?\n",
    "# pseudo-labelling?\n",
    "# try GCP fast guide connecting\n",
    "# can it happen that any smaller than max of others\n",
    "# why black feats are predicted all ones\n",
    "# on test, do I have horns?\n",
    "# normalize input feats\n",
    "# what is the score without meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epochs: 5 starting now: 10\n",
      "DataSet 4 train size 13042 fold 0\n",
      "adding dummy serieses 168\n",
      "DataSet 4 valid size 6656 fold 0\n",
      "setFeats, augmentation 0\n",
      "dataset train: 13042 valid: 6656 loader train: 407 valid: 208\n",
      "loading model model.b5.f0.v76\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.024 time per batch: 1.256\n",
      "Batch 8 device: xla:1 time passed: 8.078 time per batch: 1.010\n",
      "Batch 12 device: xla:1 time passed: 11.467 time per batch: 0.956\n",
      "Batch 16 device: xla:1 time passed: 14.949 time per batch: 0.934\n",
      "Batch 20 device: xla:1 time passed: 18.163 time per batch: 0.908\n",
      "Batch 24 device: xla:1 time passed: 21.370 time per batch: 0.890\n",
      "Batch 28 device: xla:1 time passed: 24.647 time per batch: 0.880\n",
      "Batch 32 device: xla:1 time passed: 27.968 time per batch: 0.874\n",
      "Batch 36 device: xla:1 time passed: 31.442 time per batch: 0.873\n",
      "Batch 40 device: xla:1 time passed: 34.668 time per batch: 0.867\n",
      "Batch 44 device: xla:1 time passed: 38.015 time per batch: 0.864\n",
      "Batch 48 device: xla:1 time passed: 41.317 time per batch: 0.861\n",
      "Batch 4 device: xla:1 time passed: 3.671 time per batch: 0.918\n",
      "Batch 8 device: xla:1 time passed: 5.305 time per batch: 0.663\n",
      "Batch 12 device: xla:1 time passed: 7.320 time per batch: 0.610\n",
      "Batch 16 device: xla:1 time passed: 9.673 time per batch: 0.605\n",
      "Batch 20 device: xla:1 time passed: 11.344 time per batch: 0.567\n",
      "Batch 24 device: xla:1 time passed: 13.329 time per batch: 0.555\n",
      "ver 76, epoch 6, fold 0, train ll: 0.0596, val ll: 0.1543, cor: 0.6871, auc: 0.9706, lr: 0.001\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.119 time per batch: 1.280\n",
      "Batch 8 device: xla:1 time passed: 8.108 time per batch: 1.013\n",
      "Batch 12 device: xla:1 time passed: 11.513 time per batch: 0.959\n",
      "Batch 16 device: xla:1 time passed: 14.881 time per batch: 0.930\n",
      "Batch 20 device: xla:1 time passed: 18.194 time per batch: 0.910\n",
      "Batch 24 device: xla:1 time passed: 21.587 time per batch: 0.899\n",
      "Batch 28 device: xla:1 time passed: 24.898 time per batch: 0.889\n",
      "Batch 32 device: xla:1 time passed: 28.217 time per batch: 0.882\n",
      "Batch 36 device: xla:1 time passed: 31.591 time per batch: 0.878\n",
      "Batch 40 device: xla:1 time passed: 34.949 time per batch: 0.874\n",
      "Batch 44 device: xla:1 time passed: 38.201 time per batch: 0.868\n",
      "Batch 48 device: xla:1 time passed: 41.520 time per batch: 0.865\n",
      "Batch 4 device: xla:1 time passed: 3.688 time per batch: 0.922\n",
      "Batch 8 device: xla:1 time passed: 5.321 time per batch: 0.665\n",
      "Batch 12 device: xla:1 time passed: 7.478 time per batch: 0.623\n",
      "Batch 16 device: xla:1 time passed: 9.534 time per batch: 0.596\n",
      "Batch 20 device: xla:1 time passed: 11.285 time per batch: 0.564\n",
      "Batch 24 device: xla:1 time passed: 13.279 time per batch: 0.553\n",
      "ver 76, epoch 7, fold 0, train ll: 0.0578, val ll: 0.1742, cor: 0.5307, auc: 0.8820, lr: 0.001\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.102 time per batch: 1.275\n",
      "Batch 8 device: xla:1 time passed: 8.024 time per batch: 1.003\n",
      "Batch 12 device: xla:1 time passed: 11.393 time per batch: 0.949\n",
      "Batch 16 device: xla:1 time passed: 14.617 time per batch: 0.914\n",
      "Batch 20 device: xla:1 time passed: 17.972 time per batch: 0.899\n",
      "Batch 24 device: xla:1 time passed: 21.260 time per batch: 0.886\n",
      "Batch 28 device: xla:1 time passed: 24.595 time per batch: 0.878\n",
      "Batch 32 device: xla:1 time passed: 27.841 time per batch: 0.870\n",
      "Batch 36 device: xla:1 time passed: 31.066 time per batch: 0.863\n",
      "Batch 40 device: xla:1 time passed: 34.245 time per batch: 0.856\n",
      "Batch 44 device: xla:1 time passed: 37.565 time per batch: 0.854\n",
      "Batch 48 device: xla:1 time passed: 40.799 time per batch: 0.850\n",
      "Batch 4 device: xla:1 time passed: 3.791 time per batch: 0.948\n",
      "Batch 8 device: xla:1 time passed: 5.380 time per batch: 0.672\n",
      "Batch 12 device: xla:1 time passed: 7.332 time per batch: 0.611\n",
      "Batch 16 device: xla:1 time passed: 9.385 time per batch: 0.587\n",
      "Batch 20 device: xla:1 time passed: 11.369 time per batch: 0.568\n",
      "Batch 24 device: xla:1 time passed: 13.334 time per batch: 0.556\n",
      "ver 76, epoch 8, fold 0, train ll: 0.0601, val ll: 0.1297, cor: 0.6876, auc: 0.9704, lr: 0.001\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.015 time per batch: 1.254\n",
      "Batch 8 device: xla:1 time passed: 8.016 time per batch: 1.002\n",
      "Batch 12 device: xla:1 time passed: 11.306 time per batch: 0.942\n",
      "Batch 16 device: xla:1 time passed: 14.619 time per batch: 0.914\n",
      "Batch 20 device: xla:1 time passed: 18.012 time per batch: 0.901\n",
      "Batch 24 device: xla:1 time passed: 21.281 time per batch: 0.887\n",
      "Batch 28 device: xla:1 time passed: 24.586 time per batch: 0.878\n",
      "Batch 32 device: xla:1 time passed: 27.952 time per batch: 0.873\n",
      "Batch 36 device: xla:1 time passed: 31.311 time per batch: 0.870\n",
      "Batch 40 device: xla:1 time passed: 34.656 time per batch: 0.866\n",
      "Batch 44 device: xla:1 time passed: 38.000 time per batch: 0.864\n",
      "Batch 48 device: xla:1 time passed: 41.472 time per batch: 0.864\n",
      "Batch 4 device: xla:1 time passed: 3.834 time per batch: 0.958\n",
      "Batch 8 device: xla:1 time passed: 5.522 time per batch: 0.690\n",
      "Batch 12 device: xla:1 time passed: 7.462 time per batch: 0.622\n",
      "Batch 16 device: xla:1 time passed: 9.670 time per batch: 0.604\n",
      "Batch 20 device: xla:1 time passed: 11.512 time per batch: 0.576\n",
      "Batch 24 device: xla:1 time passed: 13.601 time per batch: 0.567\n",
      "ver 76, epoch 9, fold 0, train ll: 0.0590, val ll: 0.1859, cor: 0.5588, auc: 0.9123, lr: 0.001\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.082 time per batch: 1.271\n",
      "Batch 8 device: xla:1 time passed: 8.270 time per batch: 1.034\n",
      "Batch 12 device: xla:1 time passed: 11.509 time per batch: 0.959\n",
      "Batch 16 device: xla:1 time passed: 14.890 time per batch: 0.931\n",
      "Batch 20 device: xla:1 time passed: 18.247 time per batch: 0.912\n",
      "Batch 24 device: xla:1 time passed: 21.480 time per batch: 0.895\n",
      "Batch 28 device: xla:1 time passed: 24.830 time per batch: 0.887\n",
      "Batch 32 device: xla:1 time passed: 28.119 time per batch: 0.879\n",
      "Batch 36 device: xla:1 time passed: 31.509 time per batch: 0.875\n",
      "Batch 40 device: xla:1 time passed: 34.665 time per batch: 0.867\n",
      "Batch 44 device: xla:1 time passed: 38.003 time per batch: 0.864\n",
      "Batch 48 device: xla:1 time passed: 41.345 time per batch: 0.861\n",
      "Batch 4 device: xla:1 time passed: 3.800 time per batch: 0.950\n",
      "Batch 8 device: xla:1 time passed: 5.471 time per batch: 0.684\n",
      "Batch 12 device: xla:1 time passed: 7.445 time per batch: 0.620\n",
      "Batch 16 device: xla:1 time passed: 9.531 time per batch: 0.596\n",
      "Batch 20 device: xla:1 time passed: 11.596 time per batch: 0.580\n",
      "Batch 24 device: xla:1 time passed: 13.478 time per batch: 0.562\n",
      "ver 76, epoch 10, fold 0, train ll: 0.0656, val ll: 0.2672, cor: 0.5409, auc: 0.9305, lr: 0.001\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.283 time per batch: 1.321\n",
      "Batch 8 device: xla:1 time passed: 8.286 time per batch: 1.036\n",
      "Batch 12 device: xla:1 time passed: 11.643 time per batch: 0.970\n",
      "Batch 16 device: xla:1 time passed: 14.959 time per batch: 0.935\n",
      "Batch 20 device: xla:1 time passed: 18.266 time per batch: 0.913\n",
      "Batch 24 device: xla:1 time passed: 21.653 time per batch: 0.902\n",
      "Batch 28 device: xla:1 time passed: 24.905 time per batch: 0.889\n",
      "Batch 32 device: xla:1 time passed: 28.286 time per batch: 0.884\n",
      "Batch 36 device: xla:1 time passed: 31.607 time per batch: 0.878\n",
      "Batch 40 device: xla:1 time passed: 35.057 time per batch: 0.876\n",
      "Batch 44 device: xla:1 time passed: 38.341 time per batch: 0.871\n",
      "Batch 48 device: xla:1 time passed: 41.664 time per batch: 0.868\n",
      "Batch 4 device: xla:1 time passed: 3.991 time per batch: 0.998\n",
      "Batch 8 device: xla:1 time passed: 5.489 time per batch: 0.686\n",
      "Batch 12 device: xla:1 time passed: 7.565 time per batch: 0.630\n",
      "Batch 16 device: xla:1 time passed: 9.556 time per batch: 0.597\n",
      "Batch 20 device: xla:1 time passed: 11.559 time per batch: 0.578\n",
      "Batch 24 device: xla:1 time passed: 13.568 time per batch: 0.565\n",
      "ver 76, epoch 11, fold 0, train ll: 0.0569, val ll: 0.1074, cor: 0.7399, auc: 0.9744, lr: 0.001\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.082 time per batch: 1.271\n",
      "Batch 8 device: xla:1 time passed: 8.121 time per batch: 1.015\n",
      "Batch 12 device: xla:1 time passed: 11.333 time per batch: 0.944\n",
      "Batch 16 device: xla:1 time passed: 14.675 time per batch: 0.917\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20 device: xla:1 time passed: 17.900 time per batch: 0.895\n",
      "Batch 24 device: xla:1 time passed: 21.214 time per batch: 0.884\n",
      "Batch 28 device: xla:1 time passed: 24.443 time per batch: 0.873\n",
      "Batch 32 device: xla:1 time passed: 27.785 time per batch: 0.868\n",
      "Batch 36 device: xla:1 time passed: 31.028 time per batch: 0.862\n",
      "Batch 40 device: xla:1 time passed: 34.355 time per batch: 0.859\n",
      "Batch 44 device: xla:1 time passed: 37.610 time per batch: 0.855\n",
      "Batch 48 device: xla:1 time passed: 40.895 time per batch: 0.852\n",
      "Batch 4 device: xla:1 time passed: 3.898 time per batch: 0.975\n",
      "Batch 8 device: xla:1 time passed: 5.387 time per batch: 0.673\n",
      "Batch 12 device: xla:1 time passed: 7.426 time per batch: 0.619\n",
      "Batch 16 device: xla:1 time passed: 9.542 time per batch: 0.596\n",
      "Batch 20 device: xla:1 time passed: 11.454 time per batch: 0.573\n",
      "Batch 24 device: xla:1 time passed: 13.474 time per batch: 0.561\n",
      "ver 76, epoch 12, fold 0, train ll: 0.0557, val ll: 0.1182, cor: 0.7065, auc: 0.9526, lr: 0.001\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.191 time per batch: 1.298\n",
      "Batch 8 device: xla:1 time passed: 8.233 time per batch: 1.029\n",
      "Batch 12 device: xla:1 time passed: 11.639 time per batch: 0.970\n",
      "Batch 16 device: xla:1 time passed: 14.959 time per batch: 0.935\n",
      "Batch 20 device: xla:1 time passed: 18.312 time per batch: 0.916\n",
      "Batch 24 device: xla:1 time passed: 21.678 time per batch: 0.903\n",
      "Batch 28 device: xla:1 time passed: 24.959 time per batch: 0.891\n",
      "Batch 32 device: xla:1 time passed: 28.287 time per batch: 0.884\n",
      "Batch 36 device: xla:1 time passed: 31.592 time per batch: 0.878\n",
      "Batch 40 device: xla:1 time passed: 35.009 time per batch: 0.875\n",
      "Batch 44 device: xla:1 time passed: 38.214 time per batch: 0.868\n",
      "Batch 48 device: xla:1 time passed: 41.516 time per batch: 0.865\n",
      "Batch 4 device: xla:1 time passed: 3.744 time per batch: 0.936\n",
      "Batch 8 device: xla:1 time passed: 5.319 time per batch: 0.665\n",
      "Batch 12 device: xla:1 time passed: 7.397 time per batch: 0.616\n",
      "Batch 16 device: xla:1 time passed: 9.333 time per batch: 0.583\n",
      "Batch 20 device: xla:1 time passed: 11.301 time per batch: 0.565\n",
      "Batch 24 device: xla:1 time passed: 13.328 time per batch: 0.555\n",
      "ver 76, epoch 13, fold 0, train ll: 0.0561, val ll: 0.1225, cor: 0.6671, auc: 0.9495, lr: 0.001\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.171 time per batch: 1.293\n",
      "Batch 8 device: xla:1 time passed: 7.977 time per batch: 0.997\n",
      "Batch 12 device: xla:1 time passed: 11.360 time per batch: 0.947\n",
      "Batch 16 device: xla:1 time passed: 14.562 time per batch: 0.910\n",
      "Batch 20 device: xla:1 time passed: 17.872 time per batch: 0.894\n",
      "Batch 24 device: xla:1 time passed: 21.164 time per batch: 0.882\n",
      "Batch 28 device: xla:1 time passed: 24.523 time per batch: 0.876\n",
      "Batch 32 device: xla:1 time passed: 27.778 time per batch: 0.868\n",
      "Batch 36 device: xla:1 time passed: 31.028 time per batch: 0.862\n",
      "Batch 40 device: xla:1 time passed: 34.217 time per batch: 0.855\n",
      "Batch 44 device: xla:1 time passed: 37.459 time per batch: 0.851\n",
      "Batch 48 device: xla:1 time passed: 40.724 time per batch: 0.848\n",
      "Batch 4 device: xla:1 time passed: 3.755 time per batch: 0.939\n",
      "Batch 8 device: xla:1 time passed: 5.355 time per batch: 0.669\n",
      "Batch 12 device: xla:1 time passed: 7.342 time per batch: 0.612\n",
      "Batch 16 device: xla:1 time passed: 9.461 time per batch: 0.591\n",
      "Batch 20 device: xla:1 time passed: 11.394 time per batch: 0.570\n",
      "Batch 24 device: xla:1 time passed: 13.293 time per batch: 0.554\n",
      "ver 76, epoch 14, fold 0, train ll: 0.0587, val ll: 0.1310, cor: 0.7057, auc: 0.9716, lr: 0.001\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.172 time per batch: 1.293\n",
      "Batch 8 device: xla:1 time passed: 8.024 time per batch: 1.003\n",
      "Batch 12 device: xla:1 time passed: 11.253 time per batch: 0.938\n",
      "Batch 16 device: xla:1 time passed: 14.687 time per batch: 0.918\n",
      "Batch 20 device: xla:1 time passed: 18.056 time per batch: 0.903\n",
      "Batch 24 device: xla:1 time passed: 21.365 time per batch: 0.890\n",
      "Batch 28 device: xla:1 time passed: 24.757 time per batch: 0.884\n",
      "Batch 32 device: xla:1 time passed: 28.107 time per batch: 0.878\n",
      "Batch 36 device: xla:1 time passed: 31.469 time per batch: 0.874\n",
      "Batch 40 device: xla:1 time passed: 34.931 time per batch: 0.873\n",
      "Batch 44 device: xla:1 time passed: 38.262 time per batch: 0.870\n",
      "Batch 48 device: xla:1 time passed: 41.721 time per batch: 0.869\n",
      "Batch 4 device: xla:1 time passed: 3.873 time per batch: 0.968\n",
      "Batch 8 device: xla:1 time passed: 5.543 time per batch: 0.693\n",
      "Batch 12 device: xla:1 time passed: 7.440 time per batch: 0.620\n",
      "Batch 16 device: xla:1 time passed: 9.468 time per batch: 0.592\n",
      "Batch 20 device: xla:1 time passed: 11.441 time per batch: 0.572\n",
      "Batch 24 device: xla:1 time passed: 13.442 time per batch: 0.560\n",
      "ver 76, epoch 15, fold 0, train ll: 0.0564, val ll: 0.1109, cor: 0.7578, auc: 0.9798, lr: 0.001\n",
      "total running time 638.065290927887\n",
      "completed epochs: 15 starting now: 20\n",
      "DataSet 4 train size 13042 fold 0\n",
      "adding dummy serieses 168\n",
      "DataSet 4 valid size 6656 fold 0\n",
      "setFeats, augmentation 0\n",
      "dataset train: 13042 valid: 6656 loader train: 407 valid: 208\n",
      "loading model model.b15.f0.v76\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.452 time per batch: 1.363\n",
      "Batch 8 device: xla:1 time passed: 8.401 time per batch: 1.050\n",
      "Batch 12 device: xla:1 time passed: 11.763 time per batch: 0.980\n",
      "Batch 16 device: xla:1 time passed: 15.246 time per batch: 0.953\n",
      "Batch 20 device: xla:1 time passed: 18.542 time per batch: 0.927\n",
      "Batch 24 device: xla:1 time passed: 21.903 time per batch: 0.913\n",
      "Batch 28 device: xla:1 time passed: 25.465 time per batch: 0.909\n",
      "Batch 32 device: xla:1 time passed: 28.948 time per batch: 0.905\n",
      "Batch 36 device: xla:1 time passed: 32.394 time per batch: 0.900\n",
      "Batch 40 device: xla:1 time passed: 35.892 time per batch: 0.897\n",
      "Batch 44 device: xla:1 time passed: 39.254 time per batch: 0.892\n",
      "Batch 48 device: xla:1 time passed: 42.668 time per batch: 0.889\n",
      "Batch 4 device: xla:1 time passed: 3.792 time per batch: 0.948\n",
      "Batch 8 device: xla:1 time passed: 5.524 time per batch: 0.691\n",
      "Batch 12 device: xla:1 time passed: 7.699 time per batch: 0.642\n",
      "Batch 16 device: xla:1 time passed: 9.580 time per batch: 0.599\n",
      "Batch 20 device: xla:1 time passed: 11.757 time per batch: 0.588\n",
      "Batch 24 device: xla:1 time passed: 13.594 time per batch: 0.566\n",
      "ver 76, epoch 16, fold 0, train ll: 0.0523, val ll: 0.0933, cor: 0.7899, auc: 0.9756, lr: 0.0002\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.356 time per batch: 1.339\n",
      "Batch 8 device: xla:1 time passed: 8.462 time per batch: 1.058\n",
      "Batch 12 device: xla:1 time passed: 11.780 time per batch: 0.982\n",
      "Batch 16 device: xla:1 time passed: 15.160 time per batch: 0.948\n",
      "Batch 20 device: xla:1 time passed: 18.569 time per batch: 0.928\n",
      "Batch 24 device: xla:1 time passed: 21.914 time per batch: 0.913\n",
      "Batch 28 device: xla:1 time passed: 25.385 time per batch: 0.907\n",
      "Batch 32 device: xla:1 time passed: 28.817 time per batch: 0.901\n",
      "Batch 36 device: xla:1 time passed: 32.191 time per batch: 0.894\n",
      "Batch 40 device: xla:1 time passed: 35.586 time per batch: 0.890\n",
      "Batch 44 device: xla:1 time passed: 39.049 time per batch: 0.887\n",
      "Batch 48 device: xla:1 time passed: 42.500 time per batch: 0.885\n",
      "Batch 4 device: xla:1 time passed: 4.052 time per batch: 1.013\n",
      "Batch 8 device: xla:1 time passed: 5.605 time per batch: 0.701\n",
      "Batch 12 device: xla:1 time passed: 7.514 time per batch: 0.626\n",
      "Batch 16 device: xla:1 time passed: 9.534 time per batch: 0.596\n",
      "Batch 20 device: xla:1 time passed: 11.582 time per batch: 0.579\n",
      "Batch 24 device: xla:1 time passed: 13.659 time per batch: 0.569\n",
      "ver 76, epoch 17, fold 0, train ll: 0.0505, val ll: 0.0846, cor: 0.7987, auc: 0.9784, lr: 0.0002\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.156 time per batch: 1.289\n",
      "Batch 8 device: xla:1 time passed: 8.200 time per batch: 1.025\n",
      "Batch 12 device: xla:1 time passed: 11.579 time per batch: 0.965\n",
      "Batch 16 device: xla:1 time passed: 14.822 time per batch: 0.926\n",
      "Batch 20 device: xla:1 time passed: 18.281 time per batch: 0.914\n",
      "Batch 24 device: xla:1 time passed: 21.585 time per batch: 0.899\n",
      "Batch 28 device: xla:1 time passed: 24.947 time per batch: 0.891\n",
      "Batch 32 device: xla:1 time passed: 28.241 time per batch: 0.883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 36 device: xla:1 time passed: 31.538 time per batch: 0.876\n",
      "Batch 40 device: xla:1 time passed: 34.866 time per batch: 0.872\n",
      "Batch 44 device: xla:1 time passed: 38.211 time per batch: 0.868\n",
      "Batch 48 device: xla:1 time passed: 41.365 time per batch: 0.862\n",
      "Batch 4 device: xla:1 time passed: 3.843 time per batch: 0.961\n",
      "Batch 8 device: xla:1 time passed: 5.351 time per batch: 0.669\n",
      "Batch 12 device: xla:1 time passed: 7.323 time per batch: 0.610\n",
      "Batch 16 device: xla:1 time passed: 9.293 time per batch: 0.581\n",
      "Batch 20 device: xla:1 time passed: 11.351 time per batch: 0.568\n",
      "Batch 24 device: xla:1 time passed: 13.289 time per batch: 0.554\n",
      "ver 76, epoch 18, fold 0, train ll: 0.0502, val ll: 0.0791, cor: 0.8181, auc: 0.9836, lr: 0.0002\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.076 time per batch: 1.269\n",
      "Batch 8 device: xla:1 time passed: 8.110 time per batch: 1.014\n",
      "Batch 12 device: xla:1 time passed: 11.410 time per batch: 0.951\n",
      "Batch 16 device: xla:1 time passed: 14.682 time per batch: 0.918\n",
      "Batch 20 device: xla:1 time passed: 18.016 time per batch: 0.901\n",
      "Batch 24 device: xla:1 time passed: 21.365 time per batch: 0.890\n",
      "Batch 28 device: xla:1 time passed: 24.720 time per batch: 0.883\n",
      "Batch 32 device: xla:1 time passed: 28.000 time per batch: 0.875\n",
      "Batch 36 device: xla:1 time passed: 31.385 time per batch: 0.872\n",
      "Batch 40 device: xla:1 time passed: 34.646 time per batch: 0.866\n",
      "Batch 44 device: xla:1 time passed: 37.955 time per batch: 0.863\n",
      "Batch 48 device: xla:1 time passed: 41.299 time per batch: 0.860\n",
      "Batch 4 device: xla:1 time passed: 3.885 time per batch: 0.971\n",
      "Batch 8 device: xla:1 time passed: 5.453 time per batch: 0.682\n",
      "Batch 12 device: xla:1 time passed: 7.460 time per batch: 0.622\n",
      "Batch 16 device: xla:1 time passed: 9.402 time per batch: 0.588\n",
      "Batch 20 device: xla:1 time passed: 11.360 time per batch: 0.568\n",
      "Batch 24 device: xla:1 time passed: 13.382 time per batch: 0.558\n",
      "ver 76, epoch 19, fold 0, train ll: 0.0501, val ll: 0.0799, cor: 0.8180, auc: 0.9838, lr: 0.0002\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.102 time per batch: 1.275\n",
      "Batch 8 device: xla:1 time passed: 8.081 time per batch: 1.010\n",
      "Batch 12 device: xla:1 time passed: 11.381 time per batch: 0.948\n",
      "Batch 16 device: xla:1 time passed: 14.812 time per batch: 0.926\n",
      "Batch 20 device: xla:1 time passed: 18.254 time per batch: 0.913\n",
      "Batch 24 device: xla:1 time passed: 21.444 time per batch: 0.894\n",
      "Batch 28 device: xla:1 time passed: 24.770 time per batch: 0.885\n",
      "Batch 32 device: xla:1 time passed: 28.147 time per batch: 0.880\n",
      "Batch 36 device: xla:1 time passed: 31.490 time per batch: 0.875\n",
      "Batch 40 device: xla:1 time passed: 34.866 time per batch: 0.872\n",
      "Batch 44 device: xla:1 time passed: 38.073 time per batch: 0.865\n",
      "Batch 48 device: xla:1 time passed: 41.459 time per batch: 0.864\n",
      "Batch 4 device: xla:1 time passed: 3.900 time per batch: 0.975\n",
      "Batch 8 device: xla:1 time passed: 5.566 time per batch: 0.696\n",
      "Batch 12 device: xla:1 time passed: 7.514 time per batch: 0.626\n",
      "Batch 16 device: xla:1 time passed: 9.538 time per batch: 0.596\n",
      "Batch 20 device: xla:1 time passed: 11.585 time per batch: 0.579\n",
      "Batch 24 device: xla:1 time passed: 13.571 time per batch: 0.565\n",
      "ver 76, epoch 20, fold 0, train ll: 0.0495, val ll: 0.0758, cor: 0.8250, auc: 0.9850, lr: 0.0002\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.158 time per batch: 1.290\n",
      "Batch 8 device: xla:1 time passed: 8.224 time per batch: 1.028\n",
      "Batch 12 device: xla:1 time passed: 11.541 time per batch: 0.962\n",
      "Batch 16 device: xla:1 time passed: 15.042 time per batch: 0.940\n",
      "Batch 20 device: xla:1 time passed: 18.332 time per batch: 0.917\n",
      "Batch 24 device: xla:1 time passed: 21.652 time per batch: 0.902\n",
      "Batch 28 device: xla:1 time passed: 24.977 time per batch: 0.892\n",
      "Batch 32 device: xla:1 time passed: 28.292 time per batch: 0.884\n",
      "Batch 36 device: xla:1 time passed: 31.606 time per batch: 0.878\n",
      "Batch 40 device: xla:1 time passed: 34.986 time per batch: 0.875\n",
      "Batch 44 device: xla:1 time passed: 38.321 time per batch: 0.871\n",
      "Batch 48 device: xla:1 time passed: 41.700 time per batch: 0.869\n",
      "Batch 4 device: xla:1 time passed: 3.872 time per batch: 0.968\n",
      "Batch 8 device: xla:1 time passed: 5.507 time per batch: 0.688\n",
      "Batch 12 device: xla:1 time passed: 7.435 time per batch: 0.620\n",
      "Batch 16 device: xla:1 time passed: 9.412 time per batch: 0.588\n",
      "Batch 20 device: xla:1 time passed: 11.468 time per batch: 0.573\n",
      "Batch 24 device: xla:1 time passed: 13.397 time per batch: 0.558\n",
      "ver 76, epoch 21, fold 0, train ll: 0.0505, val ll: 0.0815, cor: 0.8193, auc: 0.9842, lr: 0.0002\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.286 time per batch: 1.322\n",
      "Batch 8 device: xla:1 time passed: 8.158 time per batch: 1.020\n",
      "Batch 12 device: xla:1 time passed: 11.442 time per batch: 0.954\n",
      "Batch 16 device: xla:1 time passed: 14.781 time per batch: 0.924\n",
      "Batch 20 device: xla:1 time passed: 18.180 time per batch: 0.909\n",
      "Batch 24 device: xla:1 time passed: 21.675 time per batch: 0.903\n",
      "Batch 28 device: xla:1 time passed: 25.045 time per batch: 0.894\n",
      "Batch 32 device: xla:1 time passed: 28.470 time per batch: 0.890\n",
      "Batch 36 device: xla:1 time passed: 31.693 time per batch: 0.880\n",
      "Batch 40 device: xla:1 time passed: 35.033 time per batch: 0.876\n",
      "Batch 44 device: xla:1 time passed: 38.413 time per batch: 0.873\n",
      "Batch 48 device: xla:1 time passed: 41.823 time per batch: 0.871\n",
      "Batch 4 device: xla:1 time passed: 3.841 time per batch: 0.960\n",
      "Batch 8 device: xla:1 time passed: 5.544 time per batch: 0.693\n",
      "Batch 12 device: xla:1 time passed: 7.589 time per batch: 0.632\n",
      "Batch 16 device: xla:1 time passed: 9.518 time per batch: 0.595\n",
      "Batch 20 device: xla:1 time passed: 11.620 time per batch: 0.581\n",
      "Batch 24 device: xla:1 time passed: 13.690 time per batch: 0.570\n",
      "ver 76, epoch 22, fold 0, train ll: 0.0498, val ll: 0.0903, cor: 0.7808, auc: 0.9810, lr: 0.0002\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.276 time per batch: 1.319\n",
      "Batch 8 device: xla:1 time passed: 8.257 time per batch: 1.032\n",
      "Batch 12 device: xla:1 time passed: 11.732 time per batch: 0.978\n",
      "Batch 16 device: xla:1 time passed: 15.185 time per batch: 0.949\n",
      "Batch 20 device: xla:1 time passed: 18.637 time per batch: 0.932\n",
      "Batch 24 device: xla:1 time passed: 22.035 time per batch: 0.918\n",
      "Batch 28 device: xla:1 time passed: 25.479 time per batch: 0.910\n",
      "Batch 32 device: xla:1 time passed: 28.860 time per batch: 0.902\n",
      "Batch 36 device: xla:1 time passed: 32.231 time per batch: 0.895\n",
      "Batch 40 device: xla:1 time passed: 35.717 time per batch: 0.893\n",
      "Batch 44 device: xla:1 time passed: 39.147 time per batch: 0.890\n",
      "Batch 48 device: xla:1 time passed: 42.463 time per batch: 0.885\n",
      "Batch 4 device: xla:1 time passed: 3.822 time per batch: 0.956\n",
      "Batch 8 device: xla:1 time passed: 5.623 time per batch: 0.703\n",
      "Batch 12 device: xla:1 time passed: 7.585 time per batch: 0.632\n",
      "Batch 16 device: xla:1 time passed: 9.572 time per batch: 0.598\n",
      "Batch 20 device: xla:1 time passed: 11.656 time per batch: 0.583\n",
      "Batch 24 device: xla:1 time passed: 13.637 time per batch: 0.568\n",
      "ver 76, epoch 23, fold 0, train ll: 0.0500, val ll: 0.0797, cor: 0.8186, auc: 0.9830, lr: 0.0002\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.346 time per batch: 1.337\n",
      "Batch 8 device: xla:1 time passed: 8.395 time per batch: 1.049\n",
      "Batch 12 device: xla:1 time passed: 11.590 time per batch: 0.966\n",
      "Batch 16 device: xla:1 time passed: 14.907 time per batch: 0.932\n",
      "Batch 20 device: xla:1 time passed: 18.156 time per batch: 0.908\n",
      "Batch 24 device: xla:1 time passed: 21.469 time per batch: 0.895\n",
      "Batch 28 device: xla:1 time passed: 24.666 time per batch: 0.881\n",
      "Batch 32 device: xla:1 time passed: 27.858 time per batch: 0.871\n",
      "Batch 36 device: xla:1 time passed: 31.149 time per batch: 0.865\n",
      "Batch 40 device: xla:1 time passed: 34.374 time per batch: 0.859\n",
      "Batch 44 device: xla:1 time passed: 37.745 time per batch: 0.858\n",
      "Batch 48 device: xla:1 time passed: 41.021 time per batch: 0.855\n",
      "Batch 4 device: xla:1 time passed: 3.960 time per batch: 0.990\n",
      "Batch 8 device: xla:1 time passed: 5.342 time per batch: 0.668\n",
      "Batch 12 device: xla:1 time passed: 7.326 time per batch: 0.610\n",
      "Batch 16 device: xla:1 time passed: 9.303 time per batch: 0.581\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20 device: xla:1 time passed: 11.311 time per batch: 0.566\n",
      "Batch 24 device: xla:1 time passed: 13.318 time per batch: 0.555\n",
      "ver 76, epoch 24, fold 0, train ll: 0.0491, val ll: 0.0838, cor: 0.8171, auc: 0.9815, lr: 0.0002\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.099 time per batch: 1.275\n",
      "Batch 8 device: xla:1 time passed: 8.058 time per batch: 1.007\n",
      "Batch 12 device: xla:1 time passed: 11.365 time per batch: 0.947\n",
      "Batch 16 device: xla:1 time passed: 14.509 time per batch: 0.907\n",
      "Batch 20 device: xla:1 time passed: 18.062 time per batch: 0.903\n",
      "Batch 24 device: xla:1 time passed: 21.075 time per batch: 0.878\n",
      "Batch 28 device: xla:1 time passed: 24.402 time per batch: 0.872\n",
      "Batch 32 device: xla:1 time passed: 27.597 time per batch: 0.862\n",
      "Batch 36 device: xla:1 time passed: 30.872 time per batch: 0.858\n",
      "Batch 40 device: xla:1 time passed: 34.187 time per batch: 0.855\n",
      "Batch 44 device: xla:1 time passed: 37.496 time per batch: 0.852\n",
      "Batch 48 device: xla:1 time passed: 40.721 time per batch: 0.848\n",
      "Batch 4 device: xla:1 time passed: 4.361 time per batch: 1.090\n",
      "Batch 8 device: xla:1 time passed: 5.599 time per batch: 0.700\n",
      "Batch 12 device: xla:1 time passed: 7.574 time per batch: 0.631\n",
      "Batch 16 device: xla:1 time passed: 9.696 time per batch: 0.606\n",
      "Batch 20 device: xla:1 time passed: 11.686 time per batch: 0.584\n",
      "Batch 24 device: xla:1 time passed: 13.475 time per batch: 0.561\n",
      "ver 76, epoch 25, fold 0, train ll: 0.0498, val ll: 0.0774, cor: 0.8228, auc: 0.9828, lr: 0.0002\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.189 time per batch: 1.297\n",
      "Batch 8 device: xla:1 time passed: 8.031 time per batch: 1.004\n",
      "Batch 12 device: xla:1 time passed: 11.347 time per batch: 0.946\n",
      "Batch 16 device: xla:1 time passed: 14.540 time per batch: 0.909\n",
      "Batch 20 device: xla:1 time passed: 17.807 time per batch: 0.890\n",
      "Batch 24 device: xla:1 time passed: 21.125 time per batch: 0.880\n",
      "Batch 28 device: xla:1 time passed: 24.381 time per batch: 0.871\n",
      "Batch 32 device: xla:1 time passed: 27.714 time per batch: 0.866\n",
      "Batch 36 device: xla:1 time passed: 31.092 time per batch: 0.864\n",
      "Batch 40 device: xla:1 time passed: 34.224 time per batch: 0.856\n",
      "Batch 44 device: xla:1 time passed: 37.558 time per batch: 0.854\n",
      "Batch 48 device: xla:1 time passed: 40.749 time per batch: 0.849\n",
      "Batch 4 device: xla:1 time passed: 3.807 time per batch: 0.952\n",
      "Batch 8 device: xla:1 time passed: 5.312 time per batch: 0.664\n",
      "Batch 12 device: xla:1 time passed: 7.369 time per batch: 0.614\n",
      "Batch 16 device: xla:1 time passed: 9.270 time per batch: 0.579\n",
      "Batch 20 device: xla:1 time passed: 11.256 time per batch: 0.563\n",
      "Batch 24 device: xla:1 time passed: 13.237 time per batch: 0.552\n",
      "ver 76, epoch 26, fold 0, train ll: 0.0496, val ll: 0.0769, cor: 0.8253, auc: 0.9849, lr: 0.0002\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.122 time per batch: 1.280\n",
      "Batch 8 device: xla:1 time passed: 8.023 time per batch: 1.003\n",
      "Batch 12 device: xla:1 time passed: 11.177 time per batch: 0.931\n",
      "Batch 16 device: xla:1 time passed: 14.481 time per batch: 0.905\n",
      "Batch 20 device: xla:1 time passed: 17.766 time per batch: 0.888\n",
      "Batch 24 device: xla:1 time passed: 21.046 time per batch: 0.877\n",
      "Batch 28 device: xla:1 time passed: 24.279 time per batch: 0.867\n",
      "Batch 32 device: xla:1 time passed: 27.561 time per batch: 0.861\n",
      "Batch 36 device: xla:1 time passed: 30.745 time per batch: 0.854\n",
      "Batch 40 device: xla:1 time passed: 33.990 time per batch: 0.850\n",
      "Batch 44 device: xla:1 time passed: 37.568 time per batch: 0.854\n",
      "Batch 48 device: xla:1 time passed: 40.878 time per batch: 0.852\n",
      "Batch 4 device: xla:1 time passed: 3.802 time per batch: 0.950\n",
      "Batch 8 device: xla:1 time passed: 5.307 time per batch: 0.663\n",
      "Batch 12 device: xla:1 time passed: 7.246 time per batch: 0.604\n",
      "Batch 16 device: xla:1 time passed: 9.261 time per batch: 0.579\n",
      "Batch 20 device: xla:1 time passed: 11.206 time per batch: 0.560\n",
      "Batch 24 device: xla:1 time passed: 13.309 time per batch: 0.555\n",
      "ver 76, epoch 27, fold 0, train ll: 0.0495, val ll: 0.0794, cor: 0.8226, auc: 0.9843, lr: 0.0002\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.104 time per batch: 1.276\n",
      "Batch 8 device: xla:1 time passed: 8.106 time per batch: 1.013\n",
      "Batch 12 device: xla:1 time passed: 11.387 time per batch: 0.949\n",
      "Batch 16 device: xla:1 time passed: 14.671 time per batch: 0.917\n",
      "Batch 20 device: xla:1 time passed: 17.984 time per batch: 0.899\n",
      "Batch 24 device: xla:1 time passed: 21.292 time per batch: 0.887\n",
      "Batch 28 device: xla:1 time passed: 24.558 time per batch: 0.877\n",
      "Batch 32 device: xla:1 time passed: 27.800 time per batch: 0.869\n",
      "Batch 36 device: xla:1 time passed: 31.057 time per batch: 0.863\n",
      "Batch 40 device: xla:1 time passed: 34.312 time per batch: 0.858\n",
      "Batch 44 device: xla:1 time passed: 37.575 time per batch: 0.854\n",
      "Batch 48 device: xla:1 time passed: 40.946 time per batch: 0.853\n",
      "Batch 4 device: xla:1 time passed: 3.898 time per batch: 0.974\n",
      "Batch 8 device: xla:1 time passed: 5.460 time per batch: 0.682\n",
      "Batch 12 device: xla:1 time passed: 7.472 time per batch: 0.623\n",
      "Batch 16 device: xla:1 time passed: 9.458 time per batch: 0.591\n",
      "Batch 20 device: xla:1 time passed: 11.663 time per batch: 0.583\n",
      "Batch 24 device: xla:1 time passed: 13.416 time per batch: 0.559\n",
      "ver 76, epoch 28, fold 0, train ll: 0.0494, val ll: 0.0843, cor: 0.8055, auc: 0.9806, lr: 0.0002\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.074 time per batch: 1.268\n",
      "Batch 8 device: xla:1 time passed: 7.956 time per batch: 0.994\n",
      "Batch 12 device: xla:1 time passed: 11.236 time per batch: 0.936\n",
      "Batch 16 device: xla:1 time passed: 14.480 time per batch: 0.905\n",
      "Batch 20 device: xla:1 time passed: 17.772 time per batch: 0.889\n",
      "Batch 24 device: xla:1 time passed: 21.046 time per batch: 0.877\n",
      "Batch 28 device: xla:1 time passed: 24.318 time per batch: 0.868\n",
      "Batch 32 device: xla:1 time passed: 27.599 time per batch: 0.862\n",
      "Batch 36 device: xla:1 time passed: 30.877 time per batch: 0.858\n",
      "Batch 40 device: xla:1 time passed: 34.120 time per batch: 0.853\n",
      "Batch 44 device: xla:1 time passed: 37.323 time per batch: 0.848\n",
      "Batch 48 device: xla:1 time passed: 40.554 time per batch: 0.845\n",
      "Batch 4 device: xla:1 time passed: 3.808 time per batch: 0.952\n",
      "Batch 8 device: xla:1 time passed: 5.381 time per batch: 0.673\n",
      "Batch 12 device: xla:1 time passed: 7.339 time per batch: 0.612\n",
      "Batch 16 device: xla:1 time passed: 9.350 time per batch: 0.584\n",
      "Batch 20 device: xla:1 time passed: 11.308 time per batch: 0.565\n",
      "Batch 24 device: xla:1 time passed: 13.272 time per batch: 0.553\n",
      "ver 76, epoch 29, fold 0, train ll: 0.0496, val ll: 0.0800, cor: 0.8067, auc: 0.9823, lr: 0.0002\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.083 time per batch: 1.271\n",
      "Batch 8 device: xla:1 time passed: 8.073 time per batch: 1.009\n",
      "Batch 12 device: xla:1 time passed: 11.308 time per batch: 0.942\n",
      "Batch 16 device: xla:1 time passed: 14.420 time per batch: 0.901\n",
      "Batch 20 device: xla:1 time passed: 17.623 time per batch: 0.881\n",
      "Batch 24 device: xla:1 time passed: 20.908 time per batch: 0.871\n",
      "Batch 28 device: xla:1 time passed: 24.208 time per batch: 0.865\n",
      "Batch 32 device: xla:1 time passed: 27.324 time per batch: 0.854\n",
      "Batch 36 device: xla:1 time passed: 30.618 time per batch: 0.851\n",
      "Batch 40 device: xla:1 time passed: 33.875 time per batch: 0.847\n",
      "Batch 44 device: xla:1 time passed: 37.161 time per batch: 0.845\n",
      "Batch 48 device: xla:1 time passed: 40.367 time per batch: 0.841\n",
      "Batch 4 device: xla:1 time passed: 3.769 time per batch: 0.942\n",
      "Batch 8 device: xla:1 time passed: 5.330 time per batch: 0.666\n",
      "Batch 12 device: xla:1 time passed: 7.350 time per batch: 0.612\n",
      "Batch 16 device: xla:1 time passed: 9.347 time per batch: 0.584\n",
      "Batch 20 device: xla:1 time passed: 11.319 time per batch: 0.566\n",
      "Batch 24 device: xla:1 time passed: 13.290 time per batch: 0.554\n",
      "ver 76, epoch 30, fold 0, train ll: 0.0498, val ll: 0.0794, cor: 0.8205, auc: 0.9837, lr: 0.0002\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.170 time per batch: 1.293\n",
      "Batch 8 device: xla:1 time passed: 8.192 time per batch: 1.024\n",
      "Batch 12 device: xla:1 time passed: 11.487 time per batch: 0.957\n",
      "Batch 16 device: xla:1 time passed: 14.775 time per batch: 0.923\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20 device: xla:1 time passed: 17.996 time per batch: 0.900\n",
      "Batch 24 device: xla:1 time passed: 21.230 time per batch: 0.885\n",
      "Batch 28 device: xla:1 time passed: 24.623 time per batch: 0.879\n",
      "Batch 32 device: xla:1 time passed: 27.707 time per batch: 0.866\n",
      "Batch 36 device: xla:1 time passed: 31.037 time per batch: 0.862\n",
      "Batch 40 device: xla:1 time passed: 34.302 time per batch: 0.858\n",
      "Batch 44 device: xla:1 time passed: 37.596 time per batch: 0.854\n",
      "Batch 48 device: xla:1 time passed: 40.817 time per batch: 0.850\n",
      "Batch 4 device: xla:1 time passed: 3.737 time per batch: 0.934\n",
      "Batch 8 device: xla:1 time passed: 5.405 time per batch: 0.676\n",
      "Batch 12 device: xla:1 time passed: 7.382 time per batch: 0.615\n",
      "Batch 16 device: xla:1 time passed: 9.641 time per batch: 0.603\n",
      "Batch 20 device: xla:1 time passed: 11.452 time per batch: 0.573\n",
      "Batch 24 device: xla:1 time passed: 13.392 time per batch: 0.558\n",
      "ver 76, epoch 31, fold 0, train ll: 0.0493, val ll: 0.0769, cor: 0.8257, auc: 0.9848, lr: 0.0002\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.198 time per batch: 1.299\n",
      "Batch 8 device: xla:1 time passed: 8.128 time per batch: 1.016\n",
      "Batch 12 device: xla:1 time passed: 11.421 time per batch: 0.952\n",
      "Batch 16 device: xla:1 time passed: 14.672 time per batch: 0.917\n",
      "Batch 20 device: xla:1 time passed: 17.900 time per batch: 0.895\n",
      "Batch 24 device: xla:1 time passed: 21.173 time per batch: 0.882\n",
      "Batch 28 device: xla:1 time passed: 24.430 time per batch: 0.872\n",
      "Batch 32 device: xla:1 time passed: 27.794 time per batch: 0.869\n",
      "Batch 36 device: xla:1 time passed: 31.064 time per batch: 0.863\n",
      "Batch 40 device: xla:1 time passed: 34.337 time per batch: 0.858\n",
      "Batch 44 device: xla:1 time passed: 37.672 time per batch: 0.856\n",
      "Batch 48 device: xla:1 time passed: 40.958 time per batch: 0.853\n",
      "Batch 4 device: xla:1 time passed: 3.893 time per batch: 0.973\n",
      "Batch 8 device: xla:1 time passed: 5.424 time per batch: 0.678\n",
      "Batch 12 device: xla:1 time passed: 7.441 time per batch: 0.620\n",
      "Batch 16 device: xla:1 time passed: 9.554 time per batch: 0.597\n",
      "Batch 20 device: xla:1 time passed: 11.457 time per batch: 0.573\n",
      "Batch 24 device: xla:1 time passed: 13.434 time per batch: 0.560\n",
      "ver 76, epoch 32, fold 0, train ll: 0.0482, val ll: 0.0771, cor: 0.8223, auc: 0.9845, lr: 0.0002\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.084 time per batch: 1.271\n",
      "Batch 8 device: xla:1 time passed: 8.014 time per batch: 1.002\n",
      "Batch 12 device: xla:1 time passed: 11.311 time per batch: 0.943\n",
      "Batch 16 device: xla:1 time passed: 14.572 time per batch: 0.911\n",
      "Batch 20 device: xla:1 time passed: 17.876 time per batch: 0.894\n",
      "Batch 24 device: xla:1 time passed: 21.186 time per batch: 0.883\n",
      "Batch 28 device: xla:1 time passed: 24.437 time per batch: 0.873\n",
      "Batch 32 device: xla:1 time passed: 27.744 time per batch: 0.867\n",
      "Batch 36 device: xla:1 time passed: 31.017 time per batch: 0.862\n",
      "Batch 40 device: xla:1 time passed: 34.196 time per batch: 0.855\n",
      "Batch 44 device: xla:1 time passed: 37.355 time per batch: 0.849\n",
      "Batch 48 device: xla:1 time passed: 40.603 time per batch: 0.846\n",
      "Batch 4 device: xla:1 time passed: 3.756 time per batch: 0.939\n",
      "Batch 8 device: xla:1 time passed: 5.441 time per batch: 0.680\n",
      "Batch 12 device: xla:1 time passed: 7.353 time per batch: 0.613\n",
      "Batch 16 device: xla:1 time passed: 9.451 time per batch: 0.591\n",
      "Batch 20 device: xla:1 time passed: 11.362 time per batch: 0.568\n",
      "Batch 24 device: xla:1 time passed: 13.342 time per batch: 0.556\n",
      "ver 76, epoch 33, fold 0, train ll: 0.0481, val ll: 0.0822, cor: 0.8249, auc: 0.9851, lr: 0.0002\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.265 time per batch: 1.316\n",
      "Batch 8 device: xla:1 time passed: 8.133 time per batch: 1.017\n",
      "Batch 12 device: xla:1 time passed: 11.403 time per batch: 0.950\n",
      "Batch 16 device: xla:1 time passed: 14.583 time per batch: 0.911\n",
      "Batch 20 device: xla:1 time passed: 17.875 time per batch: 0.894\n",
      "Batch 24 device: xla:1 time passed: 21.116 time per batch: 0.880\n",
      "Batch 28 device: xla:1 time passed: 24.425 time per batch: 0.872\n",
      "Batch 32 device: xla:1 time passed: 27.613 time per batch: 0.863\n",
      "Batch 36 device: xla:1 time passed: 30.854 time per batch: 0.857\n",
      "Batch 40 device: xla:1 time passed: 34.179 time per batch: 0.854\n",
      "Batch 44 device: xla:1 time passed: 37.412 time per batch: 0.850\n",
      "Batch 48 device: xla:1 time passed: 40.652 time per batch: 0.847\n",
      "Batch 4 device: xla:1 time passed: 3.869 time per batch: 0.967\n",
      "Batch 8 device: xla:1 time passed: 5.399 time per batch: 0.675\n",
      "Batch 12 device: xla:1 time passed: 7.390 time per batch: 0.616\n",
      "Batch 16 device: xla:1 time passed: 9.380 time per batch: 0.586\n",
      "Batch 20 device: xla:1 time passed: 11.311 time per batch: 0.566\n",
      "Batch 24 device: xla:1 time passed: 13.324 time per batch: 0.555\n",
      "ver 76, epoch 34, fold 0, train ll: 0.0480, val ll: 0.0747, cor: 0.8337, auc: 0.9868, lr: 0.0002\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.178 time per batch: 1.294\n",
      "Batch 8 device: xla:1 time passed: 8.012 time per batch: 1.001\n",
      "Batch 12 device: xla:1 time passed: 11.511 time per batch: 0.959\n",
      "Batch 16 device: xla:1 time passed: 14.546 time per batch: 0.909\n",
      "Batch 20 device: xla:1 time passed: 17.803 time per batch: 0.890\n",
      "Batch 24 device: xla:1 time passed: 21.123 time per batch: 0.880\n",
      "Batch 28 device: xla:1 time passed: 24.395 time per batch: 0.871\n",
      "Batch 32 device: xla:1 time passed: 27.685 time per batch: 0.865\n",
      "Batch 36 device: xla:1 time passed: 30.938 time per batch: 0.859\n",
      "Batch 40 device: xla:1 time passed: 34.202 time per batch: 0.855\n",
      "Batch 44 device: xla:1 time passed: 37.482 time per batch: 0.852\n",
      "Batch 48 device: xla:1 time passed: 40.755 time per batch: 0.849\n",
      "Batch 4 device: xla:1 time passed: 3.776 time per batch: 0.944\n",
      "Batch 8 device: xla:1 time passed: 5.375 time per batch: 0.672\n",
      "Batch 12 device: xla:1 time passed: 7.316 time per batch: 0.610\n",
      "Batch 16 device: xla:1 time passed: 9.294 time per batch: 0.581\n",
      "Batch 20 device: xla:1 time passed: 11.274 time per batch: 0.564\n",
      "Batch 24 device: xla:1 time passed: 13.267 time per batch: 0.553\n",
      "ver 76, epoch 35, fold 0, train ll: 0.0484, val ll: 0.0723, cor: 0.8238, auc: 0.9838, lr: 0.0002\n",
      "total running time 1272.6046168804169\n",
      "completed epochs: 35 starting now: 14\n",
      "DataSet 4 train size 13042 fold 0\n",
      "adding dummy serieses 168\n",
      "DataSet 4 valid size 6656 fold 0\n",
      "setFeats, augmentation 0\n",
      "dataset train: 13042 valid: 6656 loader train: 407 valid: 208\n",
      "loading model model.b35.f0.v76\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.408 time per batch: 1.352\n",
      "Batch 8 device: xla:1 time passed: 8.425 time per batch: 1.053\n",
      "Batch 12 device: xla:1 time passed: 11.615 time per batch: 0.968\n",
      "Batch 16 device: xla:1 time passed: 14.856 time per batch: 0.929\n",
      "Batch 20 device: xla:1 time passed: 18.117 time per batch: 0.906\n",
      "Batch 24 device: xla:1 time passed: 21.383 time per batch: 0.891\n",
      "Batch 28 device: xla:1 time passed: 24.654 time per batch: 0.880\n",
      "Batch 32 device: xla:1 time passed: 27.901 time per batch: 0.872\n",
      "Batch 36 device: xla:1 time passed: 31.234 time per batch: 0.868\n",
      "Batch 40 device: xla:1 time passed: 34.331 time per batch: 0.858\n",
      "Batch 44 device: xla:1 time passed: 37.653 time per batch: 0.856\n",
      "Batch 48 device: xla:1 time passed: 40.938 time per batch: 0.853\n",
      "Batch 4 device: xla:1 time passed: 3.942 time per batch: 0.986\n",
      "Batch 8 device: xla:1 time passed: 5.481 time per batch: 0.685\n",
      "Batch 12 device: xla:1 time passed: 7.447 time per batch: 0.621\n",
      "Batch 16 device: xla:1 time passed: 9.432 time per batch: 0.590\n",
      "Batch 20 device: xla:1 time passed: 11.393 time per batch: 0.570\n",
      "Batch 24 device: xla:1 time passed: 13.310 time per batch: 0.555\n",
      "ver 76, epoch 36, fold 0, train ll: 0.0469, val ll: 0.0660, cor: 0.8376, auc: 0.9871, lr: 2e-05\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.121 time per batch: 1.280\n",
      "Batch 8 device: xla:1 time passed: 8.037 time per batch: 1.005\n",
      "Batch 12 device: xla:1 time passed: 11.270 time per batch: 0.939\n",
      "Batch 16 device: xla:1 time passed: 14.569 time per batch: 0.911\n",
      "Batch 20 device: xla:1 time passed: 17.915 time per batch: 0.896\n",
      "Batch 24 device: xla:1 time passed: 21.194 time per batch: 0.883\n",
      "Batch 28 device: xla:1 time passed: 24.533 time per batch: 0.876\n",
      "Batch 32 device: xla:1 time passed: 27.816 time per batch: 0.869\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 36 device: xla:1 time passed: 31.110 time per batch: 0.864\n",
      "Batch 40 device: xla:1 time passed: 34.361 time per batch: 0.859\n",
      "Batch 44 device: xla:1 time passed: 37.663 time per batch: 0.856\n",
      "Batch 48 device: xla:1 time passed: 40.891 time per batch: 0.852\n",
      "Batch 4 device: xla:1 time passed: 3.903 time per batch: 0.976\n",
      "Batch 8 device: xla:1 time passed: 5.442 time per batch: 0.680\n",
      "Batch 12 device: xla:1 time passed: 7.361 time per batch: 0.613\n",
      "Batch 16 device: xla:1 time passed: 9.303 time per batch: 0.581\n",
      "Batch 20 device: xla:1 time passed: 11.236 time per batch: 0.562\n",
      "Batch 24 device: xla:1 time passed: 13.224 time per batch: 0.551\n",
      "ver 76, epoch 37, fold 0, train ll: 0.0472, val ll: 0.0657, cor: 0.8377, auc: 0.9873, lr: 2e-05\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.088 time per batch: 1.272\n",
      "Batch 8 device: xla:1 time passed: 7.993 time per batch: 0.999\n",
      "Batch 12 device: xla:1 time passed: 11.278 time per batch: 0.940\n",
      "Batch 16 device: xla:1 time passed: 14.524 time per batch: 0.908\n",
      "Batch 20 device: xla:1 time passed: 17.859 time per batch: 0.893\n",
      "Batch 24 device: xla:1 time passed: 21.064 time per batch: 0.878\n",
      "Batch 28 device: xla:1 time passed: 24.367 time per batch: 0.870\n",
      "Batch 32 device: xla:1 time passed: 27.627 time per batch: 0.863\n",
      "Batch 36 device: xla:1 time passed: 30.900 time per batch: 0.858\n",
      "Batch 40 device: xla:1 time passed: 34.181 time per batch: 0.855\n",
      "Batch 44 device: xla:1 time passed: 37.399 time per batch: 0.850\n",
      "Batch 48 device: xla:1 time passed: 40.648 time per batch: 0.847\n",
      "Batch 4 device: xla:1 time passed: 3.762 time per batch: 0.941\n",
      "Batch 8 device: xla:1 time passed: 5.373 time per batch: 0.672\n",
      "Batch 12 device: xla:1 time passed: 7.341 time per batch: 0.612\n",
      "Batch 16 device: xla:1 time passed: 9.296 time per batch: 0.581\n",
      "Batch 20 device: xla:1 time passed: 11.357 time per batch: 0.568\n",
      "Batch 24 device: xla:1 time passed: 13.286 time per batch: 0.554\n",
      "ver 76, epoch 38, fold 0, train ll: 0.0472, val ll: 0.0656, cor: 0.8380, auc: 0.9874, lr: 2e-05\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.076 time per batch: 1.269\n",
      "Batch 8 device: xla:1 time passed: 7.997 time per batch: 1.000\n",
      "Batch 12 device: xla:1 time passed: 11.311 time per batch: 0.943\n",
      "Batch 16 device: xla:1 time passed: 14.510 time per batch: 0.907\n",
      "Batch 20 device: xla:1 time passed: 17.752 time per batch: 0.888\n",
      "Batch 24 device: xla:1 time passed: 21.037 time per batch: 0.877\n",
      "Batch 28 device: xla:1 time passed: 24.217 time per batch: 0.865\n",
      "Batch 32 device: xla:1 time passed: 27.518 time per batch: 0.860\n",
      "Batch 36 device: xla:1 time passed: 30.984 time per batch: 0.861\n",
      "Batch 40 device: xla:1 time passed: 34.000 time per batch: 0.850\n",
      "Batch 44 device: xla:1 time passed: 37.270 time per batch: 0.847\n",
      "Batch 48 device: xla:1 time passed: 40.498 time per batch: 0.844\n",
      "Batch 4 device: xla:1 time passed: 3.695 time per batch: 0.924\n",
      "Batch 8 device: xla:1 time passed: 5.291 time per batch: 0.661\n",
      "Batch 12 device: xla:1 time passed: 7.296 time per batch: 0.608\n",
      "Batch 16 device: xla:1 time passed: 9.218 time per batch: 0.576\n",
      "Batch 20 device: xla:1 time passed: 11.204 time per batch: 0.560\n",
      "Batch 24 device: xla:1 time passed: 13.220 time per batch: 0.551\n",
      "ver 76, epoch 39, fold 0, train ll: 0.0464, val ll: 0.0660, cor: 0.8380, auc: 0.9870, lr: 2e-05\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.116 time per batch: 1.279\n",
      "Batch 8 device: xla:1 time passed: 7.994 time per batch: 0.999\n",
      "Batch 12 device: xla:1 time passed: 11.304 time per batch: 0.942\n",
      "Batch 16 device: xla:1 time passed: 14.487 time per batch: 0.905\n",
      "Batch 20 device: xla:1 time passed: 17.735 time per batch: 0.887\n",
      "Batch 24 device: xla:1 time passed: 21.052 time per batch: 0.877\n",
      "Batch 28 device: xla:1 time passed: 24.275 time per batch: 0.867\n",
      "Batch 32 device: xla:1 time passed: 27.484 time per batch: 0.859\n",
      "Batch 36 device: xla:1 time passed: 30.773 time per batch: 0.855\n",
      "Batch 40 device: xla:1 time passed: 34.090 time per batch: 0.852\n",
      "Batch 44 device: xla:1 time passed: 37.336 time per batch: 0.849\n",
      "Batch 48 device: xla:1 time passed: 40.594 time per batch: 0.846\n",
      "Batch 4 device: xla:1 time passed: 3.846 time per batch: 0.961\n",
      "Batch 8 device: xla:1 time passed: 5.425 time per batch: 0.678\n",
      "Batch 12 device: xla:1 time passed: 7.398 time per batch: 0.616\n",
      "Batch 16 device: xla:1 time passed: 9.399 time per batch: 0.587\n",
      "Batch 20 device: xla:1 time passed: 11.368 time per batch: 0.568\n",
      "Batch 24 device: xla:1 time passed: 13.338 time per batch: 0.556\n",
      "ver 76, epoch 40, fold 0, train ll: 0.0468, val ll: 0.0663, cor: 0.8381, auc: 0.9868, lr: 2e-05\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 4.991 time per batch: 1.248\n",
      "Batch 8 device: xla:1 time passed: 7.942 time per batch: 0.993\n",
      "Batch 12 device: xla:1 time passed: 11.302 time per batch: 0.942\n",
      "Batch 16 device: xla:1 time passed: 14.462 time per batch: 0.904\n",
      "Batch 20 device: xla:1 time passed: 17.970 time per batch: 0.898\n",
      "Batch 24 device: xla:1 time passed: 21.522 time per batch: 0.897\n",
      "Batch 28 device: xla:1 time passed: 24.440 time per batch: 0.873\n",
      "Batch 32 device: xla:1 time passed: 27.677 time per batch: 0.865\n",
      "Batch 36 device: xla:1 time passed: 30.973 time per batch: 0.860\n",
      "Batch 40 device: xla:1 time passed: 34.225 time per batch: 0.856\n",
      "Batch 44 device: xla:1 time passed: 37.426 time per batch: 0.851\n",
      "Batch 48 device: xla:1 time passed: 40.626 time per batch: 0.846\n",
      "Batch 4 device: xla:1 time passed: 3.822 time per batch: 0.956\n",
      "Batch 8 device: xla:1 time passed: 5.456 time per batch: 0.682\n",
      "Batch 12 device: xla:1 time passed: 7.432 time per batch: 0.619\n",
      "Batch 16 device: xla:1 time passed: 9.376 time per batch: 0.586\n",
      "Batch 20 device: xla:1 time passed: 11.317 time per batch: 0.566\n",
      "Batch 24 device: xla:1 time passed: 13.277 time per batch: 0.553\n",
      "ver 76, epoch 41, fold 0, train ll: 0.0462, val ll: 0.0656, cor: 0.8387, auc: 0.9874, lr: 2e-05\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.045 time per batch: 1.261\n",
      "Batch 8 device: xla:1 time passed: 7.945 time per batch: 0.993\n",
      "Batch 12 device: xla:1 time passed: 11.251 time per batch: 0.938\n",
      "Batch 16 device: xla:1 time passed: 14.443 time per batch: 0.903\n",
      "Batch 20 device: xla:1 time passed: 17.731 time per batch: 0.887\n",
      "Batch 24 device: xla:1 time passed: 21.035 time per batch: 0.876\n",
      "Batch 28 device: xla:1 time passed: 24.216 time per batch: 0.865\n",
      "Batch 32 device: xla:1 time passed: 27.405 time per batch: 0.856\n",
      "Batch 36 device: xla:1 time passed: 30.715 time per batch: 0.853\n",
      "Batch 40 device: xla:1 time passed: 33.976 time per batch: 0.849\n",
      "Batch 44 device: xla:1 time passed: 37.240 time per batch: 0.846\n",
      "Batch 48 device: xla:1 time passed: 40.481 time per batch: 0.843\n",
      "Batch 4 device: xla:1 time passed: 3.791 time per batch: 0.948\n",
      "Batch 8 device: xla:1 time passed: 5.378 time per batch: 0.672\n",
      "Batch 12 device: xla:1 time passed: 7.362 time per batch: 0.613\n",
      "Batch 16 device: xla:1 time passed: 9.306 time per batch: 0.582\n",
      "Batch 20 device: xla:1 time passed: 11.296 time per batch: 0.565\n",
      "Batch 24 device: xla:1 time passed: 13.288 time per batch: 0.554\n",
      "ver 76, epoch 42, fold 0, train ll: 0.0456, val ll: 0.0655, cor: 0.8385, auc: 0.9875, lr: 2e-05\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.114 time per batch: 1.278\n",
      "Batch 8 device: xla:1 time passed: 7.881 time per batch: 0.985\n",
      "Batch 12 device: xla:1 time passed: 11.165 time per batch: 0.930\n",
      "Batch 16 device: xla:1 time passed: 14.322 time per batch: 0.895\n",
      "Batch 20 device: xla:1 time passed: 17.687 time per batch: 0.884\n",
      "Batch 24 device: xla:1 time passed: 20.892 time per batch: 0.870\n",
      "Batch 28 device: xla:1 time passed: 24.139 time per batch: 0.862\n",
      "Batch 32 device: xla:1 time passed: 27.419 time per batch: 0.857\n",
      "Batch 36 device: xla:1 time passed: 30.657 time per batch: 0.852\n",
      "Batch 40 device: xla:1 time passed: 33.884 time per batch: 0.847\n",
      "Batch 44 device: xla:1 time passed: 37.091 time per batch: 0.843\n",
      "Batch 48 device: xla:1 time passed: 40.361 time per batch: 0.841\n",
      "Batch 4 device: xla:1 time passed: 3.741 time per batch: 0.935\n",
      "Batch 8 device: xla:1 time passed: 5.366 time per batch: 0.671\n",
      "Batch 12 device: xla:1 time passed: 7.375 time per batch: 0.615\n",
      "Batch 16 device: xla:1 time passed: 9.315 time per batch: 0.582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 20 device: xla:1 time passed: 11.295 time per batch: 0.565\n",
      "Batch 24 device: xla:1 time passed: 13.311 time per batch: 0.555\n",
      "ver 76, epoch 43, fold 0, train ll: 0.0464, val ll: 0.0653, cor: 0.8388, auc: 0.9875, lr: 2e-05\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.182 time per batch: 1.296\n",
      "Batch 8 device: xla:1 time passed: 8.157 time per batch: 1.020\n",
      "Batch 12 device: xla:1 time passed: 11.481 time per batch: 0.957\n",
      "Batch 16 device: xla:1 time passed: 14.771 time per batch: 0.923\n",
      "Batch 20 device: xla:1 time passed: 18.033 time per batch: 0.902\n",
      "Batch 24 device: xla:1 time passed: 21.420 time per batch: 0.892\n",
      "Batch 28 device: xla:1 time passed: 24.713 time per batch: 0.883\n",
      "Batch 32 device: xla:1 time passed: 27.988 time per batch: 0.875\n",
      "Batch 36 device: xla:1 time passed: 31.184 time per batch: 0.866\n",
      "Batch 40 device: xla:1 time passed: 34.432 time per batch: 0.861\n",
      "Batch 44 device: xla:1 time passed: 37.721 time per batch: 0.857\n",
      "Batch 48 device: xla:1 time passed: 41.088 time per batch: 0.856\n",
      "Batch 4 device: xla:1 time passed: 3.824 time per batch: 0.956\n",
      "Batch 8 device: xla:1 time passed: 5.489 time per batch: 0.686\n",
      "Batch 12 device: xla:1 time passed: 7.448 time per batch: 0.621\n",
      "Batch 16 device: xla:1 time passed: 9.346 time per batch: 0.584\n",
      "Batch 20 device: xla:1 time passed: 11.292 time per batch: 0.565\n",
      "Batch 24 device: xla:1 time passed: 13.296 time per batch: 0.554\n",
      "ver 76, epoch 44, fold 0, train ll: 0.0463, val ll: 0.0657, cor: 0.8389, auc: 0.9874, lr: 2e-05\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.130 time per batch: 1.283\n",
      "Batch 8 device: xla:1 time passed: 8.006 time per batch: 1.001\n",
      "Batch 12 device: xla:1 time passed: 11.251 time per batch: 0.938\n",
      "Batch 16 device: xla:1 time passed: 14.472 time per batch: 0.904\n",
      "Batch 20 device: xla:1 time passed: 17.672 time per batch: 0.884\n",
      "Batch 24 device: xla:1 time passed: 21.020 time per batch: 0.876\n",
      "Batch 28 device: xla:1 time passed: 24.240 time per batch: 0.866\n",
      "Batch 32 device: xla:1 time passed: 27.439 time per batch: 0.857\n",
      "Batch 36 device: xla:1 time passed: 30.667 time per batch: 0.852\n",
      "Batch 40 device: xla:1 time passed: 33.880 time per batch: 0.847\n",
      "Batch 44 device: xla:1 time passed: 37.140 time per batch: 0.844\n",
      "Batch 48 device: xla:1 time passed: 40.403 time per batch: 0.842\n",
      "Batch 4 device: xla:1 time passed: 3.765 time per batch: 0.941\n",
      "Batch 8 device: xla:1 time passed: 5.399 time per batch: 0.675\n",
      "Batch 12 device: xla:1 time passed: 7.379 time per batch: 0.615\n",
      "Batch 16 device: xla:1 time passed: 9.395 time per batch: 0.587\n",
      "Batch 20 device: xla:1 time passed: 11.454 time per batch: 0.573\n",
      "Batch 24 device: xla:1 time passed: 13.416 time per batch: 0.559\n",
      "ver 76, epoch 45, fold 0, train ll: 0.0464, val ll: 0.0674, cor: 0.8385, auc: 0.9857, lr: 2e-05\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.026 time per batch: 1.257\n",
      "Batch 8 device: xla:1 time passed: 7.893 time per batch: 0.987\n",
      "Batch 12 device: xla:1 time passed: 11.210 time per batch: 0.934\n",
      "Batch 16 device: xla:1 time passed: 14.506 time per batch: 0.907\n",
      "Batch 20 device: xla:1 time passed: 17.749 time per batch: 0.887\n",
      "Batch 24 device: xla:1 time passed: 21.014 time per batch: 0.876\n",
      "Batch 28 device: xla:1 time passed: 24.268 time per batch: 0.867\n",
      "Batch 32 device: xla:1 time passed: 27.521 time per batch: 0.860\n",
      "Batch 36 device: xla:1 time passed: 30.665 time per batch: 0.852\n",
      "Batch 40 device: xla:1 time passed: 33.900 time per batch: 0.848\n",
      "Batch 44 device: xla:1 time passed: 37.193 time per batch: 0.845\n",
      "Batch 48 device: xla:1 time passed: 40.476 time per batch: 0.843\n",
      "Batch 4 device: xla:1 time passed: 3.825 time per batch: 0.956\n",
      "Batch 8 device: xla:1 time passed: 5.489 time per batch: 0.686\n",
      "Batch 12 device: xla:1 time passed: 7.403 time per batch: 0.617\n",
      "Batch 16 device: xla:1 time passed: 9.396 time per batch: 0.587\n",
      "Batch 20 device: xla:1 time passed: 11.387 time per batch: 0.569\n",
      "Batch 24 device: xla:1 time passed: 13.336 time per batch: 0.556\n",
      "ver 76, epoch 46, fold 0, train ll: 0.0459, val ll: 0.0657, cor: 0.8389, auc: 0.9867, lr: 2e-05\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.089 time per batch: 1.272\n",
      "Batch 8 device: xla:1 time passed: 8.001 time per batch: 1.000\n",
      "Batch 12 device: xla:1 time passed: 11.221 time per batch: 0.935\n",
      "Batch 16 device: xla:1 time passed: 14.614 time per batch: 0.913\n",
      "Batch 20 device: xla:1 time passed: 17.832 time per batch: 0.892\n",
      "Batch 24 device: xla:1 time passed: 20.991 time per batch: 0.875\n",
      "Batch 28 device: xla:1 time passed: 24.249 time per batch: 0.866\n",
      "Batch 32 device: xla:1 time passed: 27.520 time per batch: 0.860\n",
      "Batch 36 device: xla:1 time passed: 30.779 time per batch: 0.855\n",
      "Batch 40 device: xla:1 time passed: 34.012 time per batch: 0.850\n",
      "Batch 44 device: xla:1 time passed: 37.241 time per batch: 0.846\n",
      "Batch 48 device: xla:1 time passed: 40.457 time per batch: 0.843\n",
      "Batch 4 device: xla:1 time passed: 3.740 time per batch: 0.935\n",
      "Batch 8 device: xla:1 time passed: 5.433 time per batch: 0.679\n",
      "Batch 12 device: xla:1 time passed: 7.431 time per batch: 0.619\n",
      "Batch 16 device: xla:1 time passed: 9.365 time per batch: 0.585\n",
      "Batch 20 device: xla:1 time passed: 11.360 time per batch: 0.568\n",
      "Batch 24 device: xla:1 time passed: 13.301 time per batch: 0.554\n",
      "ver 76, epoch 47, fold 0, train ll: 0.0461, val ll: 0.0660, cor: 0.8391, auc: 0.9866, lr: 2e-05\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.115 time per batch: 1.279\n",
      "Batch 8 device: xla:1 time passed: 7.977 time per batch: 0.997\n",
      "Batch 12 device: xla:1 time passed: 11.240 time per batch: 0.937\n",
      "Batch 16 device: xla:1 time passed: 14.504 time per batch: 0.907\n",
      "Batch 20 device: xla:1 time passed: 17.786 time per batch: 0.889\n",
      "Batch 24 device: xla:1 time passed: 21.029 time per batch: 0.876\n",
      "Batch 28 device: xla:1 time passed: 24.235 time per batch: 0.866\n",
      "Batch 32 device: xla:1 time passed: 27.456 time per batch: 0.858\n",
      "Batch 36 device: xla:1 time passed: 30.802 time per batch: 0.856\n",
      "Batch 40 device: xla:1 time passed: 34.044 time per batch: 0.851\n",
      "Batch 44 device: xla:1 time passed: 37.300 time per batch: 0.848\n",
      "Batch 48 device: xla:1 time passed: 40.510 time per batch: 0.844\n",
      "Batch 4 device: xla:1 time passed: 3.671 time per batch: 0.918\n",
      "Batch 8 device: xla:1 time passed: 5.435 time per batch: 0.679\n",
      "Batch 12 device: xla:1 time passed: 7.420 time per batch: 0.618\n",
      "Batch 16 device: xla:1 time passed: 9.364 time per batch: 0.585\n",
      "Batch 20 device: xla:1 time passed: 11.349 time per batch: 0.567\n",
      "Batch 24 device: xla:1 time passed: 13.323 time per batch: 0.555\n",
      "ver 76, epoch 48, fold 0, train ll: 0.0460, val ll: 0.0653, cor: 0.8386, auc: 0.9872, lr: 2e-05\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.089 time per batch: 1.272\n",
      "Batch 8 device: xla:1 time passed: 8.018 time per batch: 1.002\n",
      "Batch 12 device: xla:1 time passed: 11.305 time per batch: 0.942\n",
      "Batch 16 device: xla:1 time passed: 14.612 time per batch: 0.913\n",
      "Batch 20 device: xla:1 time passed: 17.918 time per batch: 0.896\n",
      "Batch 24 device: xla:1 time passed: 21.168 time per batch: 0.882\n",
      "Batch 28 device: xla:1 time passed: 24.305 time per batch: 0.868\n",
      "Batch 32 device: xla:1 time passed: 27.562 time per batch: 0.861\n",
      "Batch 36 device: xla:1 time passed: 30.775 time per batch: 0.855\n",
      "Batch 40 device: xla:1 time passed: 34.046 time per batch: 0.851\n",
      "Batch 44 device: xla:1 time passed: 37.334 time per batch: 0.848\n",
      "Batch 48 device: xla:1 time passed: 40.586 time per batch: 0.846\n",
      "Batch 4 device: xla:1 time passed: 3.926 time per batch: 0.981\n",
      "Batch 8 device: xla:1 time passed: 5.405 time per batch: 0.676\n",
      "Batch 12 device: xla:1 time passed: 7.325 time per batch: 0.610\n",
      "Batch 16 device: xla:1 time passed: 9.273 time per batch: 0.580\n",
      "Batch 20 device: xla:1 time passed: 11.352 time per batch: 0.568\n",
      "Batch 24 device: xla:1 time passed: 13.223 time per batch: 0.551\n",
      "ver 76, epoch 49, fold 0, train ll: 0.0458, val ll: 0.0652, cor: 0.8393, auc: 0.9875, lr: 2e-05\n",
      "total running time 881.536253452301\n",
      "completed epochs: 49 starting now: 6\n",
      "DataSet 4 train size 13042 fold 0\n",
      "adding dummy serieses 168\n",
      "DataSet 4 valid size 6656 fold 0\n",
      "setFeats, augmentation 0\n",
      "dataset train: 13042 valid: 6656 loader train: 407 valid: 208\n",
      "loading model model.b49.f0.v76\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.278 time per batch: 1.320\n",
      "Batch 8 device: xla:1 time passed: 8.314 time per batch: 1.039\n",
      "Batch 12 device: xla:1 time passed: 11.683 time per batch: 0.974\n",
      "Batch 16 device: xla:1 time passed: 15.082 time per batch: 0.943\n",
      "Batch 20 device: xla:1 time passed: 18.556 time per batch: 0.928\n",
      "Batch 24 device: xla:1 time passed: 21.897 time per batch: 0.912\n",
      "Batch 28 device: xla:1 time passed: 25.285 time per batch: 0.903\n",
      "Batch 32 device: xla:1 time passed: 28.635 time per batch: 0.895\n",
      "Batch 36 device: xla:1 time passed: 32.087 time per batch: 0.891\n",
      "Batch 40 device: xla:1 time passed: 35.382 time per batch: 0.885\n",
      "Batch 44 device: xla:1 time passed: 38.711 time per batch: 0.880\n",
      "Batch 48 device: xla:1 time passed: 41.964 time per batch: 0.874\n",
      "Batch 4 device: xla:1 time passed: 3.768 time per batch: 0.942\n",
      "Batch 8 device: xla:1 time passed: 5.421 time per batch: 0.678\n",
      "Batch 12 device: xla:1 time passed: 7.442 time per batch: 0.620\n",
      "Batch 16 device: xla:1 time passed: 9.443 time per batch: 0.590\n",
      "Batch 20 device: xla:1 time passed: 11.443 time per batch: 0.572\n",
      "Batch 24 device: xla:1 time passed: 13.433 time per batch: 0.560\n",
      "ver 76, epoch 50, fold 0, train ll: 0.0457, val ll: 0.0650, cor: 0.8392, auc: 0.9876, lr: 5e-06\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.059 time per batch: 1.265\n",
      "Batch 8 device: xla:1 time passed: 8.036 time per batch: 1.004\n",
      "Batch 12 device: xla:1 time passed: 11.320 time per batch: 0.943\n",
      "Batch 16 device: xla:1 time passed: 14.597 time per batch: 0.912\n",
      "Batch 20 device: xla:1 time passed: 17.844 time per batch: 0.892\n",
      "Batch 24 device: xla:1 time passed: 21.110 time per batch: 0.880\n",
      "Batch 28 device: xla:1 time passed: 24.438 time per batch: 0.873\n",
      "Batch 32 device: xla:1 time passed: 27.776 time per batch: 0.868\n",
      "Batch 36 device: xla:1 time passed: 30.945 time per batch: 0.860\n",
      "Batch 40 device: xla:1 time passed: 34.175 time per batch: 0.854\n",
      "Batch 44 device: xla:1 time passed: 37.393 time per batch: 0.850\n",
      "Batch 48 device: xla:1 time passed: 40.728 time per batch: 0.849\n",
      "Batch 4 device: xla:1 time passed: 3.733 time per batch: 0.933\n",
      "Batch 8 device: xla:1 time passed: 5.358 time per batch: 0.670\n",
      "Batch 12 device: xla:1 time passed: 7.381 time per batch: 0.615\n",
      "Batch 16 device: xla:1 time passed: 9.313 time per batch: 0.582\n",
      "Batch 20 device: xla:1 time passed: 11.277 time per batch: 0.564\n",
      "Batch 24 device: xla:1 time passed: 13.281 time per batch: 0.553\n",
      "ver 76, epoch 51, fold 0, train ll: 0.0464, val ll: 0.0650, cor: 0.8395, auc: 0.9875, lr: 5e-06\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.142 time per batch: 1.286\n",
      "Batch 8 device: xla:1 time passed: 8.234 time per batch: 1.029\n",
      "Batch 12 device: xla:1 time passed: 11.528 time per batch: 0.961\n",
      "Batch 16 device: xla:1 time passed: 14.870 time per batch: 0.929\n",
      "Batch 20 device: xla:1 time passed: 18.128 time per batch: 0.906\n",
      "Batch 24 device: xla:1 time passed: 21.448 time per batch: 0.894\n",
      "Batch 28 device: xla:1 time passed: 24.764 time per batch: 0.884\n",
      "Batch 32 device: xla:1 time passed: 28.015 time per batch: 0.875\n",
      "Batch 36 device: xla:1 time passed: 31.287 time per batch: 0.869\n",
      "Batch 40 device: xla:1 time passed: 34.602 time per batch: 0.865\n",
      "Batch 44 device: xla:1 time passed: 37.916 time per batch: 0.862\n",
      "Batch 48 device: xla:1 time passed: 41.182 time per batch: 0.858\n",
      "Batch 4 device: xla:1 time passed: 3.929 time per batch: 0.982\n",
      "Batch 8 device: xla:1 time passed: 5.450 time per batch: 0.681\n",
      "Batch 12 device: xla:1 time passed: 7.396 time per batch: 0.616\n",
      "Batch 16 device: xla:1 time passed: 9.558 time per batch: 0.597\n",
      "Batch 20 device: xla:1 time passed: 11.331 time per batch: 0.567\n",
      "Batch 24 device: xla:1 time passed: 13.277 time per batch: 0.553\n",
      "ver 76, epoch 52, fold 0, train ll: 0.0464, val ll: 0.0650, cor: 0.8394, auc: 0.9875, lr: 5e-06\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.443 time per batch: 1.361\n",
      "Batch 8 device: xla:1 time passed: 8.066 time per batch: 1.008\n",
      "Batch 12 device: xla:1 time passed: 11.286 time per batch: 0.941\n",
      "Batch 16 device: xla:1 time passed: 14.546 time per batch: 0.909\n",
      "Batch 20 device: xla:1 time passed: 17.775 time per batch: 0.889\n",
      "Batch 24 device: xla:1 time passed: 21.055 time per batch: 0.877\n",
      "Batch 28 device: xla:1 time passed: 24.284 time per batch: 0.867\n",
      "Batch 32 device: xla:1 time passed: 27.568 time per batch: 0.861\n",
      "Batch 36 device: xla:1 time passed: 30.870 time per batch: 0.858\n",
      "Batch 40 device: xla:1 time passed: 34.146 time per batch: 0.854\n",
      "Batch 44 device: xla:1 time passed: 37.365 time per batch: 0.849\n",
      "Batch 48 device: xla:1 time passed: 40.528 time per batch: 0.844\n",
      "Batch 4 device: xla:1 time passed: 3.877 time per batch: 0.969\n",
      "Batch 8 device: xla:1 time passed: 5.485 time per batch: 0.686\n",
      "Batch 12 device: xla:1 time passed: 7.339 time per batch: 0.612\n",
      "Batch 16 device: xla:1 time passed: 9.330 time per batch: 0.583\n",
      "Batch 20 device: xla:1 time passed: 11.502 time per batch: 0.575\n",
      "Batch 24 device: xla:1 time passed: 13.236 time per batch: 0.551\n",
      "ver 76, epoch 53, fold 0, train ll: 0.0459, val ll: 0.0649, cor: 0.8395, auc: 0.9877, lr: 5e-06\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.587 time per batch: 1.397\n",
      "Batch 8 device: xla:1 time passed: 8.487 time per batch: 1.061\n",
      "Batch 12 device: xla:1 time passed: 11.751 time per batch: 0.979\n",
      "Batch 16 device: xla:1 time passed: 14.998 time per batch: 0.937\n",
      "Batch 20 device: xla:1 time passed: 18.303 time per batch: 0.915\n",
      "Batch 24 device: xla:1 time passed: 21.600 time per batch: 0.900\n",
      "Batch 28 device: xla:1 time passed: 24.820 time per batch: 0.886\n",
      "Batch 32 device: xla:1 time passed: 28.117 time per batch: 0.879\n",
      "Batch 36 device: xla:1 time passed: 31.366 time per batch: 0.871\n",
      "Batch 40 device: xla:1 time passed: 34.587 time per batch: 0.865\n",
      "Batch 44 device: xla:1 time passed: 37.946 time per batch: 0.862\n",
      "Batch 48 device: xla:1 time passed: 41.279 time per batch: 0.860\n",
      "Batch 4 device: xla:1 time passed: 3.866 time per batch: 0.967\n",
      "Batch 8 device: xla:1 time passed: 5.447 time per batch: 0.681\n",
      "Batch 12 device: xla:1 time passed: 7.383 time per batch: 0.615\n",
      "Batch 16 device: xla:1 time passed: 9.615 time per batch: 0.601\n",
      "Batch 20 device: xla:1 time passed: 11.339 time per batch: 0.567\n",
      "Batch 24 device: xla:1 time passed: 13.384 time per batch: 0.558\n",
      "ver 76, epoch 54, fold 0, train ll: 0.0460, val ll: 0.0649, cor: 0.8391, auc: 0.9875, lr: 5e-06\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.125 time per batch: 1.281\n",
      "Batch 8 device: xla:1 time passed: 8.040 time per batch: 1.005\n",
      "Batch 12 device: xla:1 time passed: 11.258 time per batch: 0.938\n",
      "Batch 16 device: xla:1 time passed: 14.581 time per batch: 0.911\n",
      "Batch 20 device: xla:1 time passed: 17.908 time per batch: 0.895\n",
      "Batch 24 device: xla:1 time passed: 21.211 time per batch: 0.884\n",
      "Batch 28 device: xla:1 time passed: 24.479 time per batch: 0.874\n",
      "Batch 32 device: xla:1 time passed: 27.735 time per batch: 0.867\n",
      "Batch 36 device: xla:1 time passed: 31.010 time per batch: 0.861\n",
      "Batch 40 device: xla:1 time passed: 34.370 time per batch: 0.859\n",
      "Batch 44 device: xla:1 time passed: 37.565 time per batch: 0.854\n",
      "Batch 48 device: xla:1 time passed: 40.815 time per batch: 0.850\n",
      "Batch 4 device: xla:1 time passed: 3.731 time per batch: 0.933\n",
      "Batch 8 device: xla:1 time passed: 5.375 time per batch: 0.672\n",
      "Batch 12 device: xla:1 time passed: 7.310 time per batch: 0.609\n",
      "Batch 16 device: xla:1 time passed: 9.276 time per batch: 0.580\n",
      "Batch 20 device: xla:1 time passed: 11.249 time per batch: 0.562\n",
      "Batch 24 device: xla:1 time passed: 13.195 time per batch: 0.550\n",
      "ver 76, epoch 55, fold 0, train ll: 0.0461, val ll: 0.0656, cor: 0.8393, auc: 0.9868, lr: 5e-06\n",
      "total running time 381.24479722976685\n",
      "total time 3173.942547082901\n"
     ]
    }
   ],
   "source": [
    "DATA_SMALL = False\n",
    "MIXUP = True\n",
    "weight_decay = 1e-3\n",
    "f = 0\n",
    "stg = time.time()\n",
    "learning_rate = 1e-3\n",
    "model, predictions = train_one(epochs=2*5, bs=bs, fold=f)\n",
    "learning_rate = 2e-4\n",
    "model, predictions = train_one(epochs=2*10, bs=bs, fold=f)\n",
    "learning_rate = 2e-5\n",
    "model, predictions = train_one(epochs=2*7, bs=bs, fold=f)\n",
    "learning_rate = 5e-6\n",
    "model, predictions = train_one(epochs=2*3, bs=bs, fold=f)\n",
    "print('total time', time.time() - stg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epochs: 0 starting now: 10\n",
      "DataSet 4 train size 12982 fold 1\n",
      "adding dummy serieses 108\n",
      "DataSet 4 valid size 6656 fold 1\n",
      "setFeats, augmentation 0\n",
      "dataset train: 12982 valid: 6656 loader train: 405 valid: 208\n",
      "starting from scratch\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 7.305 time per batch: 1.826\n",
      "Batch 8 device: xla:1 time passed: 11.670 time per batch: 1.459\n",
      "Batch 12 device: xla:1 time passed: 15.812 time per batch: 1.318\n",
      "Batch 16 device: xla:1 time passed: 19.974 time per batch: 1.248\n",
      "Batch 20 device: xla:1 time passed: 24.187 time per batch: 1.209\n",
      "Batch 24 device: xla:1 time passed: 28.265 time per batch: 1.178\n",
      "Batch 28 device: xla:1 time passed: 32.365 time per batch: 1.156\n",
      "Batch 32 device: xla:1 time passed: 36.494 time per batch: 1.140\n",
      "Batch 36 device: xla:1 time passed: 40.671 time per batch: 1.130\n",
      "Batch 40 device: xla:1 time passed: 44.918 time per batch: 1.123\n",
      "Batch 44 device: xla:1 time passed: 48.792 time per batch: 1.109\n",
      "Batch 48 device: xla:1 time passed: 52.441 time per batch: 1.093\n",
      "Batch 4 device: xla:1 time passed: 3.758 time per batch: 0.939\n",
      "Batch 8 device: xla:1 time passed: 5.338 time per batch: 0.667\n",
      "Batch 12 device: xla:1 time passed: 7.292 time per batch: 0.608\n",
      "Batch 16 device: xla:1 time passed: 9.280 time per batch: 0.580\n",
      "Batch 20 device: xla:1 time passed: 11.228 time per batch: 0.561\n",
      "Batch 24 device: xla:1 time passed: 13.223 time per batch: 0.551\n",
      "ver 76, epoch 1, fold 1, train ll: 0.1158, val ll: 0.1784, cor: 0.5618, auc: 0.9065, lr: 0.001\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.129 time per batch: 1.282\n",
      "Batch 8 device: xla:1 time passed: 8.084 time per batch: 1.010\n",
      "Batch 12 device: xla:1 time passed: 11.320 time per batch: 0.943\n",
      "Batch 16 device: xla:1 time passed: 14.553 time per batch: 0.910\n",
      "Batch 20 device: xla:1 time passed: 17.889 time per batch: 0.894\n",
      "Batch 24 device: xla:1 time passed: 21.057 time per batch: 0.877\n",
      "Batch 28 device: xla:1 time passed: 24.421 time per batch: 0.872\n",
      "Batch 32 device: xla:1 time passed: 27.728 time per batch: 0.867\n",
      "Batch 36 device: xla:1 time passed: 31.049 time per batch: 0.862\n",
      "Batch 40 device: xla:1 time passed: 34.281 time per batch: 0.857\n",
      "Batch 44 device: xla:1 time passed: 37.585 time per batch: 0.854\n",
      "Batch 48 device: xla:1 time passed: 40.863 time per batch: 0.851\n",
      "Batch 4 device: xla:1 time passed: 3.749 time per batch: 0.937\n",
      "Batch 8 device: xla:1 time passed: 5.467 time per batch: 0.683\n",
      "Batch 12 device: xla:1 time passed: 7.443 time per batch: 0.620\n",
      "Batch 16 device: xla:1 time passed: 9.453 time per batch: 0.591\n",
      "Batch 20 device: xla:1 time passed: 11.433 time per batch: 0.572\n",
      "Batch 24 device: xla:1 time passed: 13.456 time per batch: 0.561\n",
      "ver 76, epoch 2, fold 1, train ll: 0.0646, val ll: 0.0992, cor: 0.7645, auc: 0.9788, lr: 0.001\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.168 time per batch: 1.292\n",
      "Batch 8 device: xla:1 time passed: 8.068 time per batch: 1.008\n",
      "Batch 12 device: xla:1 time passed: 11.231 time per batch: 0.936\n",
      "Batch 16 device: xla:1 time passed: 14.462 time per batch: 0.904\n",
      "Batch 20 device: xla:1 time passed: 17.748 time per batch: 0.887\n",
      "Batch 24 device: xla:1 time passed: 21.016 time per batch: 0.876\n",
      "Batch 28 device: xla:1 time passed: 24.201 time per batch: 0.864\n",
      "Batch 32 device: xla:1 time passed: 27.456 time per batch: 0.858\n",
      "Batch 36 device: xla:1 time passed: 30.666 time per batch: 0.852\n",
      "Batch 40 device: xla:1 time passed: 33.957 time per batch: 0.849\n",
      "Batch 44 device: xla:1 time passed: 37.193 time per batch: 0.845\n",
      "Batch 48 device: xla:1 time passed: 40.441 time per batch: 0.843\n",
      "Batch 4 device: xla:1 time passed: 3.786 time per batch: 0.946\n",
      "Batch 8 device: xla:1 time passed: 5.445 time per batch: 0.681\n",
      "Batch 12 device: xla:1 time passed: 7.444 time per batch: 0.620\n",
      "Batch 16 device: xla:1 time passed: 9.375 time per batch: 0.586\n",
      "Batch 20 device: xla:1 time passed: 11.374 time per batch: 0.569\n",
      "Batch 24 device: xla:1 time passed: 13.332 time per batch: 0.556\n",
      "ver 76, epoch 3, fold 1, train ll: 0.0557, val ll: 0.1146, cor: 0.7250, auc: 0.9564, lr: 0.001\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 4.968 time per batch: 1.242\n",
      "Batch 8 device: xla:1 time passed: 7.953 time per batch: 0.994\n",
      "Batch 12 device: xla:1 time passed: 11.169 time per batch: 0.931\n",
      "Batch 16 device: xla:1 time passed: 14.444 time per batch: 0.903\n",
      "Batch 20 device: xla:1 time passed: 17.681 time per batch: 0.884\n",
      "Batch 24 device: xla:1 time passed: 20.866 time per batch: 0.869\n",
      "Batch 28 device: xla:1 time passed: 24.102 time per batch: 0.861\n",
      "Batch 32 device: xla:1 time passed: 27.309 time per batch: 0.853\n",
      "Batch 36 device: xla:1 time passed: 30.583 time per batch: 0.850\n",
      "Batch 40 device: xla:1 time passed: 33.780 time per batch: 0.844\n",
      "Batch 44 device: xla:1 time passed: 37.125 time per batch: 0.844\n",
      "Batch 48 device: xla:1 time passed: 40.318 time per batch: 0.840\n",
      "Batch 4 device: xla:1 time passed: 3.857 time per batch: 0.964\n",
      "Batch 8 device: xla:1 time passed: 5.415 time per batch: 0.677\n",
      "Batch 12 device: xla:1 time passed: 7.371 time per batch: 0.614\n",
      "Batch 16 device: xla:1 time passed: 9.356 time per batch: 0.585\n",
      "Batch 20 device: xla:1 time passed: 11.377 time per batch: 0.569\n",
      "Batch 24 device: xla:1 time passed: 13.399 time per batch: 0.558\n",
      "ver 76, epoch 4, fold 1, train ll: 0.0558, val ll: 0.0881, cor: 0.7938, auc: 0.9821, lr: 0.001\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 5.323 time per batch: 1.331\n",
      "Batch 8 device: xla:1 time passed: 8.298 time per batch: 1.037\n",
      "Batch 12 device: xla:1 time passed: 11.723 time per batch: 0.977\n",
      "Batch 16 device: xla:1 time passed: 15.192 time per batch: 0.950\n",
      "Batch 20 device: xla:1 time passed: 18.692 time per batch: 0.935\n",
      "Batch 24 device: xla:1 time passed: 22.058 time per batch: 0.919\n",
      "Batch 28 device: xla:1 time passed: 25.517 time per batch: 0.911\n",
      "Batch 32 device: xla:1 time passed: 28.935 time per batch: 0.904\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-e3002f7f7fbc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2e-4\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-c13af1a3040e>\u001b[0m in \u001b[0;36mtrain_one\u001b[0;34m(weight, load_model, epochs, bs, fold)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mCLOUD\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;32mnot\u001b[0m \u001b[0mCLOUD_SINGLE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m             \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_parallel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loop_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m             \u001b[0mtloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtloss_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m             \u001b[0mstate_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_parallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_models\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/torch-xla-nightly/lib/python3.6/site-packages/torch_xla/distributed/data_parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, loop_fn, loader, fixed_batch_size, batchdim)\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mthread\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthreads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m       \u001b[0mthread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m     \u001b[0mpara_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/torch-xla-nightly/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1056\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1057\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1058\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/torch-xla-nightly/lib/python3.6/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1070\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# already determined that the C code is done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "DATA_SMALL = False\n",
    "MIXUP = True\n",
    "weight_decay = 1e-3\n",
    "f = 1\n",
    "stg = time.time()\n",
    "learning_rate = 1e-3\n",
    "model, predictions = train_one(epochs=2*5, bs=bs, fold=f)\n",
    "learning_rate = 2e-4\n",
    "model, predictions = train_one(epochs=2*10, bs=bs, fold=f)\n",
    "learning_rate = 2e-5\n",
    "model, predictions = train_one(epochs=2*7, bs=bs, fold=f)\n",
    "learning_rate = 5e-6\n",
    "model, predictions = train_one(epochs=2*3, bs=bs, fold=f)\n",
    "print('total time', time.time() - stg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "DATA_SMALL = False\n",
    "MIXUP = True\n",
    "weight_decay = 1e-3\n",
    "f = 2\n",
    "stg = time.time()\n",
    "learning_rate = 1e-3\n",
    "model, predictions = train_one(epochs=2*5, bs=bs, fold=f)\n",
    "learning_rate = 2e-4\n",
    "model, predictions = train_one(epochs=2*10, bs=bs, fold=f)\n",
    "learning_rate = 2e-5\n",
    "model, predictions = train_one(epochs=2*7, bs=bs, fold=f)\n",
    "learning_rate = 5e-6\n",
    "model, predictions = train_one(epochs=2*3, bs=bs, fold=f)\n",
    "print('total time', time.time() - stg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07774986 0.07197034 0.0684278  0.06885081 0.06797048 0.06317226\n",
      " 0.06284525 0.06348502 0.06270193 0.06319469 0.06301917 0.06341296\n",
      " 0.06458436 0.06306413 0.06281109 0.06238187 0.0624256  0.06248814\n",
      " 0.06261427 0.06274988 0.06258893 0.0626691  0.06259029 0.06265682\n",
      " 0.06268046]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe1032e3240>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ver = 72\n",
    "res = (pd.read_csv(PATH_WORK/'stats.f{}.v{}'.format(0,ver)).val_loss.values +\n",
    "       pd.read_csv(PATH_WORK/'stats.f{}.v{}'.format(1,ver)).val_loss.values +\n",
    "       pd.read_csv(PATH_WORK/'stats.f{}.v{}'.format(2,ver)).val_loss.values) / 3\n",
    "\n",
    "# 5x1e-3, 10x2e-4, 7x2e-5, 3x5e-6\n",
    "print(res)\n",
    "\n",
    "plt.plot(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08195003 0.06692202 0.0669104  0.06752611 0.06747011 0.06349338\n",
      " 0.0626976  0.06288502 0.0625446  0.0630336  0.06283608 0.06253493\n",
      " 0.06272242 0.06274802 0.06280114 0.06248627 0.06244163 0.06259594\n",
      " 0.06235557 0.06252688 0.06237031 0.06266632 0.0624724  0.0625555\n",
      " 0.06249723]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe0f6999cc0>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ver = 73\n",
    "res = (pd.read_csv(PATH_WORK/'stats.f{}.v{}'.format(0,ver)).val_loss.values +\n",
    "       pd.read_csv(PATH_WORK/'stats.f{}.v{}'.format(1,ver)).val_loss.values +\n",
    "       pd.read_csv(PATH_WORK/'stats.f{}.v{}'.format(2,ver)).val_loss.values) / 3\n",
    "\n",
    "# 5x1e-3, 10x2e-4, 7x2e-5, 3x5e-6\n",
    "print(res)\n",
    "\n",
    "plt.plot(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.07336126 0.06791653 0.06672474 0.06621773 0.06522342 0.06269679\n",
      " 0.06271483 0.06261532 0.06283813 0.06239797 0.06266095 0.06256483\n",
      " 0.06280528 0.0622756  0.062841   0.0622916  0.06234851 0.06230284\n",
      " 0.06239877 0.06240432 0.06237384 0.06242769 0.06245325 0.06240852\n",
      " 0.06238066]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f5f71da3198>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD4CAYAAADsKpHdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3deXRc9X338fdXGm0jb5IsG9uSvMSWwQZjG4WlLKJ1APMk4ECg4NKEntIS0vBkoYGQ5jQQkqYhpUAWHp7SklPyEPaE1NQukEIwpKyy8YJtbIRXeUO2ZMtYu+b7/DFXziAka7TYI839vM7RmTt3fnfm+ztj38/ce3/3XnN3REREMlJdgIiIDA0KBBERARQIIiISUCCIiAigQBARkUAk1QX0xdixY33KlCmpLkNEZFhZsWLFPncv7q3dsAqEKVOmUFVVleoyRESGFTPblkw77TISERFAgSAiIgEFgoiIAAoEEREJKBBERARQIIiISECBICIiQEgC4RevbeWZ1btSXYaIyJAWikB47M0d/HplTarLEBEZ0kIRCGWFUbbXNaa6DBGRIS0UgVBamEdNfROxmO4OJyLSk1AEQllhlJb2GLUftqS6FBGRISsUgVBSGAVgh3YbiYj0KBSBUBYEgo4jiIj0LBSBMGlMHgA76ppSXImIyNAVikDIzcpk/KgcbSGIiBxFKAIB4ruNdtQrEEREehKaQCgtiFKjLQQRkR6FJxAKo+xuaKalvSPVpYiIDEmhCgR32HWgOdWliIgMSaEJBA09FRE5utAEQmlh59BTBYKISHdCEwjjR+aSnZmhQBAR6UFoAiEjwygpyNPQUxGRHoQmECB+TSOdrSwi0r1QBUJZYZ4OKouI9CBUgVBaEOVgUxsHm9pSXYqIyJATqkAo02WwRUR6FKpAKA0CoUYHlkVEPiapQDCzhWa20cyqzezWbl7PMbPHg9ffMLMpwfxrzGxVwl/MzOaaWdTMlprZu2a2zsx+OLjd6l6pTk4TEelRr4FgZpnAfcDFwCxgsZnN6tLsOqDe3acD9wB3Arj7L919rrvPBT4PbHX3VcEyd7n7icA84Gwzu3hQenQUo/OyGJUb0UgjEZFuJLOFcDpQ7e6b3b0VeAxY1KXNIuChYPopYIGZWZc2i4FHAdy90d1/F0y3AiuBkv51oW9KC6PaQhAR6UYygTAJ2JHwvCaY120bd28HDgJFXdpcRRAIicxsDHAJ8EJyJQ+M7osgItK9ZAKh6y99AO9LGzM7A2h093c+spBZhHhI/MTdN3f74WbXm1mVmVXV1tYmUe7RlRZGqalvIhbr2gURkXBLJhBqgNKE5yXArp7aBCv50UBdwutX083WAfAA8J6739vTh7v7A+5e4e4VxcXFSZR7dKWFUVrbY3xwqGXA7yUikk6SCYS3gBlmNtXMsomv3Jd0abMEuDaYvgJ40d0dwMwygCuJH3s4wsy+Tzw4vtb/8vuutCC46ql2G4mIfESvgRAcE7gReA7YADzh7uvM7A4zuzRo9iBQZGbVwE1A4tDU84CaxF1CZlYCfJv4qKWVwZDUvxqUHvXiyH0R9isQREQSRZJp5O7LgGVd5n0nYbqZ+FZAd8u+BJzZZV4N3R93OOYmFeRhpi0EEZGuQnWmMkBOJJPxI3M19FREpIvQBQLEdxvV6OQ0EZGPCGUglBTqRjkiIl2FMhDKCqPsaWimpb0j1aWIiAwZoQyE0oIo7rCzXruNREQ6hTIQyop01VMRka5CGQilBcGNcrSFICJyRCgDYdzIHLIjGbpzmohIglAGQkaGUVKQp0AQEUkQykCA+G4jHUMQEfmD0AZCWWFUWwgiIglCGwilhXk0NLdzsLEt1aWIiAwJoQ2Ezque6oxlEZG40AZCSefQU+02EhEBQhwIOjlNROSjQhsIo3KzGJ2XpV1GIiKB0AYCxA8sb9dlsEVEgJAHQvy+CNpCEBGBkAdCaUGUmvomYjFPdSkiIikX7kAojNLaEWPvoeZUlyIiknKhDwSAHTqOICIS7kDoPDlNQ09FREIeCBPH5GKmk9NERCDkgZATyeSEUbkKBBERQh4IED+OoJPTREQUCLovgohIIPSBUFYYZW9DC81tHakuRUQkpZIKBDNbaGYbzazazG7t5vUcM3s8eP0NM5sSzL/GzFYl/MXMbG7w2mlmtjZY5idmZoPZsWSVFuYBsPOAhp6KSLj1GghmlgncB1wMzAIWm9msLs2uA+rdfTpwD3AngLv/0t3nuvtc4PPAVndfFSxzP3A9MCP4WzgI/ekzDT0VEYlLZgvhdKDa3Te7eyvwGLCoS5tFwEPB9FPAgm5+8S8GHgUwswnAKHd/zd0d+AXw2X72YUA6T07TNY1EJOySCYRJwI6E5zXBvG7buHs7cBAo6tLmKoJACNrX9PKeAJjZ9WZWZWZVtbW1SZTbN8UjcsiOZGgLQURCL5lA6G7ffterwR21jZmdATS6+zt9eM/4TPcH3L3C3SuKi4uTKLdvMjKM0oI8Xb5CREIvmUCoAUoTnpcAu3pqY2YRYDRQl/D61fxh66CzfUkv73nclBZq6KmISDKB8BYww8ymmlk28ZX7ki5tlgDXBtNXAC8GxwYwswzgSuLHHgBw993AITM7MzjW8AXgPwbUkwEo08lpIiK9B0JwTOBG4DlgA/CEu68zszvM7NKg2YNAkZlVAzcBiUNTzwNq3H1zl7f+EvBvQDXwPvBfA+rJAJQWRDnU3M7BxrZUlSAiknKRZBq5+zJgWZd530mYbia+FdDdsi8BZ3Yzvwo4uQ+1HjOlCUNPT4mOTnE1IiKpEfozleEPJ6dpt5GIhJkCgY9uIYiIhJUCARiVm8WYaJYugy0ioaZACOiqpyISdgqEQFlhlJp6nZwmIuGlQAiUFOZRU99IR6zbE6ZFRNKeAiFQVhilrcPZ29Cc6lJERFJCgRAoLYiPNNKBZREJKwVCQPdFEJGwUyAEJo7Jwwx26MCyiISUAiGQHclgwqhc7TISkdBSICQoLYwqEEQktBQICXRfBBEJMwVCgrLCKB8caqG5rSPVpYiIHHcKhASdVz3VGcsiEkYKhASdQ091HEFEwkiBkODIyWm6L4KIhJACIUHxyBxyIhls369AEJHwUSAkMLP40FNtIYhICCkQuigtyGN7nQ4qi0j4KBC6KCuMUlPXiLsugy0i4aJA6KK0MMqhlnYONLaluhQRkeNKgdBFaaFGGolIOCkQuvjDfRF0HEFEwkWB0EXn2cq6ppGIhI0CoYuRuVkURLO0y0hEQkeB0A1dBltEwiipQDCzhWa20cyqzezWbl7PMbPHg9ffMLMpCa/NMbPXzGydma01s9xg/uLg+Roze9bMxg5WpwZKgSAiYdRrIJhZJnAfcDEwC1hsZrO6NLsOqHf36cA9wJ3BshHgYeAGd58NnA+0BfN/DPyxu88B1gA3DkqPBkFpQZSdB5roiOlcBBEJj2S2EE4Hqt19s7u3Ao8Bi7q0WQQ8FEw/BSwwMwMuBNa4+2oAd9/v7h2ABX/5QbtRwK4B92aQlBVGaetw9jQ0p7oUEZHjJplAmATsSHheE8zrto27twMHgSKgHHAze87MVprZLUGbNuBLwFriQTALeLC7Dzez682sysyqamtrk+7YQHSONNJuIxEJk2QCwbqZ13VfSk9tIsA5wDXB42VmtsDMsogHwjxgIvFdRt/q7sPd/QF3r3D3iuLi4iTKHbgZ40aSYbBs7e7j8nkiIkNBMoFQA5QmPC/h47t3jrQJjg+MBuqC+cvdfZ+7NwLLgPnAXAB3f9/jFw16AvijAfRjUJ0wOpc/P3MyD7++jXW7Dqa6HBGR4yKZQHgLmGFmU80sG7gaWNKlzRLg2mD6CuDFYEX/HDDHzKJBUFQC64GdwCwz6/zJfwGwYWBdGVx/e8FMCqLZ3PYf63ShOxEJhV4DITgmcCPxlfsG4Al3X2dmd5jZpUGzB4EiM6sGbgJuDZatB+4mHiqrgJXuvtTddwHfBV42szXEtxh+MLhdG5jR0Sy+ufBEqrbV8/TbO1NdjojIMWfD6ddvRUWFV1VVHbfPi8Wcy+9/lZr6Jl78RiWjcrOO22eLiAwWM1vh7hW9tdOZykeRkWHcsWg2+w+3cO9v30t1OSIix5QCoRdzSsaw+PQyHnptK+/uaUh1OSIix4wCIQk3XziTkbkRHWAWkbSmQEhCQX42N180kze21LFk9ZA5oVpEZFApEJJ09SfLOGXSaH6wbAMftrSnuhwRkUGnQEhSZnCAeW9DCz95QQeYRST9KBD6YF5ZAVdVlPLz32+h+oNDqS5HRGRQKRD66JaFM4lmZ3LbEh1gFpH0okDoo6IROXzjopn8T/V+lq3dk+pyREQGjQKhH645YzKzJozi+0vXc1gHmEUkTSgQ+qHzAPPug83c97vqVJcjIjIoFAj9VDGlkMvnT+JfX9nM5toPU12OiMiAKRAG4FsXn0RuJJPbn1mvA8wiMuwpEAageGQOX7+gnJc31fL8+r2pLkdEZEAUCAP0hbMmM3P8SO54Zj1NrR2pLkdEpN8UCAMUyczgjkWz2Xmgiftf0gFmERm+FAiD4IxpRSyaO5H/89L73Pe7ajpiOp4gIsNPJNUFpIvvf/ZkOmLOPz23kZc31XLPVXOZOCYv1WWJiCRNWwiDZGRuFj9dPI+7rjyVd3YeZOG9L7N0ze5UlyUikjQFwiAyM644rYRlXz2XacUj+PIjK7n5ydW6XLaIDAsKhGNgclE+T95wFl/5k+n8amUNn/7JK7y9vT7VZYmIHJUC4RjJyszgpgtn8tj1Z9He4Vzxf1/jZy++pwPOIjJkKRCOsdOnFrLsq+fy6VMmcNfzm1j8wOvU1DemuiwRkY9RIBwHo/Oy+PHVc7nnqlNZv7uBi3/8iu7NLCJDjgLhODEzLptXwrKvnMuMcSP4yqNvc9MTqzjU3Jbq0kREAAXCcVdWFOWJL57FVxfM4Ddv7+QLP3+Tto5YqssSEUkuEMxsoZltNLNqM7u1m9dzzOzx4PU3zGxKwmtzzOw1M1tnZmvNLDeYn21mD5jZJjN718w+N1idGuoimRl8/YJy7r16Hm9vP8BPXngv1SWJiPQeCGaWCdwHXAzMAhab2awuza4D6t19OnAPcGewbAR4GLjB3WcD5wOd+0i+DXzg7uXB+y4fcG+GmUtPncgVp5Vw3++qeXNLXarLEZGQS2YL4XSg2t03u3sr8BiwqEubRcBDwfRTwAIzM+BCYI27rwZw9/3u3nlJ0L8E/jGYH3P3fQPryvB0+6WzKSuM8vXHV3GwSccTRCR1kgmEScCOhOc1wbxu27h7O3AQKALKATez58xspZndAmBmY4LlvhfMf9LMxg+gH8PWiJwIP756Hnsbmvm7p9fqRjsikjLJBIJ1M6/rWqunNhHgHOCa4PEyM1sQzC8B/sfd5wOvAXd1++Fm15tZlZlV1dbWJlHu8HNq6Ri+fkE5S9fs5qkVNakuR0RCKplAqAFKE56XAF0H0R9pExw3GA3UBfOXu/s+d28ElgHzgf1AI/B0sPyTwfyPcfcH3L3C3SuKi4uT6tRwdEPlJzhjaiG3LVnH1n2HU12OiIRQMoHwFjDDzKaaWTZwNbCkS5slwLXB9BXAix7f9/EcMMfMokFQVALrg9eeIX6QGWABsH5APRnmMjOMe66aS1ZmBl997G0NRRWR467XQAiOCdxIfOW+AXjC3deZ2R1mdmnQ7EGgyMyqgZuAW4Nl64G7iYfKKmCluy8NlvkmcLuZrQE+D/zt4HVreJo4Jo9/vPwUVtcc5N7/3pTqckQkZGw4HcSsqKjwqqqqVJdxzN3y1GqeXFHDo399JmdOK0p1OSIyzJnZCnev6K2dzlQegm67ZDZTivLjQ1EbNRRVRI4PBcIQlJ8T4cdXz6X2UIuGoorIcaNAGKLmlIzhby+cydK1u3lSQ1FF5DhQIAxhXzxvGmdNK+L2JevYoqGoInKMKRCGsIwM4+6rTiUrM4OvaSiqiBxjCoQhbsLoPO78XHwo6j2/1VBUETl2FAjDwMKTJ3D1J0u5f/n7vPb+/lSXIyJpSoEwTHznkllMLcrnpidWcaCxNdXliEgaUiAME9Hs+FVRdx9s5tE3d/S+gIhIHykQhpFTSkZz0oRRLN/0QapLEZE0pEAYZirLi6naWs+HLe2pLkVE0owCYZipLC+mPea8Wh3KG8yJyDGkQBhmTptcQH52Jss3pefNgkQkdRQIw0x2JIOzp49l+aZaXeNIRAaVAmEYqpxZTE19E5t1OQsRGUQKhGHovBnxW4ku36jdRiIyeBQIw1BpYZRPFOfrOIKIDCoFwjBVWT6O1zfvp7mtI9WliEiaUCAMU5Uzi2lpj/HGlrpUlyIiaUKBMEydMbWQnEiGjiOIyKBRIAxTuVmZnDmtiJd0GQsRGSQKhGGssryYzbWH2VHXmOpSRCQNKBCGscqZwfBTjTYSkUGgQBjGpo3Np6QgT4EgIoNCgTCMmRmV5cW8Wr2P1nbdb1lEBkaBMMxVlhdzuLWDFdvqU12KiAxzCoRh7o+mjyWSYdptJCIDllQgmNlCM9toZtVmdms3r+eY2ePB62+Y2ZSE1+aY2Wtmts7M1ppZbpdll5jZOwPtSFiNyIlQMaVAgSAiA9ZrIJhZJnAfcDEwC1hsZrO6NLsOqHf36cA9wJ3BshHgYeAGd58NnA+0Jbz35cCHA+9GuFWWj2PD7gb2NjSnuhQRGcaS2UI4Hah2983u3go8Bizq0mYR8FAw/RSwwMwMuBBY4+6rAdx9v7t3AJjZCOAm4PsD70a4VZbHh5++rK0EERmAZAJhErAj4XlNMK/bNu7eDhwEioBywM3sOTNbaWa3JCzzPeCfgaOeVWVm15tZlZlV1dZqhdedkyaMpHhkjnYbiciAJBMI1s28rrfq6qlNBDgHuCZ4vMzMFpjZXGC6uz/d24e7+wPuXuHuFcXFxUmUGz6dw09feW8fHTHdRU1E+ieZQKgBShOelwC7emoTHDcYDdQF85e7+z53bwSWAfOBs4DTzGwr8Hug3Mxe6n83pLK8mINNbayuOZDqUkRkmEomEN4CZpjZVDPLBq4GlnRpswS4Npi+AnjR4zf8fQ6YY2bRICgqgfXufr+7T3T3KcS3HDa5+/kD7054nTtjLBmmu6iJSP/1GgjBMYEbia/cNwBPuPs6M7vDzC4Nmj0IFJlZNfEDxbcGy9YDdxMPlVXASndfOvjdkDHRbOaWjtFxBBHpt0gyjdx9GfHdPYnzvpMw3Qxc2cOyDxMfetrTe28FTk6mDjm6yvJx3PvCJuoPt1KQn53qckRkmNGZymmkcmYx7vBK9b5UlyIiw5ACIY2cMmk0BdEsXtqom+aISN8pENJIZoZx7oxiXt60j5iGn4pIHykQ0kxleTH7Pmxh/e6GVJciIsOMAiHNnFs+FtBd1ESk7xQIaWbcyFxmTxylQBCRPlMgpKHK8mJWbqunobmt98YiIgEFQhqqLC+mPea8Wr0/1aWIyDCiQEhD8ycXMCInot1GItInCoQ0lJWZwdnTi3h5Uy3xS0qJiPROgZCmKsvHsfNAE+/X6oZ0IpIcBUKaOi8YfvqSrn4qIklSIKSpkoIo08eN0HEEEUmaAiGNVZYX88aWOppaO1JdiogMAwqENFZZXkxre4zXtwyd4adNrR08t24PNz2xis/d/yr/vX5vqksasJ0Hmth9sOmYfsavV9Zw4T3LefadPcf0cyTckrofggxPp08tJDcrg+Uba/njmeNSVkfd4VZe2LCX59fv5ZX3amluizE6L4vReVn81S+quGj2eG67ZDYTx+SlrMa+2nWgiWVrd/Ofa3azascBsjMz+PanT+ILZ03GrLtbjPdPW0eMf1i6gX9/dSsjciLc8PAKvnjeNG6+aCaRTP2ek8GlQEhjuVmZnDWtiN+u38usiaNo64jR2h6jrSNGW4fTEky3Jjy2Bo/52REmj40ypSifyUVRJhflMyIn+X8uO+oa+e36vTy/fg9vbqkj5jBxdC5Xf7KMC2eP55NTCnGHB3+/hR+/sIlP3b2cmy4o5y/+aMqQXdHtbWg+EgIrttUDMHviKG5ZOJOqrfXctmQdr72/nzuvmMPovKwBf17toRa+/MhK3txSx1+ePZVvXFTOD5Zt4F9e3szbOw7ws8XzGDcqd8CfI9LJhtM49YqKCq+qqkp1GcPKo29u51u/Xtvj69mZGWRlGtmRDLIyM8iOZJCdmUFDcxv7Pmz9SNuxI3KYEoRDPCTigTGlKJ9ReRE27D7E8+v38Py6vUeutnriCSO5cNZ4Lpx9ArMnjur21/OOukZuW7KOF9/9gJMmjOIHl53MvLKCAfW7rSPG8o21LFu7m0imMbkon9LCKJML43WPzstK6pd87aEW/uudeAi8tbUO93ifPjNnAp+eM5GpY/MBiMWcB3+/hTuffZcTRufysz+bz9zSMf2uf9WOA9zw/1ZwoKmVH14+h8/Om3TktV+vrOHvnl7LyNwsfrZ4HmdMK+r350g4mNkKd6/otZ0CIb25OzvqmjDjyMo+q/Mx0466UvywpZ1t+w+zbX8jW/cfZtu++OP2ukZ2H2z+SNu8rEya2jowg4rJBVw46wQumDWeKcEKM5k6n1u3h9uXrGfvoWauOaOMmy86sU+/tN2ddbsa+NXKGpas2sX+w60URLOIZGZQe6jlI21H5kbioVYYBEVRlLLC+F9uVibPr9/D0jW7eX3zfmIOM8aN4DNzJvLpOROYPm5EjzWs3F7P/37kbT441Mw3F57IdedM7fMupMff2s7f/2Yd40bl8C+fP43ZE0d/rM27exr40sMr2V7XyDcXzuSvz502qLuqJL0oEOSYam7rYHtdI1v3xQNj54EmTjxhJAtOGk/xyJx+v++HLe3c/fwm/v3VLRTm5/D3nzmJS0+deNSV3QcNzfxm1U5+tWInG/ceIjszgwUnjeNz80uonFlMVmYGja3tbK9rZPv+RrbXNbIteNxe10hNfSNtHR//fzBtbD6fmTOBz5w6kfLxI5Puw8HGNm5+ajXPr9/Lp04ax11XnsqYaO/3uG5p7+C7z6znkTe2c870sfx08byj3hv7UHMbNz+5hmfX7WHh7BP40ZVzGJU78F1Vkn4UCDKsvbPzIN9+ei2raw5yzvSxfO+zJx/ZPQPxQHp+/V5+taKGV96rJeYwt3QMnzuthEvmTEhqBdypI+bsPtjE9v2NbKtr5GBTG+fNKOakCSP7/avb3fn3V7fyg2UbKB6Rw0//bB6nTS7ssf3ehma+9PAKVm4/wA2Vn+Dmi2aSmdH7Z7s7//bKFn747LuUFUa5/8/nc+IJo/pVs6QvBYIMex0x55E3tvGjZzfS0hHjy+dP58xphTz99k6WrtnNoZZ2Jo7O5bL5k7h8fgmfKO55V06qrKk5wI2PvM3OA01848KZfPG8aWR0WdG/tbWOv/nlSg63tPOjK+bwmTkT+/w5b26p48uPrORQcxs/uOwULp9fMlhdGHTu8QENMXdiDjF3PEbwPD7PE16LueMev0ZXXnYm0exMsgYw8KCptYO6xlbqD7dyoLGNusZWDjS2Un+4jYbmNpraOmhu66ClLXZkOv4Xo7m9g+bWDprbY0fmRzIzGJWbxai8SPAYH0E3KjfCqLysj702KjdCNDtCXlYmudkZ5GVlkpeVeUwHUygQJG180NDM95Zu4JnVuwCIZmey8OQTuGJ+CWdOK/rYCnaoaWhu49ZfrWHZ2j1Ulhdz95+eStGIHNydh1/fxnefWc+kgjwe+HwFM09IftdUVx8caubGR97mzS11XHNGGd+5ZBY5kcykl29u66DucCuRTGNUbha5Wckv21VbR4xdB5o+smuucwtsR10jH7a09/u9AbIyjbysTKLZEaLZmUeCIi87QjQrPp2TlUFDczv1h1upb2zjQGMrdYdbaWmP9fi++cF75WZ1/mWQG0mY7mZ+WyxGQ1M7Dc1tNDQFf83tNDS1cbCpjfYk72/e2ae87HhA5CZM52Vlct818/v9nSgQJO28vnk/exua+dRJ48nvwxDYocDdefiN7XzvP9dTEM3in644lWdW7+LJFTX88cxi7r1qHqOjA9//394R45+e38i/LN/MnJLRfPfS2bTHnLrDrR/5qz/cyv7DrdQ3trL/w/hjY5cz2rMjGUd+6Y7Oyzryazc+HczLzSIvO5NdB5qDFX980MGuA810JKwIsyMZlBbkMbkon7LCKMUjc4hkGBlmmEGGGRkGGRnxgQ4ZCfPMDAPaOpzG1naaWjtobOuIP7a209jaOd05Pz6vpT3GyNwIBdFsCqJZ8cf87CPPx0SzKcwPXsvPZkxe1qD/Snd3mttifwiL5nhINLXGtz6a2uJbHJ3TTa3xrY7O6c4tlKa2Dn7zN2f3uz4FgsgQtG7XQW585G227DsMwFf+ZDpf+1T5oG/lPPvOHm5+cjWHuvklHs3OpCCaTdGI+MqxKD++oiwMVpYd7gm/dOMrsIam9vhj8x9+AXd0+eVbmJ99ZGhvWWGUsqJguijK+JG5Q35LLp0pEESGqA9b2vnpi+9x+pRCFpw0/ph9Tk19Iyu21VMQ/BLu/BvIrqBO7s7h1o7g124740blaoTTEJZsIAyv7W6RNDAiJ8K3Lj7pmH9OSUGUkoLoMXlvM2NETqRPZ6/L0JfUDikzW2hmG82s2sxu7eb1HDN7PHj9DTObkvDaHDN7zczWmdlaM8s1s6iZLTWzd4P5Pxy8LomISH/0GghmlgncB1wMzAIWm9msLs2uA+rdfTpwD3BnsGwEeBi4wd1nA+cDbcEyd7n7icA84Gwzu3jg3RERkf5KZgvhdKDa3Te7eyvwGLCoS5tFwEPB9FPAAouf0XMhsMbdVwO4+35373D3Rnf/XTCvFVgJDN2B0yIiIZBMIEwCdiQ8rwnmddvG3duBg0ARUA64mT1nZivN7Jaub25mY4BLgBe6+3Azu97MqsysqrZWd/8SETlWkgmE7saKdR2a1FObCHAOcE3weJmZLTiyUHyX0qPAT9x9c3cf7u4PuHuFu1cUFxcnUa6IiPRHMoFQA5QmPC8BdvXUJljJjwbqgvnL3X2fuzcCy4D5Ccs9ALzn7vf2r3wRERksyQTCW8AMM5tqZtnA1V7YwLsAAANmSURBVMCSLm2WANcG01cAL3r8BIfngDnBqKIIUAmsBzCz7xMPjq8NvBsiIjJQvQZCcEzgRuIr9w3AE+6+zszuMLNLg2YPAkVmVg3cBNwaLFsP3E08VFYBK919qZmVAN8mPmpppZmtMrO/GuS+iYhIHwyrM5XNrBbY1s/FxwL7BrGc4STMfYdw9z/MfYdw9z+x75PdvdeDsMMqEAbCzKqSOXU7HYW57xDu/oe57xDu/ven70PzbuYiInLcKRBERAQIVyA8kOoCUijMfYdw9z/MfYdw97/PfQ/NMQQRETm6MG0hiIjIUSgQREQECEEg9HYvh3RnZluD+1CsMrO0v92cmf3czD4ws3cS5hWa2W/N7L3gsSCVNR4rPfT9djPbGXz/q8zsf6WyxmPFzErN7HdmtiG4x8pXg/lp/90fpe99/u7T+hhCcC+HTcAFxK+r9Baw2N3Xp7Sw48jMtgIV7h6Kk3PM7DzgQ+AX7n5yMO9HQJ27/zD4UVDg7t9MZZ3HQg99vx340N3vSmVtx5qZTQAmuPtKMxsJrAA+C/wFaf7dH6Xvf0ofv/t030JI5l4Okkbc/WXiF1ZMlHi/joeI/2dJOz30PRTcfbe7rwymDxG/zM4kQvDdH6XvfZbugZDMvRzSnQPPm9kKM7s+1cWkyHh33w3x/zzAuBTXc7zdaGZrgl1KabfLpKvgFr7zgDcI2Xffpe/Qx+8+3QMhmXs5pLuz3X0+8VugfjnYrSDhcT/wCWAusBv459SWc2yZ2QjgV8DX3L0h1fUcT930vc/ffboHQjL3ckhr7r4rePwAeJr4brSw2RvsZ+3c3/pBius5btx9b3Db2hjwr6Tx929mWcRXiL90918Hs0Px3XfX9/589+keCMncyyFtmVl+cJAJM8snfo/rd46+VFpKvF/HtcB/pLCW46pzZRi4jDT9/oN7uD8IbHD3uxNeSvvvvqe+9+e7T+tRRgDBUKt7gUzg5+7+Dyku6bgxs2nEtwogfjvTR9K9/2b2KHA+8Uv/7gVuA34DPAGUAduBK9097Q6+9tD384nvMnBgK/DFzn3q6cTMzgFeAdYCsWD23xHfl57W3/1R+r6YPn73aR8IIiKSnHTfZSQiIklSIIiICKBAEBGRgAJBREQABYKIiAQUCCIiAigQREQk8P8B30Me430AljsAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ver = 74\n",
    "res = (pd.read_csv(PATH_WORK/'stats.f{}.v{}'.format(0,ver)).val_loss.values +\n",
    "       pd.read_csv(PATH_WORK/'stats.f{}.v{}'.format(1,ver)).val_loss.values +\n",
    "       pd.read_csv(PATH_WORK/'stats.f{}.v{}'.format(2,ver)).val_loss.values) / 3\n",
    "\n",
    "# 5x1e-3, 10x2e-4, 7x2e-5, 3x5e-6\n",
    "print(res)\n",
    "\n",
    "plt.plot(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.08759543 0.07350885 0.07103133 0.06781943 0.08776392 0.06386395\n",
      " 0.06370772 0.06874262 0.06357561 0.06322936 0.06313038 0.06307746\n",
      " 0.06421412 0.06311881 0.06331547 0.06188674 0.06199789 0.06180143\n",
      " 0.06188602 0.06173918 0.06189277 0.06183202 0.06190042 0.06189958\n",
      " 0.06188536]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fb199979128>]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ver = 75\n",
    "res = (pd.read_csv(PATH_WORK/'stats.f{}.v{}'.format(0,ver)).val_loss.values +\n",
    "       pd.read_csv(PATH_WORK/'stats.f{}.v{}'.format(1,ver)).val_loss.values +\n",
    "       pd.read_csv(PATH_WORK/'stats.f{}.v{}'.format(2,ver)).val_loss.values) / 3\n",
    "\n",
    "# 5x1e-3, 10x2e-4, 7x2e-5, 3x5e-6\n",
    "print(res)\n",
    "\n",
    "plt.plot(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6494, 60, 6)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09629448, 0.02383027, 0.0463406 , 0.0402737 , 0.04893126,\n",
       "       0.05532886], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.mean((0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe103bf5630>]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(predictions.mean(0)[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OOF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oof_one(num_iter=1, bs=100, fold=0):\n",
    "    \n",
    "    st0 = time.time()\n",
    "    \n",
    "    cur_epoch = getCurrentBatch(fold=fold)\n",
    "    if cur_epoch is None: cur_epoch = 0\n",
    "    print('completed epochs:', cur_epoch, 'iters starting now:', num_iter)\n",
    "    \n",
    "    setSeeds(SEED + cur_epoch)\n",
    "    \n",
    "    val_ds = RSNA_DataSet(ids_df, mode='valid', bs=bs, fold=fold)\n",
    "    \n",
    "    loader_val = D.DataLoader(val_ds, num_workers=16 if (CLOUD and not CLOUD_SINGLE) else 0, batch_size=bs, \n",
    "                              shuffle=True)\n",
    "    print('dataset valid:', len(val_ds), 'loader valid:', len(loader_val))\n",
    "    \n",
    "    #model = TabularModel(n_cont = len(meta_cols), feat_sz=feat_sz, fc_drop_p=0)\n",
    "    model = ResNetModel(n_cont = len(meta_cols), feat_sz=feat_sz)\n",
    "    \n",
    "    model_file_name = modelFileName(return_last=True, fold=fold)\n",
    "    if model_file_name is not None:\n",
    "        print('loading model', model_file_name)\n",
    "        state_dict = torch.load(PATH_WORK/'models'/model_file_name)\n",
    "        model.load_state_dict(state_dict)\n",
    "    else:\n",
    "        print('starting from scratch')\n",
    "    \n",
    "    if (not CLOUD) or CLOUD_SINGLE:\n",
    "        model = model.to(device)\n",
    "    else:\n",
    "        model_parallel = dp.DataParallel(model, device_ids=devices)\n",
    "    \n",
    "    loc_data = val_ds.metadata.copy()\n",
    "    series_counts = loc_data.index.value_counts()\n",
    "    \n",
    "    loc_data['orig_idx'] = np.arange(len(loc_data))\n",
    "    loc_data = loc_data.sort_values(['SeriesInstanceUID','pos_idx1'])\n",
    "    loc_data['my_order'] = np.arange(len(loc_data))\n",
    "    loc_data = loc_data.sort_values(['orig_idx'])\n",
    "    \n",
    "    preds = []\n",
    "    \n",
    "    for i in range(num_iter):\n",
    "        \n",
    "        val_ds.setFeats(-1)\n",
    "\n",
    "        if CLOUD and (not CLOUD_SINGLE):\n",
    "            results = model_parallel(val_loop_fn, loader_val)\n",
    "            predictions = np.concatenate([results[i][0] for i in range(MAX_DEVICES)])\n",
    "            indices = np.concatenate([results[i][1] for i in range(MAX_DEVICES)])\n",
    "            offsets = np.concatenate([results[i][2] for i in range(MAX_DEVICES)])\n",
    "        else:\n",
    "            predictions, indices, offsets = val_loop_fn(model, loader_val, device)\n",
    "\n",
    "        predictions = predictions[np.argsort(indices)]\n",
    "        offsets = offsets[np.argsort(indices)]\n",
    "        assert len(predictions) == len(loc_data.index.unique())\n",
    "        assert len(predictions) == len(offsets)\n",
    "        assert np.all(indices[np.argsort(indices)] == np.array(range(len(predictions))))\n",
    "\n",
    "        val_results = []\n",
    "        for k, series in enumerate(np.sort(loc_data.index.unique())):\n",
    "            cnt = series_counts[series]\n",
    "            assert (offsets[k] + cnt) <= 60\n",
    "            val_results.append(predictions[k,offsets[k]:(offsets[k] + cnt)])\n",
    "\n",
    "        val_results = np.concatenate(val_results)\n",
    "        assert np.isnan(val_results).sum() == 0\n",
    "        val_results = val_results[loc_data.my_order]\n",
    "        assert np.isnan(val_results).sum() == 0\n",
    "        assert len(val_results) == len(loc_data)\n",
    "        \n",
    "        preds.append(val_results)\n",
    "\n",
    "        lls = [log_loss(loc_data[all_ich[k]].values, val_results[:,k], eps=1e-7, labels=[0,1])\\\n",
    "               for k in range(6)]\n",
    "        ll = (class_weights * np.array(lls)).mean()\n",
    "        cor = np.corrcoef(loc_data.loc[:,all_ich].values.reshape(-1), val_results.reshape(-1))[0,1]\n",
    "        auc = roc_auc_score(loc_data.loc[:,all_ich].values.reshape(-1), val_results.reshape(-1))\n",
    "\n",
    "        print('ver {}, iter {}, fold {}, val ll: {:.4f}, cor: {:.4f}, auc: {:.4f}'\n",
    "              .format(VERSION, i, fold, ll, cor, auc))\n",
    "    \n",
    "    print('total running time', time.time() - st0)\n",
    "    \n",
    "    return np.stack(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epochs: 25 iters starting now: 32\n",
      "adding dummy serieses 168\n",
      "DataSet 3 valid size 6656 fold 0\n",
      "dataset valid: 6656 loader valid: 208\n",
      "loading model model.b25.f0.v74\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 3.954 time per batch: 0.989\n",
      "Batch 8 device: xla:1 time passed: 5.591 time per batch: 0.699\n",
      "Batch 12 device: xla:1 time passed: 7.594 time per batch: 0.633\n",
      "Batch 16 device: xla:1 time passed: 9.724 time per batch: 0.608\n",
      "Batch 20 device: xla:1 time passed: 11.770 time per batch: 0.589\n",
      "Batch 24 device: xla:1 time passed: 13.791 time per batch: 0.575\n",
      "ver 74, iter 0, fold 0, val ll: 0.0636, cor: 0.8424, auc: 0.9881\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 3.668 time per batch: 0.917\n",
      "Batch 8 device: xla:1 time passed: 5.364 time per batch: 0.670\n",
      "Batch 12 device: xla:1 time passed: 7.370 time per batch: 0.614\n",
      "Batch 16 device: xla:1 time passed: 9.304 time per batch: 0.581\n",
      "Batch 20 device: xla:1 time passed: 11.352 time per batch: 0.568\n",
      "Batch 24 device: xla:1 time passed: 13.265 time per batch: 0.553\n",
      "ver 74, iter 1, fold 0, val ll: 0.0637, cor: 0.8423, auc: 0.9881\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 3.780 time per batch: 0.945\n",
      "Batch 8 device: xla:1 time passed: 5.454 time per batch: 0.682\n",
      "Batch 12 device: xla:1 time passed: 7.405 time per batch: 0.617\n",
      "Batch 16 device: xla:1 time passed: 9.370 time per batch: 0.586\n",
      "Batch 20 device: xla:1 time passed: 11.301 time per batch: 0.565\n",
      "Batch 24 device: xla:1 time passed: 13.312 time per batch: 0.555\n",
      "ver 74, iter 2, fold 0, val ll: 0.0637, cor: 0.8420, auc: 0.9881\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 3.628 time per batch: 0.907\n",
      "Batch 8 device: xla:1 time passed: 5.385 time per batch: 0.673\n",
      "Batch 12 device: xla:1 time passed: 7.362 time per batch: 0.614\n",
      "Batch 16 device: xla:1 time passed: 9.324 time per batch: 0.583\n",
      "Batch 20 device: xla:1 time passed: 11.284 time per batch: 0.564\n",
      "Batch 24 device: xla:1 time passed: 13.237 time per batch: 0.552\n",
      "ver 74, iter 3, fold 0, val ll: 0.0637, cor: 0.8423, auc: 0.9881\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 3.821 time per batch: 0.955\n",
      "Batch 8 device: xla:1 time passed: 5.423 time per batch: 0.678\n",
      "Batch 12 device: xla:1 time passed: 7.380 time per batch: 0.615\n",
      "Batch 16 device: xla:1 time passed: 9.329 time per batch: 0.583\n",
      "Batch 20 device: xla:1 time passed: 11.275 time per batch: 0.564\n",
      "Batch 24 device: xla:1 time passed: 13.260 time per batch: 0.552\n",
      "ver 74, iter 4, fold 0, val ll: 0.0638, cor: 0.8421, auc: 0.9881\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 4.033 time per batch: 1.008\n",
      "Batch 8 device: xla:1 time passed: 5.460 time per batch: 0.683\n",
      "Batch 12 device: xla:1 time passed: 7.404 time per batch: 0.617\n",
      "Batch 16 device: xla:1 time passed: 9.308 time per batch: 0.582\n",
      "Batch 20 device: xla:1 time passed: 11.283 time per batch: 0.564\n",
      "Batch 24 device: xla:1 time passed: 13.310 time per batch: 0.555\n",
      "ver 74, iter 5, fold 0, val ll: 0.0636, cor: 0.8423, auc: 0.9882\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 3.766 time per batch: 0.941\n",
      "Batch 8 device: xla:1 time passed: 5.381 time per batch: 0.673\n",
      "Batch 12 device: xla:1 time passed: 7.358 time per batch: 0.613\n",
      "Batch 16 device: xla:1 time passed: 9.249 time per batch: 0.578\n",
      "Batch 20 device: xla:1 time passed: 11.233 time per batch: 0.562\n",
      "Batch 24 device: xla:1 time passed: 13.273 time per batch: 0.553\n",
      "ver 74, iter 6, fold 0, val ll: 0.0638, cor: 0.8420, auc: 0.9881\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 3.835 time per batch: 0.959\n",
      "Batch 8 device: xla:1 time passed: 5.399 time per batch: 0.675\n",
      "Batch 12 device: xla:1 time passed: 7.375 time per batch: 0.615\n",
      "Batch 16 device: xla:1 time passed: 9.383 time per batch: 0.586\n",
      "Batch 20 device: xla:1 time passed: 11.359 time per batch: 0.568\n",
      "Batch 24 device: xla:1 time passed: 13.303 time per batch: 0.554\n",
      "ver 74, iter 7, fold 0, val ll: 0.0637, cor: 0.8422, auc: 0.9881\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 3.781 time per batch: 0.945\n",
      "Batch 8 device: xla:1 time passed: 5.311 time per batch: 0.664\n",
      "Batch 12 device: xla:1 time passed: 7.328 time per batch: 0.611\n",
      "Batch 16 device: xla:1 time passed: 9.299 time per batch: 0.581\n",
      "Batch 20 device: xla:1 time passed: 11.266 time per batch: 0.563\n",
      "Batch 24 device: xla:1 time passed: 13.285 time per batch: 0.554\n",
      "ver 74, iter 8, fold 0, val ll: 0.0637, cor: 0.8424, auc: 0.9881\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 3.841 time per batch: 0.960\n",
      "Batch 8 device: xla:1 time passed: 5.396 time per batch: 0.675\n",
      "Batch 12 device: xla:1 time passed: 7.342 time per batch: 0.612\n",
      "Batch 16 device: xla:1 time passed: 9.304 time per batch: 0.582\n",
      "Batch 20 device: xla:1 time passed: 11.349 time per batch: 0.567\n",
      "Batch 24 device: xla:1 time passed: 13.227 time per batch: 0.551\n",
      "ver 74, iter 9, fold 0, val ll: 0.0637, cor: 0.8425, auc: 0.9881\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 3.772 time per batch: 0.943\n",
      "Batch 8 device: xla:1 time passed: 5.330 time per batch: 0.666\n",
      "Batch 12 device: xla:1 time passed: 7.325 time per batch: 0.610\n",
      "Batch 16 device: xla:1 time passed: 9.305 time per batch: 0.582\n",
      "Batch 20 device: xla:1 time passed: 11.288 time per batch: 0.564\n",
      "Batch 24 device: xla:1 time passed: 13.208 time per batch: 0.550\n",
      "ver 74, iter 10, fold 0, val ll: 0.0637, cor: 0.8422, auc: 0.9881\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 3.807 time per batch: 0.952\n",
      "Batch 8 device: xla:1 time passed: 5.480 time per batch: 0.685\n",
      "Batch 12 device: xla:1 time passed: 7.466 time per batch: 0.622\n",
      "Batch 16 device: xla:1 time passed: 9.468 time per batch: 0.592\n",
      "Batch 20 device: xla:1 time passed: 11.599 time per batch: 0.580\n",
      "Batch 24 device: xla:1 time passed: 13.531 time per batch: 0.564\n",
      "ver 74, iter 11, fold 0, val ll: 0.0637, cor: 0.8425, auc: 0.9880\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 3.815 time per batch: 0.954\n",
      "Batch 8 device: xla:1 time passed: 5.411 time per batch: 0.676\n",
      "Batch 12 device: xla:1 time passed: 7.534 time per batch: 0.628\n",
      "Batch 16 device: xla:1 time passed: 9.379 time per batch: 0.586\n",
      "Batch 20 device: xla:1 time passed: 11.395 time per batch: 0.570\n",
      "Batch 24 device: xla:1 time passed: 13.346 time per batch: 0.556\n",
      "ver 74, iter 12, fold 0, val ll: 0.0638, cor: 0.8423, auc: 0.9880\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 3.871 time per batch: 0.968\n",
      "Batch 8 device: xla:1 time passed: 5.566 time per batch: 0.696\n",
      "Batch 12 device: xla:1 time passed: 7.528 time per batch: 0.627\n",
      "Batch 16 device: xla:1 time passed: 9.438 time per batch: 0.590\n",
      "Batch 20 device: xla:1 time passed: 11.437 time per batch: 0.572\n",
      "Batch 24 device: xla:1 time passed: 13.422 time per batch: 0.559\n",
      "ver 74, iter 13, fold 0, val ll: 0.0637, cor: 0.8424, auc: 0.9881\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 3.664 time per batch: 0.916\n",
      "Batch 8 device: xla:1 time passed: 5.380 time per batch: 0.672\n",
      "Batch 12 device: xla:1 time passed: 7.385 time per batch: 0.615\n",
      "Batch 16 device: xla:1 time passed: 9.534 time per batch: 0.596\n",
      "Batch 20 device: xla:1 time passed: 11.532 time per batch: 0.577\n",
      "Batch 24 device: xla:1 time passed: 13.455 time per batch: 0.561\n",
      "ver 74, iter 14, fold 0, val ll: 0.0637, cor: 0.8423, auc: 0.9881\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 3.808 time per batch: 0.952\n",
      "Batch 8 device: xla:1 time passed: 5.351 time per batch: 0.669\n",
      "Batch 12 device: xla:1 time passed: 7.340 time per batch: 0.612\n",
      "Batch 16 device: xla:1 time passed: 9.287 time per batch: 0.580\n",
      "Batch 20 device: xla:1 time passed: 11.231 time per batch: 0.562\n",
      "Batch 24 device: xla:1 time passed: 13.324 time per batch: 0.555\n",
      "ver 74, iter 15, fold 0, val ll: 0.0638, cor: 0.8420, auc: 0.9881\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 3.821 time per batch: 0.955\n",
      "Batch 8 device: xla:1 time passed: 5.377 time per batch: 0.672\n",
      "Batch 12 device: xla:1 time passed: 7.306 time per batch: 0.609\n",
      "Batch 16 device: xla:1 time passed: 9.294 time per batch: 0.581\n",
      "Batch 20 device: xla:1 time passed: 11.261 time per batch: 0.563\n",
      "Batch 24 device: xla:1 time passed: 13.255 time per batch: 0.552\n",
      "ver 74, iter 16, fold 0, val ll: 0.0638, cor: 0.8423, auc: 0.9880\n",
      "setFeats, augmentation -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4 device: xla:1 time passed: 3.857 time per batch: 0.964\n",
      "Batch 8 device: xla:1 time passed: 5.469 time per batch: 0.684\n",
      "Batch 12 device: xla:1 time passed: 7.485 time per batch: 0.624\n",
      "Batch 16 device: xla:1 time passed: 9.437 time per batch: 0.590\n",
      "Batch 20 device: xla:1 time passed: 11.411 time per batch: 0.571\n",
      "Batch 24 device: xla:1 time passed: 13.322 time per batch: 0.555\n",
      "ver 74, iter 17, fold 0, val ll: 0.0635, cor: 0.8429, auc: 0.9881\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 3.825 time per batch: 0.956\n",
      "Batch 8 device: xla:1 time passed: 5.354 time per batch: 0.669\n",
      "Batch 12 device: xla:1 time passed: 7.261 time per batch: 0.605\n",
      "Batch 16 device: xla:1 time passed: 9.245 time per batch: 0.578\n",
      "Batch 20 device: xla:1 time passed: 11.226 time per batch: 0.561\n",
      "Batch 24 device: xla:1 time passed: 13.129 time per batch: 0.547\n",
      "ver 74, iter 18, fold 0, val ll: 0.0636, cor: 0.8428, auc: 0.9881\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 3.616 time per batch: 0.904\n",
      "Batch 8 device: xla:1 time passed: 5.369 time per batch: 0.671\n",
      "Batch 12 device: xla:1 time passed: 7.378 time per batch: 0.615\n",
      "Batch 16 device: xla:1 time passed: 9.296 time per batch: 0.581\n",
      "Batch 20 device: xla:1 time passed: 11.238 time per batch: 0.562\n",
      "Batch 24 device: xla:1 time passed: 13.172 time per batch: 0.549\n",
      "ver 74, iter 19, fold 0, val ll: 0.0637, cor: 0.8424, auc: 0.9881\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 3.722 time per batch: 0.931\n",
      "Batch 8 device: xla:1 time passed: 5.292 time per batch: 0.662\n",
      "Batch 12 device: xla:1 time passed: 7.346 time per batch: 0.612\n",
      "Batch 16 device: xla:1 time passed: 9.303 time per batch: 0.581\n",
      "Batch 20 device: xla:1 time passed: 11.299 time per batch: 0.565\n",
      "Batch 24 device: xla:1 time passed: 13.291 time per batch: 0.554\n",
      "ver 74, iter 20, fold 0, val ll: 0.0636, cor: 0.8426, auc: 0.9881\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 4.008 time per batch: 1.002\n",
      "Batch 8 device: xla:1 time passed: 5.391 time per batch: 0.674\n",
      "Batch 12 device: xla:1 time passed: 7.406 time per batch: 0.617\n",
      "Batch 16 device: xla:1 time passed: 9.395 time per batch: 0.587\n",
      "Batch 20 device: xla:1 time passed: 11.316 time per batch: 0.566\n",
      "Batch 24 device: xla:1 time passed: 13.396 time per batch: 0.558\n",
      "ver 74, iter 21, fold 0, val ll: 0.0638, cor: 0.8423, auc: 0.9880\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 3.684 time per batch: 0.921\n",
      "Batch 8 device: xla:1 time passed: 5.420 time per batch: 0.677\n",
      "Batch 12 device: xla:1 time passed: 7.323 time per batch: 0.610\n",
      "Batch 16 device: xla:1 time passed: 9.382 time per batch: 0.586\n",
      "Batch 20 device: xla:1 time passed: 11.341 time per batch: 0.567\n",
      "Batch 24 device: xla:1 time passed: 13.289 time per batch: 0.554\n",
      "ver 74, iter 22, fold 0, val ll: 0.0637, cor: 0.8421, auc: 0.9882\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 3.718 time per batch: 0.929\n",
      "Batch 8 device: xla:1 time passed: 5.332 time per batch: 0.666\n",
      "Batch 12 device: xla:1 time passed: 7.233 time per batch: 0.603\n",
      "Batch 16 device: xla:1 time passed: 9.226 time per batch: 0.577\n",
      "Batch 20 device: xla:1 time passed: 11.231 time per batch: 0.562\n",
      "Batch 24 device: xla:1 time passed: 13.150 time per batch: 0.548\n",
      "ver 74, iter 23, fold 0, val ll: 0.0637, cor: 0.8421, auc: 0.9881\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 3.830 time per batch: 0.958\n",
      "Batch 8 device: xla:1 time passed: 5.349 time per batch: 0.669\n",
      "Batch 12 device: xla:1 time passed: 7.306 time per batch: 0.609\n",
      "Batch 16 device: xla:1 time passed: 9.299 time per batch: 0.581\n",
      "Batch 20 device: xla:1 time passed: 11.254 time per batch: 0.563\n",
      "Batch 24 device: xla:1 time passed: 13.241 time per batch: 0.552\n",
      "ver 74, iter 24, fold 0, val ll: 0.0637, cor: 0.8424, auc: 0.9881\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 3.797 time per batch: 0.949\n",
      "Batch 8 device: xla:1 time passed: 5.531 time per batch: 0.691\n",
      "Batch 12 device: xla:1 time passed: 7.451 time per batch: 0.621\n",
      "Batch 16 device: xla:1 time passed: 9.446 time per batch: 0.590\n",
      "Batch 20 device: xla:1 time passed: 11.315 time per batch: 0.566\n",
      "Batch 24 device: xla:1 time passed: 13.299 time per batch: 0.554\n",
      "ver 74, iter 25, fold 0, val ll: 0.0638, cor: 0.8422, auc: 0.9880\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 3.861 time per batch: 0.965\n",
      "Batch 8 device: xla:1 time passed: 5.483 time per batch: 0.685\n",
      "Batch 12 device: xla:1 time passed: 7.436 time per batch: 0.620\n",
      "Batch 16 device: xla:1 time passed: 9.332 time per batch: 0.583\n",
      "Batch 20 device: xla:1 time passed: 11.275 time per batch: 0.564\n",
      "Batch 24 device: xla:1 time passed: 13.306 time per batch: 0.554\n",
      "ver 74, iter 26, fold 0, val ll: 0.0638, cor: 0.8422, auc: 0.9881\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 3.718 time per batch: 0.929\n",
      "Batch 8 device: xla:1 time passed: 5.311 time per batch: 0.664\n",
      "Batch 12 device: xla:1 time passed: 7.268 time per batch: 0.606\n",
      "Batch 16 device: xla:1 time passed: 9.176 time per batch: 0.574\n",
      "Batch 20 device: xla:1 time passed: 11.283 time per batch: 0.564\n",
      "Batch 24 device: xla:1 time passed: 13.157 time per batch: 0.548\n",
      "ver 74, iter 27, fold 0, val ll: 0.0635, cor: 0.8426, auc: 0.9881\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 3.800 time per batch: 0.950\n",
      "Batch 8 device: xla:1 time passed: 5.270 time per batch: 0.659\n",
      "Batch 12 device: xla:1 time passed: 7.326 time per batch: 0.611\n",
      "Batch 16 device: xla:1 time passed: 9.274 time per batch: 0.580\n",
      "Batch 20 device: xla:1 time passed: 11.242 time per batch: 0.562\n",
      "Batch 24 device: xla:1 time passed: 13.201 time per batch: 0.550\n",
      "ver 74, iter 28, fold 0, val ll: 0.0637, cor: 0.8423, auc: 0.9881\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 4.072 time per batch: 1.018\n",
      "Batch 8 device: xla:1 time passed: 5.496 time per batch: 0.687\n",
      "Batch 12 device: xla:1 time passed: 7.437 time per batch: 0.620\n",
      "Batch 16 device: xla:1 time passed: 9.415 time per batch: 0.588\n",
      "Batch 20 device: xla:1 time passed: 11.370 time per batch: 0.568\n",
      "Batch 24 device: xla:1 time passed: 13.305 time per batch: 0.554\n",
      "ver 74, iter 29, fold 0, val ll: 0.0638, cor: 0.8421, auc: 0.9880\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 3.739 time per batch: 0.935\n",
      "Batch 8 device: xla:1 time passed: 5.270 time per batch: 0.659\n",
      "Batch 12 device: xla:1 time passed: 7.266 time per batch: 0.606\n",
      "Batch 16 device: xla:1 time passed: 9.337 time per batch: 0.584\n",
      "Batch 20 device: xla:1 time passed: 11.251 time per batch: 0.563\n",
      "Batch 24 device: xla:1 time passed: 13.186 time per batch: 0.549\n",
      "ver 74, iter 30, fold 0, val ll: 0.0636, cor: 0.8425, auc: 0.9882\n",
      "setFeats, augmentation -1\n",
      "Batch 4 device: xla:1 time passed: 3.683 time per batch: 0.921\n",
      "Batch 8 device: xla:1 time passed: 5.327 time per batch: 0.666\n",
      "Batch 12 device: xla:1 time passed: 7.267 time per batch: 0.606\n",
      "Batch 16 device: xla:1 time passed: 9.332 time per batch: 0.583\n",
      "Batch 20 device: xla:1 time passed: 11.288 time per batch: 0.564\n",
      "Batch 24 device: xla:1 time passed: 13.241 time per batch: 0.552\n",
      "ver 74, iter 31, fold 0, val ll: 0.0636, cor: 0.8424, auc: 0.9882\n",
      "total running time 558.1901004314423\n",
      "total time 558.6646049022675\n"
     ]
    }
   ],
   "source": [
    "stg = time.time()\n",
    "for fold in range(0,1):\n",
    "    predictions = oof_one(num_iter=32, bs=bs, fold=fold)\n",
    "    pickle.dump(predictions, open(PATH_WORK/'oof_{}_f{}_v{}'.format(dataset_name, fold, VERSION),'wb'))\n",
    "print('total time', time.time() - stg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# correlation between models\n",
    "# scores per slice\n",
    "# what is the best way to agg oof, model\\run levels\n",
    "# best aggregation theoretically\n",
    "# distribution of oof preds\n",
    "# score - what uniform p will get\n",
    "# 0.5 + np.sign(x-0.5) *2*(x-0.5)**2 - makes it less aggressive, is it a good transform above mean?\n",
    "# does scaling help for single runs, or is it aggregation artifact.\n",
    "\n",
    "# s101 problem.\n",
    "    # maybe 8 and 32 behave differently"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting runs aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "afuncs_names = np.array(['mean','gmean','q50','q25','q75','psig'])\n",
    "\n",
    "def scalePreds(x, power = 2, center=0.5):\n",
    "    res = x.copy()\n",
    "    res[x > center] = center + (1 - center) * ((res[x > center] - center)/(1 - center))**power\n",
    "    res[x < center] = center - center * ((center - res[x < center])/center)**power\n",
    "    return res\n",
    "\n",
    "def applyAggFunc(probs, func_name, axis=1, norm_axis=None):\n",
    "    \n",
    "    st = time.time()\n",
    "    \n",
    "    if func_name == 'mean':\n",
    "        res = probs.mean(axis)\n",
    "    elif func_name == 'max':\n",
    "        res = probs.max(axis)\n",
    "    elif func_name == 'min':\n",
    "        res = probs.min(axis)\n",
    "    elif func_name == 'gmean':\n",
    "        res = np.exp(np.log(probs).mean(axis))\n",
    "    elif func_name == 'q50':\n",
    "        res = np.quantile(probs,q=0.5,axis=axis)\n",
    "    elif func_name == 'q25':\n",
    "        res = np.quantile(probs,q=0.25,axis=axis)\n",
    "    elif func_name == 'q75':\n",
    "        res = np.quantile(probs,q=0.75,axis=axis)\n",
    "    elif func_name == 'psig':\n",
    "        mask_zero = (probs >= 1).sum(axis) > 0\n",
    "        res = 1 / (1 + np.exp(-(np.log(probs/(1-probs), where=(probs < 1)).mean(axis))))\n",
    "        res[mask_zero] = 0\n",
    "    else:\n",
    "        assert True\n",
    "    \n",
    "    res = np.clip(res, 1e-15, 1-1e-15)\n",
    "    \n",
    "    #print('applying time', func_name, time.time()-st)\n",
    "    return res\n",
    "\n",
    "def getPredsOOF():\n",
    "    preds_all = np.zeros((4,8,len(ids_df),6))\n",
    "\n",
    "    for fold in range(3):\n",
    "        preds = np.stack([pickle.load(open(PATH_WORK/'oof_{}_f{}_v{}'.format(name, fold, ver),'rb')) \\\n",
    "            for name, ver in zip(['Densenet161','Densenet169','Densenet201','se_resnext101_32x4d'], [72,73,74,75])])\n",
    "\n",
    "        preds = np.clip(preds, 1e-15, 1-1e-15)\n",
    "\n",
    "        preds_all[:,:,ids_df.fold == fold,:] = preds\n",
    "    \n",
    "    return preds_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c417cf52b0>]"
      ]
     },
     "execution_count": 223,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.arange(101)/100\n",
    "plt.plot(scalePreds(x, center=0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fold 0\n",
      "mean    [0.06301 0.063   0.06301 0.06244] [0.98803 0.98826 0.98836 0.98862]\n",
      "gmean   [0.06313 0.06312 0.06314 0.06259] [0.98802 0.98824 0.98835 0.98861]\n",
      "q50     [0.06328 0.06324 0.06325 0.06272] [0.98798 0.9882  0.98831 0.98858]\n",
      "q25     [0.06382 0.06382 0.06382 0.06339] [0.98793 0.98808 0.98824 0.98847]\n",
      "q75     [0.06326 0.06318 0.06308 0.06286] [0.988   0.98825 0.98836 0.98857]\n",
      "psig    [0.06315 0.06313 0.06314 0.06261] [0.98803 0.98824 0.98835 0.98861]\n",
      "fold 1\n",
      "mean    [0.06242 0.06224 0.06224 0.06109] [0.98841 0.98822 0.98825 0.98864]\n",
      "gmean   [0.06251 0.06238 0.06235 0.06131] [0.9884  0.9882  0.98822 0.98857]\n",
      "q50     [0.06269 0.06246 0.06247 0.06141] [0.98836 0.98817 0.98817 0.98851]\n",
      "q25     [0.06298 0.06313 0.06298 0.06223] [0.9883  0.98806 0.98806 0.98835]\n",
      "q75     [0.06304 0.06233 0.06245 0.06143] [0.98838 0.9882  0.98823 0.98863]\n",
      "psig    [0.06259 0.06239 0.06246 0.06131] [0.9884  0.9882  0.98819 0.98857]\n",
      "fold 2\n",
      "mean    [0.05989 0.06022 0.06001 0.05902] [0.98938 0.98931 0.98954 0.99005]\n",
      "gmean   [0.06005 0.06031 0.06012 0.05919] [0.98934 0.98927 0.98952 0.99002]\n",
      "q50     [0.06016 0.06044 0.06021 0.05929] [0.9893  0.98924 0.98949 0.99   ]\n",
      "q25     [0.06074 0.06075 0.06055 0.05995] [0.98921 0.98912 0.9894  0.98989]\n",
      "q75     [0.06014 0.06067 0.06034 0.05944] [0.98935 0.98929 0.98954 0.99002]\n",
      "psig    [0.06007 0.06034 0.06014 0.05919] [0.98934 0.98928 0.98952 0.99003]\n"
     ]
    }
   ],
   "source": [
    "np.set_printoptions(precision=5)\n",
    "\n",
    "for fold in range(3):\n",
    "    print('fold', fold)\n",
    "    data_fold = ids_df.loc[ids_df.fold == fold]\n",
    "\n",
    "    preds = np.stack([pickle.load(open(PATH_WORK/'oof_{}_f{}_v{}'.format(name, fold, ver),'rb')) \\\n",
    "        for name, ver in zip(['Densenet161','Densenet169','Densenet201','se_resnext101_32x4d'], [72,73,74,75])])\n",
    "\n",
    "    assert len(data_fold) == preds.shape[2]\n",
    "    \n",
    "    preds = np.clip(preds, 1e-15, 1-1e-15)\n",
    "    for afunc in afuncs_names:\n",
    "        apreds = applyAggFunc(preds, afunc)\n",
    "        res = ((- data_fold[all_ich].values * np.log(apreds) - (1 - data_fold[all_ich].values) * np.log(1 - apreds))\\\n",
    "            * class_weights).mean((1,2))\n",
    "        roc = [roc_auc_score(data_fold[all_ich].values.reshape(-1), apreds[i].reshape(-1)) for i in range(4)]\n",
    "        print('{:7s} {} {}'.format(afunc,res,np.array(roc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_all = getPredsOOF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06268, 0.06259, 0.06246, 0.06198])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((- ids_df[all_ich].values * np.log(preds_all) \n",
    "  - (1 - ids_df[all_ich].values) * np.log(np.clip(1 - preds_all,1e-15,1-1e-15)))\n",
    " * class_weights).mean((1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06177, 0.06182, 0.06176, 0.06085])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((- ids_df[all_ich].values * np.log(preds_all.mean(1)) \n",
    "  - (1 - ids_df[all_ich].values) * np.log(np.clip(1 - preds_all.mean(1),1e-15,1-1e-15)))\n",
    " * class_weights).mean((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "runs_afunc = 'mean'\n",
    "preds2 = applyAggFunc(preds_all, runs_afunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting models aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean    0.059085   1.130000 0.058993\n",
      "gmean   0.059406   1.240000 0.059135\n",
      "q50     0.059532   1.230000 0.059280\n",
      "q25     0.060557   1.400000 0.059908\n",
      "q75     0.059834   1.090000 0.059781\n",
      "psig    0.059423   1.250000 0.059121\n"
     ]
    }
   ],
   "source": [
    "for afunc in afuncs_names:\n",
    "    #print(afunc)\n",
    "    apreds = applyAggFunc(preds2, afunc, axis=0)\n",
    "    res = ((- ids_df[all_ich].values * np.log(apreds) - (1 - ids_df[all_ich].values) * np.log(1 - apreds))\\\n",
    "        * class_weights).mean()\n",
    "    \n",
    "    if True:\n",
    "        best_score = res\n",
    "        best_k = 0\n",
    "        for k in range(1,50):\n",
    "            apreds2 = scalePreds(apreds, 1.0 + 0.01 * k)\n",
    "            apreds2 = np.clip(apreds2, 1e-15, 1-1e-15)\n",
    "\n",
    "            res2 = ((- ids_df[all_ich].values * np.log(apreds2) - (1 - ids_df[all_ich].values) * np.log(1 - apreds2))\\\n",
    "                    * class_weights).mean()\n",
    "\n",
    "            if res2 > best_score: break\n",
    "            best_score = res2\n",
    "            best_k = k\n",
    "\n",
    "        print('{:7s} {:5f}   {:2f} {:5f}'.format(afunc,res,1+0.01*best_k,best_score))\n",
    "    else:\n",
    "        print('{:7s} {:5f}'.format(afunc,res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05908490515986469"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apreds = (preds2*np.array([0.25,0.25,0.25,0.25])[:,None,None]).sum(0)\n",
    "((- ids_df[all_ich].values * np.log(apreds) - (1 - ids_df[all_ich].values) * np.log(1 - apreds))\\\n",
    "        * class_weights).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05900441267604068"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "apreds = (preds2*np.array([0.2,0.2,0.2,0.4])[:,None,None]).sum(0)\n",
    "((- ids_df[all_ich].values * np.log(apreds) - (1 - ids_df[all_ich].values) * np.log(1 - apreds))\\\n",
    "        * class_weights).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_afunc = 'mean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 8, 674252, 6)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ((- ids_df[all_ich].values * np.log(preds_all) \n",
    "  - (1 - ids_df[all_ich].values) * np.log(np.clip(1 - preds_all,1e-15,1-1e-15)))\n",
    " * class_weights).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06268, 0.06259, 0.06246, 0.06198])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((- ids_df[all_ich].values * np.log(preds_all) \n",
    "  - (1 - ids_df[all_ich].values) * np.log(np.clip(1 - preds_all,1e-15,1-1e-15)))\n",
    " * class_weights).mean((1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = res\n",
    "best_k = 0\n",
    "for k in range(1,50):\n",
    "    apreds = scalePreds(preds_all, 1.0 + 0.01 * k)\n",
    "    apreds = np.clip(apreds, 1e-15, 1-1e-15)\n",
    "\n",
    "    res2 = ((- ids_df[all_ich].values * np.log(apreds) - (1 - ids_df[all_ich].values) * np.log(1 - apreds))\\\n",
    "            * class_weights).mean()\n",
    "\n",
    "    if res2 > best_score: break\n",
    "    best_score = res2\n",
    "    best_k = k\n",
    "\n",
    "print('{{:5f}   {:2f} {:5f}'.format(res,1+0.01*best_k,best_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models behavior per groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WindowCenter_1_le     0 248151   2157 [0.03705 0.03668 0.03694 0.03659]\n",
      "WindowCenter_1_le     2  10377     34 [0.11658 0.11467 0.11616 0.11358]\n",
      "WindowCenter_1_le     3 341674  75369 [0.06421 0.06464 0.06441 0.06327]\n",
      "WindowCenter_1_le     1  70894    985 [0.12686 0.1265  0.12643 0.12507]\n",
      "WindowCenter_1_le     4   3156      0 [0.09964 0.10669 0.09404 0.08973]\n",
      "BitType_le            1 323550   3088 [0.05729 0.05701 0.057   0.0565 ]\n",
      "BitType_le            0 338723  75369 [0.06359 0.06402 0.06378 0.06264]\n",
      "BitType_le            2   2252     60 [0.12916 0.126   0.13457 0.12131]\n",
      "BitType_le            4   6776     28 [0.13046 0.12838 0.13103 0.12605]\n",
      "BitType_le            3   2951      0 [0.13481 0.13559 0.1363  0.13621]\n",
      "WindowCenter_0_le     1 248151   2157 [0.03705 0.03668 0.03694 0.03659]\n",
      "WindowCenter_0_le     4  10343     34 [0.11679 0.11483 0.1163  0.11368]\n",
      "WindowCenter_0_le     2 151196   2148 [0.12164 0.12159 0.12156 0.11969]\n",
      "WindowCenter_0_le     0 213404  69272 [0.03548 0.03606 0.03585 0.03508]\n",
      "WindowCenter_0_le     3  43648   4934 [0.10451 0.10455 0.10365 0.10289]\n",
      "WindowCenter_0_le     6   3553      0 [0.09371 0.09799 0.08841 0.08391]\n",
      "WindowCenter_0_le     5   3957      0 [0.099   0.10103 0.10085 0.10098]\n",
      "pos_inc1_grp_le       3 589641  59576 [0.06462 0.06454 0.06463 0.06375]\n",
      "pos_inc1_grp_le       0  25148   3724 [0.02946 0.0313  0.03118 0.0306 ]\n",
      "pos_inc1_grp_le       1  51896  14995 [0.03864 0.03939 0.03783 0.03674]\n",
      "pos_inc1_grp_le       2   7567    250 [0.10577 0.10517 0.1037  0.10066]\n",
      "pos_inc2_grp_le       3 589642  59576 [0.06514 0.06503 0.06514 0.06424]\n",
      "pos_inc2_grp_le       0  25147   3724 [0.02631 0.0286  0.02797 0.02789]\n",
      "pos_inc2_grp_le       1  51896  14995 [0.03582 0.03666 0.03504 0.03391]\n",
      "pos_inc2_grp_le       2   7567    250 [0.0953  0.0945  0.09322 0.0909 ]\n",
      "pos_size_le          10 113318   7349 [0.0533  0.05359 0.05324 0.05253]\n",
      "pos_size_le           2  78912   9072 [0.05865 0.05859 0.05872 0.05761]\n",
      "pos_size_le           3  52218   9884 [0.07384 0.0741  0.07458 0.07266]\n",
      "pos_size_le           0 156544  21824 [0.07309 0.07336 0.07327 0.07179]\n",
      "pos_size_le           7  30393    231 [0.03315 0.03314 0.0337  0.03355]\n",
      "pos_size_le           8  25270   1596 [0.05736 0.05588 0.05449 0.05578]\n",
      "pos_size_le           4  51508    884 [0.07307 0.07264 0.07277 0.07233]\n",
      "pos_size_le           5  33180    930 [0.10449 0.10443 0.10563 0.10353]\n",
      "pos_size_le           1  79079  21840 [0.04441 0.04505 0.04466 0.04433]\n",
      "pos_size_le           6  32270    315 [0.04167 0.04145 0.0408  0.04126]\n",
      "pos_size_le           9  21560   4620 [0.05287 0.05135 0.0512  0.0495 ]\n",
      "pos_zeros_le          0 669377  77188 [0.06158 0.06157 0.06154 0.06064]\n",
      "pos_zeros_le          3   2665    917 [0.0987  0.1078  0.10104 0.09965]\n",
      "pos_zeros_le          2   1132    332 [0.08506 0.09575 0.09479 0.09166]\n",
      "pos_zeros_le          1   1078    108 [0.0624  0.06974 0.06244 0.06314]\n",
      "WindowWidth_0_le      0 541489  72448 [0.04957 0.04956 0.0496  0.04892]\n",
      "WindowWidth_0_le      1  63927    921 [0.11277 0.11411 0.11411 0.11129]\n",
      "WindowWidth_0_le      2  30067   4308 [0.09428 0.0937  0.09289 0.09182]\n",
      "WindowWidth_0_le      3  31237    762 [0.13008 0.12972 0.12899 0.12802]\n",
      "WindowWidth_0_le      5   5389     90 [0.08504 0.08551 0.08102 0.08137]\n",
      "WindowWidth_0_le      4   2143     16 [0.11418 0.104   0.10566 0.10486]\n",
      "WindowWidth_1_le      0 329597   3176 [0.05888 0.05847 0.05869 0.05806]\n",
      "WindowWidth_1_le      1 341674  75369 [0.06421 0.06464 0.06441 0.06327]\n",
      "WindowWidth_1_le      2   2981      0 [0.10247 0.11008 0.09658 0.0915 ]\n"
     ]
    }
   ],
   "source": [
    "for col in cols_le:\n",
    "    for i in ids_df[col].unique():\n",
    "        res = ((- ids_df[all_ich].values * np.log(preds_all.mean(1)) - (1 - ids_df[all_ich].values) \\\n",
    "                * np.log(1 - preds_all.mean(1))) * class_weights)[:,(ids_df[col] == i)].mean((1,2))\n",
    "        sz = (ids_df[col] == i).sum()\n",
    "        sz_test = (test_md[col] == i).sum()\n",
    "        print('{:20s} {:2d} {:6d} {:6d} {}'.format(col,i,sz,sz_test,res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standard deviation analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "stds = preds_all.std(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00673, 0.00077, 0.00338, 0.00226, 0.00435, 0.00447],\n",
       "       [0.0062 , 0.00074, 0.00314, 0.00204, 0.00397, 0.00386],\n",
       "       [0.0059 , 0.00073, 0.00294, 0.00195, 0.00365, 0.00395],\n",
       "       [0.00766, 0.00075, 0.0039 , 0.00263, 0.00479, 0.00472]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stds.mean((1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 674252, 6)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8527  0.05913 0.05106 0.08638 0.84558 0.05423 0.07018 0.84259]\n",
      "[0.17928 0.70484 0.13346 0.3709  0.11267 0.75836 0.78877 0.83044]\n",
      "[0.74571 0.01364 0.01121 0.01677 0.75372 0.01375 0.01258 0.76661]\n",
      "[0.74808 0.00857 0.74761 0.10039 0.73221 0.00924 0.02616 0.08939]\n",
      "[0.19123 0.08033 0.65973 0.07672 0.1776  0.08611 0.73698 0.8015 ]\n",
      "[0.64336 0.81652 0.28427 0.61186 0.62909 0.28193 0.28955 0.83129]\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    idx = stds[0,:,i].argmax()\n",
    "    print(preds_all[0,:,idx,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.98124 0.97982 0.18154 0.16633 0.14569 0.19571 0.99596 0.99338]\n",
      "[0.55692 0.10884 0.20442 0.59746 0.09779 0.0987  0.10347 0.37919]\n",
      "[0.91611 0.87244 0.01683 0.02782 0.85069 0.90538 0.91419 0.05308]\n",
      "[0.7601  0.16606 0.07542 0.83273 0.06887 0.88932 0.01933 0.82366]\n",
      "[0.23019 0.69766 0.20732 0.14846 0.63866 0.85328 0.12059 0.88527]\n",
      "[0.84319 0.03652 0.81045 0.85523 0.8237  0.67587 0.13533 0.77968]\n"
     ]
    }
   ],
   "source": [
    "for i in range(6):\n",
    "    idx = stds[3,:,i].argmax()\n",
    "    print(preds_all[3,:,idx,i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14442, 0.00942, 0.03493, 0.02353, 0.05386, 0.06121],\n",
       "       [0.14628, 0.01031, 0.0354 , 0.02395, 0.05451, 0.06016],\n",
       "       [0.14718, 0.00941, 0.03481, 0.02377, 0.05303, 0.06166],\n",
       "       [0.14496, 0.01316, 0.03619, 0.02431, 0.05454, 0.06067]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((- preds_all.mean(1, keepdims=True) * np.log(preds_all) \n",
    "  - (1 - preds_all.mean(1, keepdims=True)) * np.log(np.clip(1 - preds_all,1e-15,1-1e-15)))\n",
    " * class_weights).mean((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.05456, 0.0551 , 0.05498, 0.05564])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((- preds_all.mean(1, keepdims=True) * np.log(preds_all) \n",
    "  - (1 - preds_all.mean(1, keepdims=True)) * np.log(np.clip(1 - preds_all,1e-15,1-1e-15)))\n",
    " * class_weights).mean((1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.60433, 0.60165, 0.59444, 0.60381])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((- preds_all.mean((1,2), keepdims=True) * np.log(preds_all) \n",
    "  - (1 - preds_all.mean((1,2), keepdims=True)) * np.log(np.clip(1 - preds_all,1e-15,1-1e-15)))\n",
    " * class_weights).mean((1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_one(bs = 100, add_seed = 0, fold = 0, anum = 0):\n",
    "    st = time.time()\n",
    "\n",
    "    cur_epoch = getCurrentBatch(fold=fold)\n",
    "    if cur_epoch is None: cur_epoch = 0\n",
    "    print('completed epochs:', cur_epoch)\n",
    "\n",
    "    #model = TabularModel(n_cont = len(meta_cols), feat_sz=feat_sz)\n",
    "    model = ResNetModel(n_cont = len(meta_cols), feat_sz=feat_sz)\n",
    "    \n",
    "    model_file_name = modelFileName(return_last=True, fold=fold)\n",
    "    if model_file_name is not None:\n",
    "        print('loading model', model_file_name)\n",
    "        state_dict = torch.load(PATH_WORK/'models'/model_file_name)\n",
    "        model.load_state_dict(state_dict)\n",
    "    \n",
    "    if (not CLOUD) or CLOUD_SINGLE:\n",
    "        model = model.to(device)\n",
    "    else:\n",
    "        model_parallel = dp.DataParallel(model, device_ids=devices)\n",
    "\n",
    "    setSeeds(SEED + cur_epoch + add_seed)\n",
    "\n",
    "    tst_ds = RSNA_DataSet(test_md, mode='test', bs=bs, fold=fold)\n",
    "    loader_tst = D.DataLoader(tst_ds, num_workers=8 if CLOUD else 0, batch_size=bs, shuffle=False)\n",
    "    print('dataset test:', len(tst_ds), 'loader test:', len(loader_tst))\n",
    "    \n",
    "    tst_ds.setFeats(anum)\n",
    "\n",
    "    loc_data = tst_ds.metadata.copy()\n",
    "    series_counts = loc_data.index.value_counts()\n",
    "\n",
    "    loc_data['orig_idx'] = np.arange(len(loc_data))\n",
    "    loc_data = loc_data.sort_values(['SeriesInstanceUID','pos_idx1'])\n",
    "    loc_data['my_order'] = np.arange(len(loc_data))\n",
    "    loc_data = loc_data.sort_values(['orig_idx'])\n",
    "    \n",
    "    if CLOUD and (not CLOUD_SINGLE):\n",
    "        results = model_parallel(test_loop_fn, loader_tst)\n",
    "        predictions = np.concatenate([results[i][0] for i in range(MAX_DEVICES)])\n",
    "        indices = np.concatenate([results[i][1] for i in range(MAX_DEVICES)])\n",
    "        offsets = np.concatenate([results[i][2] for i in range(MAX_DEVICES)])\n",
    "    else:\n",
    "        predictions, indices, offsets = test_loop_fn(model, loader_tst, device)\n",
    "\n",
    "    predictions = predictions[np.argsort(indices)]\n",
    "    offsets = offsets[np.argsort(indices)]\n",
    "    assert len(predictions) == len(test_md.SeriesInstanceUID.unique())\n",
    "    assert np.all(indices[np.argsort(indices)] == np.array(range(len(predictions))))\n",
    "    \n",
    "    val_results = []\n",
    "    for k, series in enumerate(np.sort(loc_data.index.unique())):\n",
    "        cnt = series_counts[series]\n",
    "        assert (offsets[k] + cnt) <= 60\n",
    "        val_results.append(predictions[k,offsets[k]:(offsets[k] + cnt)])\n",
    "\n",
    "    val_results = np.concatenate(val_results)\n",
    "    assert np.isnan(val_results).sum() == 0\n",
    "    val_results = val_results[loc_data.my_order]\n",
    "    assert len(val_results) == len(loc_data)\n",
    "\n",
    "    print('test processing time:', time.time() - st)\n",
    "    \n",
    "    return val_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 18.719786643981934\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 7.524738073348999\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 7.300777435302734\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 7.019393682479858\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 7.338620185852051\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.707183599472046\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 7.020536184310913\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.742215394973755\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.834000587463379\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.748502016067505\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.961599349975586\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.836533784866333\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.859585523605347\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.786699533462524\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.7708070278167725\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.589108943939209\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.783012390136719\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.760648965835571\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.736371994018555\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.772942543029785\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.872204065322876\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.671399354934692\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.730472803115845\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.826353549957275\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.815583229064941\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.802212715148926\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.701398611068726\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.676718473434448\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.79737114906311\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.784872770309448\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.854281902313232\n",
      "completed epochs: 25\n",
      "loading model model.b25.f0.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.684962749481201\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 15.032886981964111\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.741588115692139\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.7263171672821045\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.752935886383057\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.758429288864136\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.6971049308776855\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test processing time: 6.822573900222778\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.7198805809021\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.7346508502960205\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.6780686378479\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.697108507156372\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.705166816711426\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.70893120765686\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.874312877655029\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.773636817932129\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.638176202774048\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.7585694789886475\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.70299768447876\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.662557363510132\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.71575927734375\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.659278392791748\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.894127607345581\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.811914682388306\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.84487771987915\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.739832401275635\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.71966814994812\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.7128236293792725\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.6983323097229\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.717395067214966\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.663848638534546\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.868816375732422\n",
      "completed epochs: 25\n",
      "loading model model.b25.f1.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.688223123550415\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 14.629517555236816\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.651827096939087\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.744072198867798\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.845724105834961\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.811538934707642\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.925138473510742\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.676286458969116\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.725562572479248\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.6931843757629395\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.793181419372559\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.73327898979187\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.738743543624878\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.662845611572266\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test processing time: 6.707979917526245\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.701488733291626\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.694072246551514\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.727868556976318\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.719923257827759\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.735955238342285\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.77137017250061\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.816018342971802\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.803425550460815\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.7614991664886475\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.684051752090454\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.722221374511719\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.648444414138794\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.601937294006348\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.67154598236084\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.701107025146484\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.665322542190552\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.694201231002808\n",
      "completed epochs: 25\n",
      "loading model model.b25.f2.v75\n",
      "adding dummy serieses 186\n",
      "DataSet 4 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation -1\n",
      "test processing time: 6.741651773452759\n",
      "total time 681.2849416732788\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "stg = time.time()\n",
    "for fold in range(3):\n",
    "    preds2 = []\n",
    "    for anum in range(32):\n",
    "        predictions = inference_one(fold = fold, anum = -1)\n",
    "        preds2.append(predictions)\n",
    "    preds.append(np.stack(preds2))\n",
    "preds = np.stack(preds)\n",
    "print('total time', time.time() - stg)\n",
    "\n",
    "pickle.dump(preds, open(PATH_WORK/'preds_{}_v{}'.format(dataset_name, VERSION),'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Files transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file:///home/zahar_chikishev/running/preds_Densenet161_v72 [Content-Type=application/octet-stream]...\n",
      "==> NOTE: You are uploading one or more large file(s), which would run          \n",
      "significantly faster if you enable parallel composite uploads. This\n",
      "feature can be enabled by editing the\n",
      "\"parallel_composite_upload_threshold\" value in your .boto\n",
      "configuration file. However, note that if you do this large files will\n",
      "be uploaded as `composite objects\n",
      "<https://cloud.google.com/storage/docs/composite-objects>`_,which\n",
      "means that any user who downloads such objects will need to have a\n",
      "compiled crcmod installed (see \"gsutil help crcmod\"). This is because\n",
      "without a compiled crcmod, computing checksums on composite objects is\n",
      "so slow that gsutil disables downloads of composite objects.\n",
      "\n",
      "Copying file:///home/zahar_chikishev/running/preds_Densenet169 [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/zahar_chikishev/running/preds_Densenet169_v51 [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/zahar_chikishev/running/preds_Densenet169_v73 [Content-Type=application/octet-stream]...\n",
      "/ [4 files][690.3 MiB/690.3 MiB]   36.3 MiB/s                                   \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying file:///home/zahar_chikishev/running/preds_Densenet201_v52 [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/zahar_chikishev/running/preds_Densenet201_v74 [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/zahar_chikishev/running/preds_se_resnext101_32x4d_v53 [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/zahar_chikishev/running/preds_se_resnext101_32x4d_v75 [Content-Type=application/octet-stream]...\n",
      "/ [8 files][  1.4 GiB/  1.4 GiB]   40.9 MiB/s                                   \n",
      "Operation completed over 8 objects/1.4 GiB.                                      \n",
      "Copying file:///home/zahar_chikishev/running/oof_Densenet161_f0_v72 [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/zahar_chikishev/running/oof_Densenet161_f1_v72 [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/zahar_chikishev/running/oof_Densenet161_f2_v72 [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/zahar_chikishev/running/oof_Densenet169_f0_v73 [Content-Type=application/octet-stream]...\n",
      "\\ [4 files][164.5 MiB/164.5 MiB]                                                \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m cp ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Copying file:///home/zahar_chikishev/running/oof_Densenet169_f1_v73 [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/zahar_chikishev/running/oof_Densenet169_f2_v73 [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/zahar_chikishev/running/oof_Densenet201_f0_v74 [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/zahar_chikishev/running/oof_Densenet201_f1_v74 [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/zahar_chikishev/running/oof_Densenet201_f2_v74 [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/zahar_chikishev/running/oof_se_resnext101_32x4d_f0_v75 [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/zahar_chikishev/running/oof_se_resnext101_32x4d_f1_v75 [Content-Type=application/octet-stream]...\n",
      "Copying file:///home/zahar_chikishev/running/oof_se_resnext101_32x4d_f2_v75 [Content-Type=application/octet-stream]...\n",
      "| [12 files][493.8 MiB/493.8 MiB]   33.7 MiB/s                                  \n",
      "Operation completed over 12 objects/493.8 MiB.                                   \n"
     ]
    }
   ],
   "source": [
    "!gsutil cp /home/zahar_chikishev/running/preds* gs://rsna-hemorrhage/results\n",
    "!gsutil cp /home/zahar_chikishev/running/oof* gs://rsna-hemorrhage/results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!gsutil gs://rsna-hemorrhage/results/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "!rm /home/zahar_chikishev/running/*v53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zahar_chikishev/running/preds_se_resnext101_32x4d_v53\r\n",
      "/home/zahar_chikishev/running/stats.f0.v53\r\n",
      "/home/zahar_chikishev/running/stats.f1.v53\r\n",
      "/home/zahar_chikishev/running/stats.f2.v53\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/zahar_chikishev/running/*v53"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zahar_chikishev/running/oof_Densenet161_f0_v72\r\n",
      "/home/zahar_chikishev/running/oof_Densenet161_f1_v72\r\n",
      "/home/zahar_chikishev/running/oof_Densenet161_f2_v72\r\n",
      "/home/zahar_chikishev/running/oof_Densenet169_f0_v73\r\n",
      "/home/zahar_chikishev/running/oof_Densenet169_f1_v73\r\n",
      "/home/zahar_chikishev/running/oof_Densenet169_f2_v73\r\n",
      "/home/zahar_chikishev/running/oof_Densenet201_f0_v74\r\n",
      "/home/zahar_chikishev/running/oof_Densenet201_f1_v74\r\n",
      "/home/zahar_chikishev/running/oof_Densenet201_f2_v74\r\n",
      "/home/zahar_chikishev/running/oof_se_resnext101_32x4d_f0_v75\r\n",
      "/home/zahar_chikishev/running/oof_se_resnext101_32x4d_f1_v75\r\n",
      "/home/zahar_chikishev/running/oof_se_resnext101_32x4d_f2_v75\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/zahar_chikishev/running/oof*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/zahar_chikishev/running/preds_Densenet161_v72\r\n",
      "/home/zahar_chikishev/running/preds_Densenet169_v73\r\n",
      "/home/zahar_chikishev/running/preds_Densenet201_v74\r\n",
      "/home/zahar_chikishev/running/preds_se_resnext101_32x4d_v75\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/zahar_chikishev/running/preds*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_all = getPredsOOF()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 8, 674252, 6)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def getSecondStepX(ids_df, preds_all, TH, powerLow=0.7, powerHigh=1.5, fold=0, target=0, ds_idx=0, mode='train'):\n",
    "    \n",
    "    X = np.stack([pickle.load(open(PATH_WORK/'ensemble'/'first.d{}.f{}.t{}.v{}'\n",
    "                                   .format(ds_idx,fold,target,VERSION),'rb')) for ds_idx in range(4)])\n",
    "    \n",
    "    if mode == 'train':\n",
    "        X = X[:,ids_df.fold != fold]\n",
    "        y = ids_df.loc[ids_df.fold != fold, all_ich[target]].values\n",
    "    elif mode == 'valid':\n",
    "        X = X[:,ids_df.fold == fold]\n",
    "        y = ids_df.loc[ids_df.fold == fold, all_ich[target]].values\n",
    "    else:\n",
    "        X = X\n",
    "        y = None\n",
    "    \n",
    "    ll = None\n",
    "    auc = None\n",
    "    if y is not None:\n",
    "        ll = log_loss(y, X.mean(0), eps=1e-7, labels=[0,1])\n",
    "        auc = roc_auc_score(y, X.mean(0))\n",
    "    \n",
    "    return X, y, ll, auc\n",
    "\n",
    "\n",
    "def getFirstStepX(ids_df, preds_all, TH, powerLow=0.7, powerHigh=1.5, fold=0, target=0, ds_idx=0, mode='train'):\n",
    "    \n",
    "    X0 = preds_all[ds_idx]\n",
    "    X0 = X0.mean(0)\n",
    "    \n",
    "    if mode == 'train':\n",
    "        X0 = X0[ids_df.fold != fold]\n",
    "        y = ids_df.loc[ids_df.fold != fold, all_ich[target]].values\n",
    "    elif mode == 'valid':\n",
    "        X0 = X0[ids_df.fold == fold]\n",
    "        y = ids_df.loc[ids_df.fold == fold, all_ich[target]].values\n",
    "    else:\n",
    "        X0 = X0\n",
    "        y = None\n",
    "    \n",
    "    X0 = X0[:,target]\n",
    "    \n",
    "    ll = None\n",
    "    auc = None\n",
    "    if y is not None:\n",
    "        ll = log_loss(y, X0, eps=1e-7, labels=[0,1])\n",
    "        auc = roc_auc_score(y, X0)\n",
    "    \n",
    "    X = np.stack([np.where(X0 > TH,X0,np.zeros(X0.shape)),\n",
    "                  np.where(X0 > TH,scalePreds(X0,power=powerLow,center=TH),np.zeros(X0.shape)),\n",
    "                  np.where(X0 > TH,scalePreds(X0,power=powerHigh,center=TH),np.zeros(X0.shape)),\n",
    "                  np.where(X0 <= TH,X0,np.zeros(X0.shape)),\n",
    "                  np.where(X0 <= TH,scalePreds(X0,power=powerLow,center=TH),np.zeros(X0.shape)),\n",
    "                  np.where(X0 <= TH,scalePreds(X0,power=powerHigh,center=TH),np.zeros(X0.shape)),\n",
    "                  ])\n",
    "    \n",
    "    return X, y, ll, auc\n",
    "\n",
    "\n",
    "def train_ensemble(ids_df, preds_all, fold = 0, target = 0, ds_idx = 0, first_step = True):\n",
    "    \n",
    "    if first_step:\n",
    "        print('starting model',ds_idx,'fold',fold,'target',target)\n",
    "    else:\n",
    "        print('starting fold',fold,'target',target)\n",
    "    \n",
    "    st = time.time()\n",
    "    intercept = False\n",
    "    \n",
    "    limit_low = 1e-15\n",
    "    limit_high = 1 - 1e-4\n",
    "    \n",
    "    prior = ids_df.loc[ids_df.fold != fold, all_ich[target]].mean()\n",
    "    powerLow = 0.7\n",
    "    powerHigh = 1.5\n",
    "    \n",
    "    def my_objective(x,preds,vals):\n",
    "        preds_sum = np.clip((preds * x[:,None]).sum(0), limit_low, limit_high)\n",
    "        res = (- vals * np.log(preds_sum) - (1 - vals) * np.log(1 - preds_sum)).mean()\n",
    "        #print('x   ',x, x.sum())\n",
    "        print('obj ',res)\n",
    "        return res\n",
    "\n",
    "    def my_grad(x,preds,vals):\n",
    "        preds_sum = np.clip((preds * x[:,None]).sum(0), limit_low, limit_high)\n",
    "        res = (- vals * preds / preds_sum + (1 - vals) * preds / (1 - preds_sum)).mean(1)\n",
    "        #print('grad',res)\n",
    "        return res\n",
    "\n",
    "    def my_hess(x,preds,vals):\n",
    "        preds_sum = np.clip((preds * x[:,None]).sum(0), limit_low, limit_high)\n",
    "        res = (preds * np.expand_dims(preds, axis=1) * (vals / preds_sum**2 + (1 - vals) / (1 - preds_sum)**2)).mean(2)\n",
    "        return res\n",
    "    \n",
    "    if first_step: data_func = getFirstStepX\n",
    "    else: data_func = getSecondStepX\n",
    "    \n",
    "    X,y,ll_train,auc_train =  data_func(ids_df, preds_all, TH=prior, powerLow=powerLow, powerHigh=powerHigh, \n",
    "                                        fold=fold, target=target, ds_idx=ds_idx)\n",
    "    \n",
    "    bnds_low = np.zeros(X.shape[0])\n",
    "    bnds_high = np.ones(X.shape[0])\n",
    "    \n",
    "    if first_step:\n",
    "        initial_sol = np.array([1,0,0,1,0,0])\n",
    "        splitted = True\n",
    "    else:\n",
    "        initial_sol = np.ones(4)/4\n",
    "        splitted = False\n",
    "    \n",
    "    if intercept:\n",
    "        X = np.concatenate([X,np.ones((1,X.shape[1]))], axis=0)\n",
    "        bnds_low = np.concatenate([bnds_low, -0.1*np.ones(1)])\n",
    "        bnds_high = np.concatenate([bnds_high, 0.1*np.ones(1)])\n",
    "        initial_sol = np.concatenate([initial_sol, np.zeros(1)])\n",
    "\n",
    "    bnds = sp.optimize.Bounds(bnds_low, bnds_high)\n",
    "    cons = sp.optimize.LinearConstraint(np.ones((1,X.shape[0])), 0.95 + splitted, 1.01 + splitted)\n",
    "    \n",
    "    model = sp.optimize.minimize(my_objective, initial_sol, jac=my_grad, hess=my_hess, args=(X, y),\n",
    "                                 bounds=bnds, method='trust-constr', constraints=cons,\n",
    "                                 options={'gtol': 1e-11, 'initial_tr_radius': 0.1, 'initial_barrier_parameter': 0.01})\n",
    "    model.prior = prior\n",
    "    model.powerLow = powerLow\n",
    "    model.powerHigh = powerHigh\n",
    "    \n",
    "    pickle.dump(model, open(PATH_WORK/'ensemble'/'model.d{}.f{}.t{}.v{}'\n",
    "                            .format(ds_idx,fold,target,VERSION),'wb'))\n",
    "\n",
    "    train_preds = (X*np.expand_dims(model.x, axis=1)).sum(0)\n",
    "    ll_train2 = log_loss(y, train_preds, eps=1e-7, labels=[0,1])\n",
    "    auc_train2 = roc_auc_score(y, train_preds)\n",
    "    \n",
    "    X,y,ll_val,auc_val =  data_func(ids_df, preds_all, TH=prior, powerLow=powerLow, powerHigh=powerHigh, \n",
    "                                    fold=fold, target=target, ds_idx=ds_idx, mode='valid')\n",
    "    \n",
    "    val_preds = (X*np.expand_dims(model.x, axis=1)).sum(0)\n",
    "    \n",
    "    if first_step:\n",
    "        total_preds = np.zeros(len(ids_df))\n",
    "        total_preds[ids_df.fold == fold] = val_preds\n",
    "        total_preds[ids_df.fold != fold] = train_preds\n",
    "        pickle.dump(total_preds, open(PATH_WORK/'ensemble'/'first.d{}.f{}.t{}.v{}'\n",
    "                                      .format(ds_idx,fold,target,VERSION),'wb'))\n",
    "    \n",
    "    ll_val2 = log_loss(y, val_preds, eps=1e-7, labels=[0,1])\n",
    "    auc_val2 = roc_auc_score(y, val_preds)\n",
    "    \n",
    "    print('v{} d{} f{} t{}: original ll {:.4f} auc {:.4f}, ensemble ll {:.4f} auc {:.4f}'\n",
    "          .format(VERSION,ds_idx,fold,target,ll_val,auc_val,ll_val2,auc_val2))\n",
    "    \n",
    "    run_time = time.time() - st\n",
    "    print('running time', run_time)\n",
    "    \n",
    "    stats = pd.DataFrame([[VERSION,ds_idx,fold,target,\n",
    "                           ll_train,auc_train,ll_train2,auc_train2,\n",
    "                           ll_val,auc_val,ll_val2,auc_val2,run_time]], \n",
    "                           columns = \n",
    "                            ['version','ds_idx','fold','target',\n",
    "                             'train_loss','train_auc','train_loss_ens','train_auc_ens', \n",
    "                             'valid_loss','valid_auc','valid_loss_ens','valid_auc_ens',\n",
    "                             'run_time'\n",
    "                             ])\n",
    "\n",
    "    stats_filename = PATH_WORK/'ensemble'/'stats.v{}'.format(VERSION)\n",
    "    if stats_filename.is_file():\n",
    "        stats = pd.concat([pd.read_csv(stats_filename), stats], sort=False)\n",
    "    stats.to_csv(stats_filename, index=False)\n",
    "\n",
    "#model.cols = Xt.columns\n",
    "#predictions[data_filt['fold'] == i] = (Xv*model.x).sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting model 0 fold 0 target 0\n",
      "obj  0.09926467873459585\n",
      "obj  0.09911825742334494\n",
      "obj  0.09879360692811408\n",
      "obj  0.09924841676091367\n",
      "obj  0.09864416385657723\n",
      "obj  0.09882009438985377\n",
      "obj  0.09873836280505063\n",
      "obj  0.09876450308381256\n",
      "obj  0.09842748253010512\n",
      "obj  0.09827468443299503\n",
      "obj  0.09822368150726406\n",
      "obj  0.09820574978303684\n",
      "obj  0.09820132499038028\n",
      "obj  0.09820118081760587\n",
      "obj  0.09820117621084451\n",
      "obj  0.09819981323989463\n",
      "obj  0.09819973421441387\n",
      "obj  0.09819973420764647\n",
      "obj  0.09819970224116535\n",
      "v76 d0 f0 t0: original ll 0.1002 auc 0.9860, ensemble ll 0.0993 auc 0.9860\n",
      "running time 8.924199342727661\n",
      "starting model 0 fold 0 target 1\n",
      "obj  0.017194960667545497\n",
      "obj  0.017154764825970326\n",
      "obj  0.01703908798721283\n",
      "obj  0.016814591872711455\n",
      "obj  0.016535083638072236\n",
      "obj  0.016200246732408354\n",
      "obj  0.01636177171287361\n",
      "obj  0.016160342471153083\n",
      "obj  0.01595771472933327\n",
      "obj  0.01562679281309271\n",
      "obj  0.015519065089372872\n",
      "obj  0.015476325790266877\n",
      "obj  0.015472975183255815\n",
      "obj  0.015471549563289692\n",
      "obj  0.015471213474896793\n",
      "obj  0.015471076904887226\n",
      "obj  0.015471076892186221\n",
      "obj  0.015471059500403977\n",
      "obj  0.015471059500003895\n",
      "obj  0.015471055076549256\n",
      "obj  0.015471053815404801\n",
      "v76 d0 f0 t1: original ll 0.0178 auc 0.9684, ensemble ll 0.0172 auc 0.9684\n",
      "running time 10.065671920776367\n",
      "starting model 0 fold 0 target 2\n",
      "obj  0.04130186388543923\n",
      "obj  0.04128691105626661\n",
      "obj  0.04126800992407741\n",
      "obj  0.04131376759801301\n",
      "obj  0.041251157390389155\n",
      "obj  0.04125504861111706\n",
      "obj  0.04125391429341011\n",
      "obj  0.041256431179694165\n",
      "obj  0.041234399256870044\n",
      "obj  0.04123437435768693\n",
      "obj  0.0412257426645296\n",
      "obj  0.04122062006257879\n",
      "obj  0.041213937715183335\n",
      "obj  0.04121377205547188\n",
      "obj  0.04121377117240336\n",
      "obj  0.041212556607687634\n",
      "obj  0.041212435304807296\n",
      "obj  0.041212413861334535\n",
      "obj  0.041212408056458164\n",
      "obj  0.04121240805645615\n",
      "obj  0.041212407562862347\n",
      "v76 d0 f0 t2: original ll 0.0440 auc 0.9915, ensemble ll 0.0438 auc 0.9915\n",
      "running time 9.36994481086731\n",
      "starting model 0 fold 0 target 3\n",
      "obj  0.025093027201830032\n",
      "obj  0.025100394301201444\n",
      "obj  0.025130331771298354\n",
      "obj  0.024988868375246682\n",
      "obj  0.025097263650387988\n",
      "obj  0.025054121204625794\n",
      "obj  0.025061726579547004\n",
      "obj  0.025057605331654405\n",
      "obj  0.025008917286164264\n",
      "obj  0.024930977177313934\n",
      "obj  0.024921989438787755\n",
      "obj  0.024921864661049464\n",
      "obj  0.02492000227769396\n",
      "obj  0.024919638047047887\n",
      "obj  0.02491951480152825\n",
      "obj  0.024919514639449098\n",
      "obj  0.02491946830468867\n",
      "v76 d0 f0 t3: original ll 0.0266 auc 0.9959, ensemble ll 0.0266 auc 0.9959\n",
      "running time 7.835027456283569\n",
      "starting model 0 fold 0 target 4\n",
      "obj  0.06508414696959167\n",
      "obj  0.0650056020801253\n",
      "obj  0.06491419625664151\n",
      "obj  0.06497496061298053\n",
      "obj  0.0648810419592342\n",
      "obj  0.06488125244413598\n",
      "obj  0.06487887350873016\n",
      "obj  0.06488442942755945\n",
      "obj  0.06485983527874441\n",
      "obj  0.06481835189446068\n",
      "obj  0.0648115224628417\n",
      "obj  0.06481042076138278\n",
      "obj  0.06480848958800706\n",
      "obj  0.06480846763911267\n",
      "obj  0.06480832354558752\n",
      "obj  0.06480829711419953\n",
      "obj  0.064808290607813\n",
      "obj  0.06480828923632044\n",
      "v76 d0 f0 t4: original ll 0.0681 auc 0.9790, ensemble ll 0.0682 auc 0.9790\n",
      "running time 8.25486969947815\n",
      "starting model 0 fold 0 target 5\n",
      "obj  0.08088457673442043\n",
      "obj  0.0806451840179266\n",
      "obj  0.08043551659117197\n",
      "obj  0.08055013180260083\n",
      "obj  0.08032833878639116\n",
      "obj  0.08032575318791417\n",
      "obj  0.08032739956764756\n",
      "obj  0.08032478270545654\n",
      "obj  0.0802604810241581\n",
      "obj  0.08020005127053502\n",
      "obj  0.08018601922894833\n",
      "obj  0.08018142074466215\n",
      "obj  0.08018129515884148\n",
      "obj  0.08018017906076774\n",
      "obj  0.08017938599737226\n",
      "obj  0.08017933371311724\n",
      "obj  0.08017933367552121\n",
      "obj  0.08017931747466646\n",
      "v76 d0 f0 t5: original ll 0.0842 auc 0.9772, ensemble ll 0.0838 auc 0.9772\n",
      "running time 8.321561336517334\n",
      "starting model 1 fold 0 target 0\n",
      "obj  0.10022737575467663\n",
      "obj  0.10006793712200956\n",
      "obj  0.09977361432007564\n",
      "obj  0.10024628817033122\n",
      "obj  0.09967222691178416\n",
      "obj  0.09984305294403571\n",
      "obj  0.09977380101914916\n",
      "obj  0.09979853263160113\n",
      "obj  0.09949426006337911\n",
      "obj  0.09934096329764579\n",
      "obj  0.09928340460784862\n",
      "obj  0.09927359357641861\n",
      "obj  0.0992718719211591\n",
      "obj  0.09927186127070649\n",
      "obj  0.09927140164860707\n",
      "obj  0.09927127534440172\n",
      "obj  0.09927127533531543\n",
      "obj  0.09927124761531825\n",
      "obj  0.09927124761445177\n",
      "obj  0.09927123983128541\n",
      "obj  0.0992712398312745\n",
      "obj  0.09927123905997809\n",
      "obj  0.09927123886802655\n",
      "v76 d1 f0 t0: original ll 0.1004 auc 0.9861, ensemble ll 0.0994 auc 0.9861\n",
      "running time 10.190027952194214\n",
      "starting model 1 fold 0 target 1\n",
      "obj  0.016226887492066654\n",
      "obj  0.016138335502280582\n",
      "obj  0.01587037182029892\n",
      "obj  0.015434029614551151\n",
      "obj  0.015490485043057486\n",
      "obj  0.015575306850886036\n",
      "obj  0.015577878805233895\n",
      "obj  0.015409768738209494\n",
      "obj  0.015239790353159583\n",
      "obj  0.015016562214853507\n",
      "obj  0.014930647745665791\n",
      "obj  0.014877201014269583\n",
      "obj  0.014876290549613149\n",
      "obj  0.01486635913312086\n",
      "obj  0.01486621241379777\n",
      "obj  0.014864154527434078\n",
      "obj  0.014864121723075718\n",
      "obj  0.014863502755814524\n",
      "obj  0.014863412530539126\n",
      "obj  0.014863374568509502\n",
      "obj  0.014863363085970577\n",
      "obj  0.014863361597062093\n",
      "obj  0.014863361033572307\n",
      "obj  0.014863360944771975\n",
      "v76 d1 f0 t1: original ll 0.0182 auc 0.9710, ensemble ll 0.0176 auc 0.9707\n",
      "running time 10.40234637260437\n",
      "starting model 1 fold 0 target 2\n",
      "obj  0.040825026075886756\n",
      "obj  0.040818604846836234\n",
      "obj  0.04081838908317119\n",
      "obj  0.04088605692101715\n",
      "obj  0.04080358648446539\n",
      "obj  0.04079491171364329\n",
      "obj  0.04079528213387888\n",
      "obj  0.04079551730486396\n",
      "obj  0.04076246378633235\n",
      "obj  0.040760003364837566\n",
      "obj  0.040747528145357786\n",
      "obj  0.04074246477778873\n",
      "obj  0.04074089622775395\n",
      "obj  0.04073973231573859\n",
      "obj  0.04073968223146861\n",
      "obj  0.04073967842593826\n",
      "obj  0.040739667014362706\n",
      "obj  0.04073966364609567\n",
      "v76 d1 f0 t2: original ll 0.0443 auc 0.9920, ensemble ll 0.0442 auc 0.9920\n",
      "running time 8.197378158569336\n",
      "starting model 1 fold 0 target 3\n",
      "obj  0.02493549364873909\n",
      "obj  0.02494834741019865\n",
      "obj  0.024987516080678986\n",
      "obj  0.024821540023231194\n",
      "obj  0.024960769610232695\n",
      "obj  0.02491742958312754\n",
      "obj  0.02492569731897339\n",
      "obj  0.024920753618523053\n",
      "obj  0.02484959877370836\n",
      "obj  0.024784560382454447\n",
      "obj  0.024756601773583076\n",
      "obj  0.024753654165201467\n",
      "obj  0.02475217098345273\n",
      "obj  0.024752162791651815\n",
      "obj  0.024751814608321484\n",
      "obj  0.024751711992578623\n",
      "obj  0.024751702786170456\n",
      "obj  0.024751701674394028\n",
      "obj  0.02475170128853179\n",
      "v76 d1 f0 t3: original ll 0.0267 auc 0.9962, ensemble ll 0.0266 auc 0.9962\n",
      "running time 8.542699813842773\n",
      "starting model 1 fold 0 target 4\n",
      "obj  0.06558400816020572\n",
      "obj  0.06551894124157923\n",
      "obj  0.06547875095960487\n",
      "obj  0.06566122416399331\n",
      "obj  0.0654496283581345\n",
      "obj  0.06543874089612162\n",
      "obj  0.06543648705448495\n",
      "obj  0.06543923563604058\n",
      "obj  0.06542716793759187\n",
      "obj  0.06539538915819841\n",
      "obj  0.06538701526507462\n",
      "obj  0.06537767808780665\n",
      "obj  0.06537475513604858\n",
      "obj  0.06537432718154636\n",
      "obj  0.06537427867676443\n",
      "obj  0.06537427276800112\n",
      "v76 d1 f0 t4: original ll 0.0672 auc 0.9801, ensemble ll 0.0671 auc 0.9801\n",
      "running time 7.673709869384766\n",
      "starting model 1 fold 0 target 5\n",
      "obj  0.08060534559399915\n",
      "obj  0.08045527957904894\n",
      "obj  0.08031653296029988\n",
      "obj  0.08050467911887646\n",
      "obj  0.08022936617596266\n",
      "obj  0.08023974523213337\n",
      "obj  0.08024097633993309\n",
      "obj  0.08023226235641338\n",
      "obj  0.08012825516820521\n",
      "obj  0.08005773639377047\n",
      "obj  0.0800337227801675\n",
      "obj  0.08003059511933643\n",
      "obj  0.08002816653925879\n",
      "obj  0.08002803301110639\n",
      "obj  0.08002763569973699\n",
      "obj  0.08002757434989416\n",
      "obj  0.08002755898868112\n",
      "obj  0.0800275545528154\n",
      "v76 d1 f0 t5: original ll 0.0840 auc 0.9774, ensemble ll 0.0833 auc 0.9774\n",
      "running time 8.319159746170044\n",
      "starting model 2 fold 0 target 0\n",
      "obj  0.09980969686785161\n",
      "obj  0.09966658306631213\n",
      "obj  0.0993677902440094\n",
      "obj  0.09979947067795951\n",
      "obj  0.09924943980365503\n",
      "obj  0.09940976193187175\n",
      "obj  0.09934032759506081\n",
      "obj  0.09936366440219797\n",
      "obj  0.09908192650600227\n",
      "obj  0.09892816357498478\n",
      "obj  0.09888103381630221\n",
      "obj  0.09886398805770176\n",
      "obj  0.09885945518763141\n",
      "obj  0.09885874859028518\n",
      "obj  0.09885867241108856\n",
      "v76 d2 f0 t0: original ll 0.1004 auc 0.9860, ensemble ll 0.0997 auc 0.9860\n",
      "running time 7.15198826789856\n",
      "starting model 2 fold 0 target 1\n",
      "obj  0.0168050345862677\n",
      "obj  0.016712453541523464\n",
      "obj  0.016506788403717446\n",
      "obj  0.01595818432334316\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obj  0.01582789872048471\n",
      "obj  0.015890395565302765\n",
      "obj  0.01587206927353005\n",
      "obj  0.01566773375040523\n",
      "obj  0.015483292115584118\n",
      "obj  0.01517522710003437\n",
      "obj  0.015067664385264664\n",
      "obj  0.015020024939038736\n",
      "obj  0.015016965237238668\n",
      "obj  0.015015166561747433\n",
      "obj  0.015015165992919821\n",
      "obj  0.01501488853989072\n",
      "obj  0.015014830835405944\n",
      "obj  0.015014811708787107\n",
      "obj  0.015014811708688382\n",
      "obj  0.015014807660111906\n",
      "obj  0.015014807121803794\n",
      "v76 d2 f0 t1: original ll 0.0171 auc 0.9733, ensemble ll 0.0166 auc 0.9733\n",
      "running time 9.217986583709717\n",
      "starting model 2 fold 0 target 2\n",
      "obj  0.04055074252562783\n",
      "obj  0.040547359297080955\n",
      "obj  0.04054297278993602\n",
      "obj  0.0406297113625654\n",
      "obj  0.0405314214155455\n",
      "obj  0.04052971146610897\n",
      "obj  0.040528791580052896\n",
      "obj  0.04052941272034514\n",
      "obj  0.04052357970279575\n",
      "obj  0.0405235742353708\n",
      "obj  0.040513855816514566\n",
      "obj  0.04051385053850938\n",
      "obj  0.04050751850005939\n",
      "obj  0.04050676015503658\n",
      "obj  0.040505866555216194\n",
      "obj  0.04050575095212523\n",
      "obj  0.0405057139836041\n",
      "obj  0.04050571398305452\n",
      "obj  0.04050570581440354\n",
      "obj  0.040505703458920336\n",
      "v76 d2 f0 t2: original ll 0.0448 auc 0.9916, ensemble ll 0.0446 auc 0.9916\n",
      "running time 8.887040615081787\n",
      "starting model 2 fold 0 target 3\n",
      "obj  0.025123308052214192\n",
      "obj  0.025148157230843682\n",
      "obj  0.02516273342428595\n",
      "obj  0.02504269456436714\n",
      "obj  0.025139553619309966\n",
      "obj  0.025108074504499037\n",
      "obj  0.025110953226908494\n",
      "obj  0.025107417071697467\n",
      "obj  0.02505989430733173\n",
      "obj  0.025006674892177394\n",
      "obj  0.024988779592481306\n",
      "obj  0.024986149483584286\n",
      "obj  0.024985047862660748\n",
      "obj  0.02498504691517878\n",
      "obj  0.02498477342547716\n",
      "obj  0.024984705531061472\n",
      "obj  0.024984692224440996\n",
      "obj  0.02498468999597831\n",
      "obj  0.024984689489191074\n",
      "v76 d2 f0 t3: original ll 0.0270 auc 0.9960, ensemble ll 0.0270 auc 0.9960\n",
      "running time 8.491024494171143\n",
      "starting model 2 fold 0 target 4\n",
      "obj  0.06511640766668834\n",
      "obj  0.06506009221773827\n",
      "obj  0.06501100747033671\n",
      "obj  0.06523786816275087\n",
      "obj  0.06497821447370612\n",
      "obj  0.06497329416895271\n",
      "obj  0.06497011248580965\n",
      "obj  0.06497407304789492\n",
      "obj  0.06496906687433243\n",
      "obj  0.06492280339976064\n",
      "obj  0.06492269361609725\n",
      "obj  0.06491539412161311\n",
      "obj  0.06491187668838436\n",
      "obj  0.06491182487937341\n",
      "obj  0.06491168333928364\n",
      "obj  0.06491165345133361\n",
      "obj  0.06491164689086928\n",
      "v76 d2 f0 t4: original ll 0.0675 auc 0.9797, ensemble ll 0.0674 auc 0.9797\n",
      "running time 7.911777019500732\n",
      "starting model 2 fold 0 target 5\n",
      "obj  0.08067953392499468\n",
      "obj  0.08044035840457497\n",
      "obj  0.08024701845826451\n",
      "obj  0.08032856070302354\n",
      "obj  0.08014349100774142\n",
      "obj  0.08013458800376441\n",
      "obj  0.08013642889667677\n",
      "obj  0.0801353134069234\n",
      "obj  0.08009280786480823\n",
      "obj  0.08004524112822632\n",
      "obj  0.08003240901313582\n",
      "obj  0.08002843463168502\n",
      "obj  0.08002829056762545\n",
      "obj  0.08002719978226161\n",
      "obj  0.0800265377961428\n",
      "obj  0.08002653033222225\n",
      "obj  0.08002645633766882\n",
      "obj  0.08002644171124917\n",
      "v76 d2 f0 t5: original ll 0.0840 auc 0.9775, ensemble ll 0.0836 auc 0.9775\n",
      "running time 8.143568277359009\n",
      "starting model 3 fold 0 target 0\n",
      "obj  0.09821370470674369\n",
      "obj  0.09803365063383213\n",
      "obj  0.09778744636960787\n",
      "obj  0.0983004176638072\n",
      "obj  0.0977038903987194\n",
      "obj  0.09788330697630142\n",
      "obj  0.0978011408182324\n",
      "obj  0.0978274055552335\n",
      "obj  0.09750100299792468\n",
      "obj  0.09733572445859483\n",
      "obj  0.09725753639435418\n",
      "obj  0.0972346496372381\n",
      "obj  0.09723069539950314\n",
      "obj  0.09723067449561022\n",
      "obj  0.09722964627040748\n",
      "obj  0.09722964116282316\n",
      "obj  0.09722948069718546\n",
      "obj  0.09722948059548493\n",
      "obj  0.09722948059045122\n",
      "obj  0.09722944940622154\n",
      "obj  0.09722944189917074\n",
      "obj  0.097229440149422\n",
      "obj  0.09722943974395457\n",
      "v76 d3 f0 t0: original ll 0.0997 auc 0.9863, ensemble ll 0.0989 auc 0.9863\n",
      "running time 10.043643474578857\n",
      "starting model 3 fold 0 target 1\n",
      "obj  0.014700965490743519\n",
      "obj  0.014700028078493594\n",
      "obj  0.014697721730433577\n",
      "obj  0.014558185155343939\n",
      "obj  0.014629847583636666\n",
      "obj  0.014632386901100823\n",
      "obj  0.014633476001876635\n",
      "obj  0.014611825272796034\n",
      "obj  0.01459295971814118\n",
      "obj  0.014526185897718343\n",
      "obj  0.014501608791126481\n",
      "obj  0.014497276669057736\n",
      "obj  0.014496804881789347\n",
      "obj  0.014496587513540882\n",
      "obj  0.014496565767404889\n",
      "v76 d3 f0 t1: original ll 0.0173 auc 0.9702, ensemble ll 0.0173 auc 0.9702\n",
      "running time 7.008997917175293\n",
      "starting model 3 fold 0 target 2\n",
      "obj  0.04046981063452452\n",
      "obj  0.04046593538377204\n",
      "obj  0.04047479972799617\n",
      "obj  0.04050800645363974\n",
      "obj  0.04046116649883481\n",
      "obj  0.04045020282035521\n",
      "obj  0.04045131092937722\n",
      "obj  0.04045076518734575\n",
      "obj  0.04042232965490731\n",
      "obj  0.0404222874811763\n",
      "obj  0.040414700525369385\n",
      "obj  0.04041156738737975\n",
      "obj  0.04040757790794012\n",
      "obj  0.04040748534657333\n",
      "obj  0.04040686358445861\n",
      "obj  0.04040682650668275\n",
      "obj  0.040406825445106966\n",
      "obj  0.040406816511647584\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511766724\n",
      "obj  0.040406816511703365\n",
      "obj  0.040406816511703365\n",
      "obj  0.04040681651166063\n",
      "obj  0.04040681651166063\n",
      "v76 d3 f0 t2: original ll 0.0441 auc 0.9919, ensemble ll 0.0440 auc 0.9919\n",
      "running time 10.088359594345093\n",
      "starting model 3 fold 0 target 3\n",
      "obj  0.024837810545178068\n",
      "obj  0.024848916498035778\n",
      "obj  0.024890232316470372\n",
      "obj  0.02472407670323437\n",
      "obj  0.024864349014486538\n",
      "obj  0.024821335031042287\n",
      "obj  0.024829504651008194\n",
      "obj  0.024824539563667807\n",
      "obj  0.024766943610476495\n",
      "obj  0.024697814795649493\n",
      "obj  0.024669189149658464\n",
      "obj  0.024665943870313897\n",
      "obj  0.02466460471841175\n",
      "obj  0.02466459985489693\n",
      "obj  0.0246642436217033\n",
      "obj  0.02466414364456416\n",
      "obj  0.024664131824970757\n",
      "obj  0.02466413182475818\n",
      "obj  0.024664128477868492\n",
      "obj  0.02466412789537415\n",
      "v76 d3 f0 t3: original ll 0.0270 auc 0.9958, ensemble ll 0.0269 auc 0.9958\n",
      "running time 8.902011394500732\n",
      "starting model 3 fold 0 target 4\n",
      "obj  0.06475737656331912\n",
      "obj  0.06469832409673001\n",
      "obj  0.06465124344429796\n",
      "obj  0.06476056439874125\n",
      "obj  0.06463087152706722\n",
      "obj  0.06462573719990032\n",
      "obj  0.0646278169693091\n",
      "obj  0.06461257778139576\n",
      "obj  0.06458004140236437\n",
      "obj  0.06455392607596319\n",
      "obj  0.06453475548527891\n",
      "obj  0.06452944540646995\n",
      "obj  0.06452944139559168\n",
      "obj  0.06452882289849689\n",
      "obj  0.06452872674195305\n",
      "obj  0.06452868229382276\n",
      "obj  0.06452867298789314\n",
      "obj  0.06452867115236967\n",
      "v76 d3 f0 t4: original ll 0.0661 auc 0.9808, ensemble ll 0.0661 auc 0.9808\n",
      "running time 8.11098337173462\n",
      "starting model 3 fold 0 target 5\n",
      "obj  0.07917067147971014\n",
      "obj  0.07905776826529846\n",
      "obj  0.07894110033423342\n",
      "obj  0.07910743441238575\n",
      "obj  0.07886469160321065\n",
      "obj  0.07887713604407381\n",
      "obj  0.07887665398130171\n",
      "obj  0.07888588142943445\n",
      "obj  0.07880097645775075\n",
      "obj  0.07873234205938094\n",
      "obj  0.07871084163623764\n",
      "obj  0.07870581962701599\n",
      "obj  0.07870494959238307\n",
      "obj  0.07870470961581728\n",
      "obj  0.07870469890903727\n",
      "obj  0.07870469659781448\n",
      "obj  0.07870469618539182\n",
      "v76 d3 f0 t5: original ll 0.0833 auc 0.9781, ensemble ll 0.0827 auc 0.9781\n",
      "running time 7.882029294967651\n",
      "starting model 0 fold 1 target 0\n",
      "obj  0.09841298898972578\n",
      "obj  0.09829829264687637\n",
      "obj  0.09807565294145858\n",
      "obj  0.09862904653700781\n",
      "obj  0.09800930648937718\n",
      "obj  0.09820098176572331\n",
      "obj  0.09812082203184569\n",
      "obj  0.09814866492627983\n",
      "obj  0.09775433538752884\n",
      "obj  0.09757375884440271\n",
      "obj  0.09752284122702191\n",
      "obj  0.09750535799286544\n",
      "obj  0.09750151761233261\n",
      "obj  0.0975008278566738\n",
      "obj  0.09750082785667367\n",
      "obj  0.0975008278566738\n",
      "obj  0.09750082785667367\n",
      "obj  0.0975008278566738\n",
      "obj  0.09750082785667367\n",
      "obj  0.0975008278566738\n",
      "obj  0.09750082785667367\n",
      "obj  0.0975008278566738\n",
      "obj  0.09750082785667367\n",
      "obj  0.0975008278566738\n",
      "obj  0.09750082785667367\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obj  0.09750099645261982\n",
      "obj  0.09750077697684284\n",
      "obj  0.09750077696884875\n",
      "obj  0.09750072393521342\n",
      "obj  0.09750072393332238\n",
      "obj  0.09750071018758627\n",
      "v76 d0 f1 t0: original ll 0.1019 auc 0.9857, ensemble ll 0.1007 auc 0.9857\n",
      "running time 9.243682861328125\n",
      "starting model 0 fold 1 target 1\n",
      "obj  0.017858351424191295\n",
      "obj  0.017802504308753474\n",
      "obj  0.017565354111875493\n",
      "obj  0.01726474879793198\n",
      "obj  0.017311781203564865\n",
      "obj  0.01736019986228441\n",
      "obj  0.01736396698978517\n",
      "obj  0.017302048941322107\n",
      "obj  0.01704722203779455\n",
      "obj  0.016912450270774402\n",
      "obj  0.016870555830797596\n",
      "obj  0.016856195743700245\n",
      "obj  0.016852159173078234\n",
      "obj  0.016851080303497107\n",
      "obj  0.016850754490850505\n",
      "obj  0.016850605711319487\n",
      "obj  0.0168506056675316\n",
      "obj  0.016850558535376006\n",
      "obj  0.016850546044407166\n",
      "obj  0.016850546044329912\n",
      "obj  0.016850543521725745\n",
      "obj  0.01685054352172195\n",
      "obj  0.016850542909607816\n",
      "obj  0.016850542771125063\n",
      "v76 d0 f1 t1: original ll 0.0164 auc 0.9573, ensemble ll 0.0147 auc 0.9573\n",
      "running time 10.520732164382935\n",
      "starting model 0 fold 1 target 2\n",
      "obj  0.04156216529174326\n",
      "obj  0.041533000952494854\n",
      "obj  0.041527297811990715\n",
      "obj  0.04152304936956466\n",
      "obj  0.041504345502875746\n",
      "obj  0.04148887126184566\n",
      "obj  0.04149002807568122\n",
      "obj  0.041490184354577894\n",
      "obj  0.041483862925811896\n",
      "obj  0.041459855894004326\n",
      "obj  0.04145346605500124\n",
      "obj  0.04145223864628452\n",
      "obj  0.041451284938544976\n",
      "obj  0.04145114098611791\n",
      "obj  0.041451124789720975\n",
      "obj  0.04145111875054359\n",
      "obj  0.0414511183114825\n",
      "obj  0.04145111825726488\n",
      "v76 d0 f1 t2: original ll 0.0435 auc 0.9914, ensemble ll 0.0433 auc 0.9914\n",
      "running time 9.17185926437378\n",
      "starting model 0 fold 1 target 3\n",
      "obj  0.02585916876972873\n",
      "obj  0.02586533615545094\n",
      "obj  0.025873504467944477\n",
      "obj  0.02583094087312304\n",
      "obj  0.02585279414200362\n",
      "obj  0.025830640126859983\n",
      "obj  0.025834003091841317\n",
      "obj  0.02583225686189903\n",
      "obj  0.02581450324876319\n",
      "obj  0.025779727120808855\n",
      "obj  0.025778927145049934\n",
      "obj  0.025773643878652688\n",
      "obj  0.025771004520375954\n",
      "obj  0.02577082826999976\n",
      "obj  0.025770582556460097\n",
      "obj  0.02577050270022235\n",
      "obj  0.025770493907825397\n",
      "v76 d0 f1 t3: original ll 0.0251 auc 0.9964, ensemble ll 0.0249 auc 0.9964\n",
      "running time 8.30525541305542\n",
      "starting model 0 fold 1 target 4\n",
      "obj  0.06673627514146419\n",
      "obj  0.06672882909599452\n",
      "obj  0.0666355437473637\n",
      "obj  0.06688572985307303\n",
      "obj  0.06662716001891665\n",
      "obj  0.06664400786760995\n",
      "obj  0.06664378546982462\n",
      "obj  0.0666487326910167\n",
      "obj  0.0666177368656975\n",
      "obj  0.06659265764931425\n",
      "obj  0.0665822628896322\n",
      "obj  0.06657654206452329\n",
      "obj  0.06657416848100253\n",
      "obj  0.0665741645249309\n",
      "obj  0.0665741126668922\n",
      "obj  0.06657411261277965\n",
      "obj  0.06657409888711543\n",
      "obj  0.06657409634245835\n",
      "v76 d0 f1 t4: original ll 0.0648 auc 0.9797, ensemble ll 0.0647 auc 0.9797\n",
      "running time 8.674980401992798\n",
      "starting model 0 fold 1 target 5\n",
      "obj  0.08128238805477153\n",
      "obj  0.0810950577668775\n",
      "obj  0.0809376534029094\n",
      "obj  0.08118215710330907\n",
      "obj  0.08086486833297259\n",
      "obj  0.08089361045887884\n",
      "obj  0.08089195009299585\n",
      "obj  0.08088902023840153\n",
      "obj  0.08077016451927119\n",
      "obj  0.08067605923490935\n",
      "obj  0.08064462962641164\n",
      "obj  0.08063853325654943\n",
      "obj  0.08063683622272506\n",
      "obj  0.08063591634713048\n",
      "obj  0.08063572559927519\n",
      "obj  0.08063570106877387\n",
      "v76 d0 f1 t5: original ll 0.0834 auc 0.9772, ensemble ll 0.0828 auc 0.9772\n",
      "running time 8.111640214920044\n",
      "starting model 1 fold 1 target 0\n",
      "obj  0.09927187122465124\n",
      "obj  0.09909328496670873\n",
      "obj  0.09886537162484209\n",
      "obj  0.0992875427019446\n",
      "obj  0.09879185716072653\n",
      "obj  0.09893634361553312\n",
      "obj  0.09887109870244883\n",
      "obj  0.09889201707753581\n",
      "obj  0.09860822145005449\n",
      "obj  0.09848167123705402\n",
      "obj  0.09844582735308167\n",
      "obj  0.09842737545328317\n",
      "obj  0.09842296581243475\n",
      "obj  0.09842191428040027\n",
      "obj  0.09842191129734178\n",
      "obj  0.09842163616240682\n",
      "obj  0.09842159438590334\n",
      "obj  0.09842158465850374\n",
      "obj  0.09842158237042793\n",
      "obj  0.09842158184967519\n",
      "v76 d1 f1 t0: original ll 0.1023 auc 0.9855, ensemble ll 0.1012 auc 0.9855\n",
      "running time 9.422157764434814\n",
      "starting model 1 fold 1 target 1\n",
      "obj  0.017795680691249247\n",
      "obj  0.017721254684510097\n",
      "obj  0.017418883099249438\n",
      "obj  0.017022180351469737\n",
      "obj  0.01717092127634977\n",
      "obj  0.01721368350086536\n",
      "obj  0.017220125548560915\n",
      "obj  0.017150292527040697\n",
      "obj  0.016882028984477857\n",
      "obj  0.016729524487126474\n",
      "obj  0.016651066138677912\n",
      "obj  0.01663501566983042\n",
      "obj  0.016632836548012078\n",
      "obj  0.016632482857746732\n",
      "obj  0.016632398994869763\n",
      "obj  0.016632393750526775\n",
      "v76 d1 f1 t1: original ll 0.0150 auc 0.9560, ensemble ll 0.0142 auc 0.9560\n",
      "running time 7.7681190967559814\n",
      "starting model 1 fold 1 target 2\n",
      "obj  0.041647530569392785\n",
      "obj  0.04161065164698271\n",
      "obj  0.04161437206204817\n",
      "obj  0.04158866209824554\n",
      "obj  0.04159697004507412\n",
      "obj  0.041575443888711816\n",
      "obj  0.04157667571108304\n",
      "obj  0.041575655573177765\n",
      "obj  0.041559883567312765\n",
      "obj  0.04153880923143934\n",
      "obj  0.041531950416959514\n",
      "obj  0.04153158886393351\n",
      "obj  0.041531182288093896\n",
      "obj  0.041531096332642854\n",
      "obj  0.04153106735136857\n",
      "v76 d1 f1 t2: original ll 0.0426 auc 0.9913, ensemble ll 0.0425 auc 0.9913\n",
      "running time 7.761897325515747\n",
      "starting model 1 fold 1 target 3\n",
      "obj  0.026083467587335145\n",
      "obj  0.0260774272359128\n",
      "obj  0.02610287930999458\n",
      "obj  0.026012710735357367\n",
      "obj  0.026088131869908732\n",
      "obj  0.026047943049179637\n",
      "obj  0.02605311012521967\n",
      "obj  0.026048778784787615\n",
      "obj  0.02600195730232793\n",
      "obj  0.025950451024866147\n",
      "obj  0.02593121775362035\n",
      "obj  0.025928364803951724\n",
      "obj  0.02592745322412958\n",
      "obj  0.025927452243536937\n",
      "obj  0.025927212793134435\n",
      "obj  0.025927172622613858\n",
      "v76 d1 f1 t3: original ll 0.0244 auc 0.9967, ensemble ll 0.0242 auc 0.9967\n",
      "running time 7.6561806201934814\n",
      "starting model 1 fold 1 target 4\n",
      "obj  0.06642388599502547\n",
      "obj  0.06636597735319236\n",
      "obj  0.0663229578876268\n",
      "obj  0.06641027740305569\n",
      "obj  0.06629545467855837\n",
      "obj  0.06628484651990561\n",
      "obj  0.06628767247183077\n",
      "obj  0.06627160659632952\n",
      "obj  0.06623834073989716\n",
      "obj  0.06621558023182642\n",
      "obj  0.06620284626213993\n",
      "obj  0.06619640612105301\n",
      "obj  0.06619382513861467\n",
      "obj  0.06619357996068168\n",
      "obj  0.06619340135691822\n",
      "obj  0.06619335525185813\n",
      "obj  0.06619334092075155\n",
      "v76 d1 f1 t4: original ll 0.0655 auc 0.9789, ensemble ll 0.0655 auc 0.9789\n",
      "running time 8.054045915603638\n",
      "starting model 1 fold 1 target 5\n",
      "obj  0.08076313741011973\n",
      "obj  0.08062941846458882\n",
      "obj  0.08049887310548355\n",
      "obj  0.08070836675032954\n",
      "obj  0.08042324893496652\n",
      "obj  0.0804439865725281\n",
      "obj  0.08044296463093735\n",
      "obj  0.08044245849370696\n",
      "obj  0.08034372480365877\n",
      "obj  0.08025765125679811\n",
      "obj  0.08023052362522515\n",
      "obj  0.08022605500542287\n",
      "obj  0.08022435266062354\n",
      "obj  0.0802236607343103\n",
      "obj  0.08022347940262957\n",
      "obj  0.08022347394744343\n",
      "v76 d1 f1 t5: original ll 0.0836 auc 0.9771, ensemble ll 0.0830 auc 0.9771\n",
      "running time 7.684167861938477\n",
      "starting model 2 fold 1 target 0\n",
      "obj  0.09885806895981615\n",
      "obj  0.09868784731004004\n",
      "obj  0.09848292639294276\n",
      "obj  0.09890289614491178\n",
      "obj  0.0984101288650192\n",
      "obj  0.09855299123700567\n",
      "obj  0.09848570576012894\n",
      "obj  0.09850640626384556\n",
      "obj  0.09823020099680918\n",
      "obj  0.09810091141310034\n",
      "obj  0.09806092988874877\n",
      "obj  0.09804389581241742\n",
      "obj  0.09803945531464174\n",
      "obj  0.09803862270036161\n",
      "obj  0.09803861978826675\n",
      "obj  0.0980386196124787\n",
      "obj  0.09803835974371222\n",
      "obj  0.09803831701874918\n",
      "obj  0.09803830755231642\n",
      "obj  0.09803830554238938\n",
      "obj  0.09803830506051654\n",
      "obj  0.09803830494670267\n",
      "v76 d2 f1 t0: original ll 0.1023 auc 0.9853, ensemble ll 0.1013 auc 0.9853\n",
      "running time 10.270868062973022\n",
      "starting model 2 fold 1 target 1\n",
      "obj  0.01764816679280705\n",
      "obj  0.017569157717274835\n",
      "obj  0.017254449879715875\n",
      "obj  0.01693470560175329\n",
      "obj  0.01698485841039333\n",
      "obj  0.0170863907302851\n",
      "obj  0.01708008741246613\n",
      "obj  0.01700099011245537\n",
      "obj  0.016710300504408186\n",
      "obj  0.016557322510831966\n",
      "obj  0.01650682853951219\n",
      "obj  0.016486908605052645\n",
      "obj  0.016481651567961917\n",
      "obj  0.016480487185609925\n",
      "obj  0.016480336752189743\n",
      "obj  0.016480300090537244\n",
      "obj  0.016480287377463758\n",
      "v76 d2 f1 t1: original ll 0.0154 auc 0.9541, ensemble ll 0.0139 auc 0.9541\n",
      "running time 8.067929983139038\n",
      "starting model 2 fold 1 target 2\n",
      "obj  0.04172438031331492\n",
      "obj  0.041699848119866094\n",
      "obj  0.04169061376026161\n",
      "obj  0.04170645155172006\n",
      "obj  0.04167150125565115\n",
      "obj  0.04166419574650107\n",
      "obj  0.0416632238178934\n",
      "obj  0.04166423196251549\n",
      "obj  0.04166105169953242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obj  0.04164196555199435\n",
      "obj  0.041633308602263434\n",
      "obj  0.04163322023104152\n",
      "obj  0.04163084026612783\n",
      "obj  0.041630789033000146\n",
      "obj  0.04163073923847467\n",
      "obj  0.041630725501591144\n",
      "v76 d2 f1 t2: original ll 0.0424 auc 0.9914, ensemble ll 0.0423 auc 0.9914\n",
      "running time 7.631128549575806\n",
      "starting model 2 fold 1 target 3\n",
      "obj  0.026332275648543103\n",
      "obj  0.02632141025714658\n",
      "obj  0.02633682035914422\n",
      "obj  0.02628172321537291\n",
      "obj  0.026326539627635006\n",
      "obj  0.026300155419501828\n",
      "obj  0.026302976363027263\n",
      "obj  0.026300564395781857\n",
      "obj  0.026271278755242003\n",
      "obj  0.02623853681671383\n",
      "obj  0.026232191388396775\n",
      "obj  0.026228131474380596\n",
      "obj  0.02622761044324288\n",
      "obj  0.026227348370534604\n",
      "obj  0.026227313152361605\n",
      "obj  0.026227311643180602\n",
      "obj  0.02622729718866243\n",
      "obj  0.026227297188650477\n",
      "obj  0.02622729669443555\n",
      "obj  0.02622729653250453\n",
      "v76 d2 f1 t3: original ll 0.0246 auc 0.9967, ensemble ll 0.0244 auc 0.9967\n",
      "running time 10.114450454711914\n",
      "starting model 2 fold 1 target 4\n",
      "obj  0.06610268842037552\n",
      "obj  0.06607503118305545\n",
      "obj  0.06598961322459883\n",
      "obj  0.06620950271133537\n",
      "obj  0.06596930982651067\n",
      "obj  0.0659779167821469\n",
      "obj  0.0659787187621191\n",
      "obj  0.06598324091609585\n",
      "obj  0.06595860066983635\n",
      "obj  0.0659196557531568\n",
      "obj  0.06591965204885926\n",
      "obj  0.06591420673729372\n",
      "obj  0.06591235295149525\n",
      "obj  0.06591233461150268\n",
      "obj  0.06591222177535774\n",
      "obj  0.06591220007813275\n",
      "obj  0.06591219362491861\n",
      "obj  0.06591219257808523\n",
      "v76 d2 f1 t4: original ll 0.0655 auc 0.9788, ensemble ll 0.0654 auc 0.9788\n",
      "running time 8.715568542480469\n",
      "starting model 2 fold 1 target 5\n",
      "obj  0.08106725367715861\n",
      "obj  0.08085406499117127\n",
      "obj  0.08068995696621704\n",
      "obj  0.08084989119970903\n",
      "obj  0.08061027236254842\n",
      "obj  0.08061626617005445\n",
      "obj  0.08061630445442873\n",
      "obj  0.08061213467445079\n",
      "obj  0.08053398266601215\n",
      "obj  0.08047312400022395\n",
      "obj  0.08045485526138287\n",
      "obj  0.08044967910769177\n",
      "obj  0.08044832124462592\n",
      "obj  0.08044815502470197\n",
      "obj  0.08044813768372101\n",
      "obj  0.0804481313158157\n",
      "v76 d2 f1 t5: original ll 0.0832 auc 0.9772, ensemble ll 0.0827 auc 0.9772\n",
      "running time 8.073350667953491\n",
      "starting model 3 fold 1 target 0\n",
      "obj  0.09785885988239593\n",
      "obj  0.0976942383953712\n",
      "obj  0.09744585008768482\n",
      "obj  0.0978415974111158\n",
      "obj  0.09734813467741157\n",
      "obj  0.09748898327901324\n",
      "obj  0.09741797030432729\n",
      "obj  0.09743859931428038\n",
      "obj  0.09716880415325145\n",
      "obj  0.09705057745298956\n",
      "obj  0.09700999491451179\n",
      "obj  0.0969938165084819\n",
      "obj  0.09698880380854703\n",
      "obj  0.09698767675197412\n",
      "obj  0.09698757037201938\n",
      "obj  0.09698757036325042\n",
      "obj  0.09698753360719632\n",
      "v76 d3 f1 t0: original ll 0.1004 auc 0.9861, ensemble ll 0.0993 auc 0.9861\n",
      "running time 8.097523212432861\n",
      "starting model 3 fold 1 target 1\n",
      "obj  0.016486850473642678\n",
      "obj  0.016443169269200853\n",
      "obj  0.01635275645453831\n",
      "obj  0.016252825209468746\n",
      "obj  0.016271435200644384\n",
      "obj  0.016277911768934904\n",
      "obj  0.0162827435483864\n",
      "obj  0.01626776865841488\n",
      "obj  0.01624623372108627\n",
      "obj  0.01617445984617662\n",
      "obj  0.016153708924555723\n",
      "obj  0.016150315521224732\n",
      "obj  0.01614962440835941\n",
      "obj  0.01614935356270037\n",
      "obj  0.01614934694597158\n",
      "obj  0.01614925717187973\n",
      "obj  0.016149256977365464\n",
      "obj  0.016149249289903862\n",
      "obj  0.016149247564272523\n",
      "obj  0.016149247126098318\n",
      "obj  0.016149247026935376\n",
      "v76 d3 f1 t1: original ll 0.0137 auc 0.9626, ensemble ll 0.0140 auc 0.9626\n",
      "running time 10.393662929534912\n",
      "starting model 3 fold 1 target 2\n",
      "obj  0.04136311298795242\n",
      "obj  0.04131256908999186\n",
      "obj  0.04131599060000555\n",
      "obj  0.0412713801188698\n",
      "obj  0.04130182563204821\n",
      "obj  0.04127641459764672\n",
      "obj  0.04127891174091\n",
      "obj  0.04127668883932573\n",
      "obj  0.04125381150459123\n",
      "obj  0.041234649390155774\n",
      "obj  0.041224106045653235\n",
      "obj  0.04122353424093834\n",
      "obj  0.04122244224855144\n",
      "obj  0.04122220527774062\n",
      "obj  0.04122216570041495\n",
      "obj  0.041222159216750584\n",
      "v76 d3 f1 t2: original ll 0.0423 auc 0.9917, ensemble ll 0.0423 auc 0.9917\n",
      "running time 9.428631782531738\n",
      "starting model 3 fold 1 target 3\n",
      "obj  0.026101492966878567\n",
      "obj  0.0260646396821562\n",
      "obj  0.026076838829822577\n",
      "obj  0.025995655511937887\n",
      "obj  0.0260668985401484\n",
      "obj  0.026032118397545212\n",
      "obj  0.02603633232562892\n",
      "obj  0.02603232144900381\n",
      "obj  0.025987035273262654\n",
      "obj  0.0259402928623387\n",
      "obj  0.0259218725765935\n",
      "obj  0.025919008543384935\n",
      "obj  0.025918166464336187\n",
      "obj  0.0259181656419957\n",
      "obj  0.025917970523482676\n",
      "obj  0.02591794779772179\n",
      "obj  0.025917942821084977\n",
      "v76 d3 f1 t3: original ll 0.0245 auc 0.9964, ensemble ll 0.0243 auc 0.9964\n",
      "running time 8.642770051956177\n",
      "starting model 3 fold 1 target 4\n",
      "obj  0.06526290526195332\n",
      "obj  0.06524620299239682\n",
      "obj  0.0652046376690387\n",
      "obj  0.06534954983696434\n",
      "obj  0.06518665645481178\n",
      "obj  0.0651871351497016\n",
      "obj  0.06518756247469487\n",
      "obj  0.06518988504197938\n",
      "obj  0.06517506766409562\n",
      "obj  0.06517498925562726\n",
      "obj  0.06514975201793051\n",
      "obj  0.06514970124238506\n",
      "obj  0.06514373581003399\n",
      "obj  0.06514071533170786\n",
      "obj  0.06514054395529562\n",
      "obj  0.06514030792157531\n",
      "obj  0.06514027950666856\n",
      "obj  0.06514026979588435\n",
      "v76 d3 f1 t4: original ll 0.0651 auc 0.9796, ensemble ll 0.0649 auc 0.9796\n",
      "running time 11.321305513381958\n",
      "starting model 3 fold 1 target 5\n",
      "obj  0.08016001314494317\n",
      "obj  0.08002863440858873\n",
      "obj  0.07989068888555476\n",
      "obj  0.08010349759771176\n",
      "obj  0.07981183559837528\n",
      "obj  0.07983426315382533\n",
      "obj  0.07983361000431553\n",
      "obj  0.07983168349991478\n",
      "obj  0.07972683734646815\n",
      "obj  0.07964228238048772\n",
      "obj  0.07961431543758238\n",
      "obj  0.07960997649764465\n",
      "obj  0.07960781460867335\n",
      "obj  0.07960762216090199\n",
      "obj  0.07960758568331049\n",
      "v76 d3 f1 t5: original ll 0.0813 auc 0.9780, ensemble ll 0.0808 auc 0.9780\n",
      "running time 8.433978796005249\n",
      "starting model 0 fold 2 target 0\n",
      "obj  0.10101079550314367\n",
      "obj  0.10088957453209065\n",
      "obj  0.10061105156370151\n",
      "obj  0.10112042025405826\n",
      "obj  0.10049436271598601\n",
      "obj  0.10068117603173816\n",
      "obj  0.10059122925552612\n",
      "obj  0.10061897946029255\n",
      "obj  0.10020257761281584\n",
      "obj  0.10005534849070412\n",
      "obj  0.09997944458555383\n",
      "obj  0.09995652867773214\n",
      "obj  0.09995599733797676\n",
      "obj  0.09994986683971668\n",
      "obj  0.09994968098656018\n",
      "obj  0.0999496664578158\n",
      "obj  0.09994866967169855\n",
      "obj  0.09994853039258748\n",
      "obj  0.09994849047427451\n",
      "obj  0.09994848180395523\n",
      "obj  0.09994847979263596\n",
      "obj  0.0999484793748581\n",
      "v76 d0 f2 t0: original ll 0.0967 auc 0.9874, ensemble ll 0.0958 auc 0.9874\n",
      "running time 11.340031623840332\n",
      "starting model 0 fold 2 target 1\n",
      "obj  0.01708799894987406\n",
      "obj  0.017052157390948845\n",
      "obj  0.01695132103466231\n",
      "obj  0.01671000423882432\n",
      "obj  0.01651155611276567\n",
      "obj  0.01622369192244509\n",
      "obj  0.016330422025852883\n",
      "obj  0.016183467327808017\n",
      "obj  0.015908486224797493\n",
      "obj  0.015745553517473677\n",
      "obj  0.01566379386925386\n",
      "obj  0.015616354590201581\n",
      "obj  0.01561574332118367\n",
      "obj  0.015604458739587029\n",
      "obj  0.015603135784550198\n",
      "obj  0.015602708932636421\n",
      "obj  0.015602652368799845\n",
      "obj  0.015602638053998323\n",
      "obj  0.015602632068732334\n",
      "v76 d0 f2 t1: original ll 0.0180 auc 0.9592, ensemble ll 0.0169 auc 0.9592\n",
      "running time 9.94560718536377\n",
      "starting model 0 fold 2 target 2\n",
      "obj  0.043759365835025085\n",
      "obj  0.04367315235527235\n",
      "obj  0.04361674089188761\n",
      "obj  0.04362901764928989\n",
      "obj  0.04358066036183642\n",
      "obj  0.043568260374125256\n",
      "obj  0.04356932496386384\n",
      "obj  0.04357065496897056\n",
      "obj  0.04353877795105422\n",
      "obj  0.04351812959899519\n",
      "obj  0.04350725938854459\n",
      "obj  0.04350085113599935\n",
      "obj  0.043499618832101974\n",
      "obj  0.043497929464236154\n",
      "obj  0.04349792755759952\n",
      "obj  0.04349780593419177\n",
      "obj  0.043497768618567655\n",
      "obj  0.04349775521301859\n",
      "obj  0.04349775521256763\n",
      "obj  0.0434977533741274\n",
      "v76 d0 f2 t2: original ll 0.0391 auc 0.9922, ensemble ll 0.0393 auc 0.9922\n",
      "running time 10.337557792663574\n",
      "starting model 0 fold 2 target 3\n",
      "obj  0.02584247201761601\n",
      "obj  0.02584583981274754\n",
      "obj  0.02588577202382662\n",
      "obj  0.02574030409602686\n",
      "obj  0.0258623294882222\n",
      "obj  0.025822794692289104\n",
      "obj  0.025830047685123522\n",
      "obj  0.025825607558346404\n",
      "obj  0.025770285559461505\n",
      "obj  0.025706353780882204\n",
      "obj  0.02568163942327803\n",
      "obj  0.025679155741905428\n",
      "obj  0.025677987209706123\n",
      "obj  0.02567797494863856\n",
      "obj  0.025677665747963287\n",
      "obj  0.025677576930892867\n",
      "obj  0.025677564011845143\n",
      "obj  0.02567756263999901\n",
      "obj  0.025677562220404713\n",
      "v76 d0 f2 t3: original ll 0.0251 auc 0.9965, ensemble ll 0.0250 auc 0.9965\n",
      "running time 9.752910852432251\n",
      "starting model 0 fold 2 target 4\n",
      "obj  0.06649118969388688\n",
      "obj  0.06642604667735208\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obj  0.06641534140119051\n",
      "obj  0.06649403672625516\n",
      "obj  0.06639016763108413\n",
      "obj  0.06636460118171864\n",
      "obj  0.06636573210870925\n",
      "obj  0.06635222879844709\n",
      "obj  0.06633331299239933\n",
      "obj  0.06632599113684723\n",
      "obj  0.06630839239821341\n",
      "obj  0.06630439396793016\n",
      "obj  0.06630196927273457\n",
      "obj  0.06630156123184806\n",
      "obj  0.06630102806635572\n",
      "obj  0.06630097427285388\n",
      "obj  0.0663009648128465\n",
      "obj  0.06630096223227523\n",
      "v76 d0 f2 t4: original ll 0.0653 auc 0.9813, ensemble ll 0.0652 auc 0.9813\n",
      "running time 9.342431545257568\n",
      "starting model 0 fold 2 target 5\n",
      "obj  0.0838054893217125\n",
      "obj  0.08367130919681294\n",
      "obj  0.08351203793477135\n",
      "obj  0.08377102642641442\n",
      "obj  0.0834349402453839\n",
      "obj  0.08347039838479013\n",
      "obj  0.08346927288995916\n",
      "obj  0.08346456211479943\n",
      "obj  0.08332543998638148\n",
      "obj  0.08322267306878238\n",
      "obj  0.08318626245334458\n",
      "obj  0.08317805696424786\n",
      "obj  0.08317606749579778\n",
      "obj  0.08317542497177427\n",
      "obj  0.08317542243295956\n",
      "obj  0.08317525877906967\n",
      "obj  0.08317523652193703\n",
      "obj  0.0831752318441997\n",
      "obj  0.08317523070951273\n",
      "obj  0.08317523043953569\n",
      "v76 d0 f2 t5: original ll 0.0784 auc 0.9800, ensemble ll 0.0778 auc 0.9800\n",
      "running time 10.275765419006348\n",
      "starting model 1 fold 2 target 0\n",
      "obj  0.10131022597410808\n",
      "obj  0.10119971411492508\n",
      "obj  0.10092382847459641\n",
      "obj  0.1016170553520021\n",
      "obj  0.10086361713975549\n",
      "obj  0.10110622553246547\n",
      "obj  0.10101568232670123\n",
      "obj  0.1010512931645364\n",
      "obj  0.10053297073455247\n",
      "obj  0.10028720735441383\n",
      "obj  0.10021338680509592\n",
      "obj  0.10019462619084754\n",
      "obj  0.10019015389042948\n",
      "obj  0.10018948282892842\n",
      "obj  0.10018908259683192\n",
      "obj  0.1001890825698623\n",
      "obj  0.10018903113698652\n",
      "obj  0.10018901366123925\n",
      "v76 d1 f2 t0: original ll 0.0982 auc 0.9868, ensemble ll 0.0977 auc 0.9868\n",
      "running time 9.418562889099121\n",
      "starting model 1 fold 2 target 1\n",
      "obj  0.01658377124954033\n",
      "obj  0.016527989953833498\n",
      "obj  0.01627715694847644\n",
      "obj  0.01584722693639505\n",
      "obj  0.015977279305331396\n",
      "obj  0.01601340029280964\n",
      "obj  0.01601567901203088\n",
      "obj  0.0159376169006515\n",
      "obj  0.01572972205349083\n",
      "obj  0.01560205904695709\n",
      "obj  0.015507019981460443\n",
      "obj  0.015489229821900416\n",
      "obj  0.015482567199029552\n",
      "obj  0.015482486664741951\n",
      "obj  0.015480127876471782\n",
      "obj  0.015479946674910561\n",
      "obj  0.015479875755987497\n",
      "v76 d1 f2 t1: original ll 0.0174 auc 0.9603, ensemble ll 0.0161 auc 0.9603\n",
      "running time 9.101394653320312\n",
      "starting model 1 fold 2 target 2\n",
      "obj  0.04343479820118352\n",
      "obj  0.043377958608166164\n",
      "obj  0.04335997471916264\n",
      "obj  0.043362895376565666\n",
      "obj  0.04332918351431276\n",
      "obj  0.04331237637687261\n",
      "obj  0.043311632879876896\n",
      "obj  0.04331255048098178\n",
      "obj  0.04330507521364576\n",
      "obj  0.04328026211110955\n",
      "obj  0.04327302758392121\n",
      "obj  0.04326934971709733\n",
      "obj  0.043269178802774935\n",
      "obj  0.043269125739998894\n",
      "obj  0.0432691257392563\n",
      "obj  0.043269119179710526\n",
      "obj  0.043269117896332164\n",
      "v76 d1 f2 t2: original ll 0.0390 auc 0.9927, ensemble ll 0.0391 auc 0.9927\n",
      "running time 9.106258630752563\n",
      "starting model 1 fold 2 target 3\n",
      "obj  0.025534108170620344\n",
      "obj  0.025526046212399\n",
      "obj  0.025563431009173333\n",
      "obj  0.0254548786151275\n",
      "obj  0.025560840990814463\n",
      "obj  0.025517931573922806\n",
      "obj  0.025523702532585377\n",
      "obj  0.025517901024762608\n",
      "obj  0.025446818383309824\n",
      "obj  0.02538459490088787\n",
      "obj  0.025359687397408397\n",
      "obj  0.025358325516498477\n",
      "obj  0.025357382738952366\n",
      "obj  0.025357381249592345\n",
      "obj  0.025357188349876145\n",
      "obj  0.02535715438717161\n",
      "v76 d1 f2 t3: original ll 0.0255 auc 0.9965, ensemble ll 0.0253 auc 0.9965\n",
      "running time 7.535891056060791\n",
      "starting model 1 fold 2 target 4\n",
      "obj  0.06637251541389882\n",
      "obj  0.06635282315704456\n",
      "obj  0.06630294690410023\n",
      "obj  0.06645787937395324\n",
      "obj  0.06628800800589311\n",
      "obj  0.06629107193380887\n",
      "obj  0.0662917591636322\n",
      "obj  0.06629441532663907\n",
      "obj  0.06626828738854999\n",
      "obj  0.06624784715275261\n",
      "obj  0.06623857879738287\n",
      "obj  0.06623529163825273\n",
      "obj  0.06623374760720527\n",
      "obj  0.06623362179782293\n",
      "obj  0.06623302520707086\n",
      "obj  0.06623302126708072\n",
      "obj  0.06623296889752704\n",
      "obj  0.06623295852312838\n",
      "obj  0.06623295593184493\n",
      "v76 d1 f2 t4: original ll 0.0656 auc 0.9807, ensemble ll 0.0654 auc 0.9807\n",
      "running time 8.518043518066406\n",
      "starting model 1 fold 2 target 5\n",
      "obj  0.08378640158545253\n",
      "obj  0.08366517025946933\n",
      "obj  0.08348994332908517\n",
      "obj  0.0842515697721505\n",
      "obj  0.08347148696222696\n",
      "obj  0.0835107768730284\n",
      "obj  0.08350698335146577\n",
      "obj  0.08346353883841293\n",
      "obj  0.08334045517916507\n",
      "obj  0.08310290527111992\n",
      "obj  0.08303112055464701\n",
      "obj  0.08301107938402555\n",
      "obj  0.08300713139026138\n",
      "obj  0.08300710577939036\n",
      "obj  0.08300631539793647\n",
      "obj  0.08300624324684194\n",
      "v76 d1 f2 t5: original ll 0.0776 auc 0.9803, ensemble ll 0.0774 auc 0.9803\n",
      "running time 7.437007188796997\n",
      "starting model 2 fold 2 target 0\n",
      "obj  0.10132774728508945\n",
      "obj  0.10123918010530207\n",
      "obj  0.1010082553530008\n",
      "obj  0.10155401854701958\n",
      "obj  0.10094179216666845\n",
      "obj  0.10113056179238392\n",
      "obj  0.10105731860254155\n",
      "obj  0.10108442922046663\n",
      "obj  0.1007092614512355\n",
      "obj  0.10052202679906436\n",
      "obj  0.10047244289549195\n",
      "obj  0.10045523330658138\n",
      "obj  0.1004517412738207\n",
      "obj  0.10045108841779081\n",
      "obj  0.10045095753572691\n",
      "obj  0.10045095747450367\n",
      "obj  0.10045093031418724\n",
      "v76 d2 f2 t0: original ll 0.0973 auc 0.9873, ensemble ll 0.0966 auc 0.9873\n",
      "running time 7.908686637878418\n",
      "starting model 2 fold 2 target 1\n",
      "obj  0.01627033074054602\n",
      "obj  0.01621033211099174\n",
      "obj  0.01606682070737097\n",
      "obj  0.015548215035625697\n",
      "obj  0.015563041704441208\n",
      "obj  0.01560709300629199\n",
      "obj  0.01558038549731692\n",
      "obj  0.01547315350397737\n",
      "obj  0.015303254766505955\n",
      "obj  0.015112368829223986\n",
      "obj  0.01502955694651347\n",
      "obj  0.01497678632980188\n",
      "obj  0.014976378964455724\n",
      "obj  0.014966622392676767\n",
      "obj  0.014965137112242457\n",
      "obj  0.014964575868454014\n",
      "obj  0.014964512437994472\n",
      "v76 d2 f2 t1: original ll 0.0182 auc 0.9615, ensemble ll 0.0166 auc 0.9615\n",
      "running time 8.063029050827026\n",
      "starting model 2 fold 2 target 2\n",
      "obj  0.043585568079368746\n",
      "obj  0.04354487509165994\n",
      "obj  0.043519487169370506\n",
      "obj  0.04355111047508941\n",
      "obj  0.043486101243889824\n",
      "obj  0.04347876126962287\n",
      "obj  0.0434765553436048\n",
      "obj  0.04347893002373234\n",
      "obj  0.04347641415007566\n",
      "obj  0.043451790361519055\n",
      "obj  0.04344959057723766\n",
      "obj  0.04344121997138845\n",
      "obj  0.043438567972930824\n",
      "obj  0.04343847293587762\n",
      "obj  0.04343836505286163\n",
      "obj  0.043438340118030556\n",
      "obj  0.04343833510655174\n",
      "obj  0.04343833505234133\n",
      "obj  0.043438333983608524\n",
      "v76 d2 f2 t2: original ll 0.0387 auc 0.9927, ensemble ll 0.0387 auc 0.9927\n",
      "running time 10.782039165496826\n",
      "starting model 2 fold 2 target 3\n",
      "obj  0.02579358044700845\n",
      "obj  0.025784292391699392\n",
      "obj  0.025818925185718024\n",
      "obj  0.025716402689048175\n",
      "obj  0.025814759262724477\n",
      "obj  0.025773095630898864\n",
      "obj  0.02577852683465122\n",
      "obj  0.02577314215376959\n",
      "obj  0.02570932738956375\n",
      "obj  0.025650501005595835\n",
      "obj  0.025626681475269905\n",
      "obj  0.025624850351820067\n",
      "obj  0.025623795005785825\n",
      "obj  0.02562378374490418\n",
      "obj  0.025623643151151464\n",
      "obj  0.025623533186049877\n",
      "obj  0.025623527499167596\n",
      "obj  0.025623526456330592\n",
      "v76 d2 f2 t3: original ll 0.0257 auc 0.9965, ensemble ll 0.0256 auc 0.9965\n",
      "running time 9.63846206665039\n",
      "starting model 2 fold 2 target 4\n",
      "obj  0.06647600614280591\n",
      "obj  0.06646781419149421\n",
      "obj  0.06640361860730498\n",
      "obj  0.06656382069542523\n",
      "obj  0.0663779266850448\n",
      "obj  0.06637856399369228\n",
      "obj  0.06637946938396748\n",
      "obj  0.06638231091735236\n",
      "obj  0.06636980977327511\n",
      "obj  0.06634608371416761\n",
      "obj  0.06634608099226366\n",
      "obj  0.06634383408996057\n",
      "obj  0.06634269822106037\n",
      "obj  0.06634265428963079\n",
      "obj  0.06634242945447029\n",
      "obj  0.06634241793721658\n",
      "obj  0.06634241793678729\n",
      "obj  0.06634241425456022\n",
      "obj  0.06634241383808051\n",
      "v76 d2 f2 t4: original ll 0.0647 auc 0.9817, ensemble ll 0.0645 auc 0.9817\n",
      "running time 10.614097118377686\n",
      "starting model 2 fold 2 target 5\n",
      "obj  0.08359807135806538\n",
      "obj  0.08346892693190024\n",
      "obj  0.08332152457718474\n",
      "obj  0.08358261285629554\n",
      "obj  0.08324918990652838\n",
      "obj  0.08328562935566504\n",
      "obj  0.08327416348164024\n",
      "obj  0.08327803756983883\n",
      "obj  0.08314238245674738\n",
      "obj  0.0830437700136554\n",
      "obj  0.08300691648309831\n",
      "obj  0.08299914137150054\n",
      "obj  0.08299722359853132\n",
      "obj  0.08299648811307998\n",
      "obj  0.08299635259500796\n",
      "obj  0.08299635258169093\n",
      "obj  0.08299631690758058\n",
      "obj  0.08299631174829375\n",
      "obj  0.08299631076155414\n",
      "v76 d2 f2 t5: original ll 0.0781 auc 0.9803, ensemble ll 0.0777 auc 0.9803\n",
      "running time 10.43120288848877\n",
      "starting model 3 fold 2 target 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obj  0.10005711843715578\n",
      "obj  0.09993817452176108\n",
      "obj  0.09969992069084524\n",
      "obj  0.10024465572849456\n",
      "obj  0.09962187709187256\n",
      "obj  0.0998124628651744\n",
      "obj  0.09972860007544063\n",
      "obj  0.0997563527079437\n",
      "obj  0.09934467983926538\n",
      "obj  0.09915630029569884\n",
      "obj  0.0991061100188211\n",
      "obj  0.09909004362649247\n",
      "obj  0.09908660159601775\n",
      "obj  0.09908659362542352\n",
      "obj  0.09908563830824471\n",
      "obj  0.09908551044539116\n",
      "v76 d3 f2 t0: original ll 0.0960 auc 0.9878, ensemble ll 0.0952 auc 0.9878\n",
      "running time 8.956891298294067\n",
      "starting model 3 fold 2 target 1\n",
      "obj  0.015472664408318435\n",
      "obj  0.015463880235014812\n",
      "obj  0.015479735878037469\n",
      "obj  0.015425427850545003\n",
      "obj  0.015485381053848547\n",
      "obj  0.01546363287119493\n",
      "obj  0.015466031764365182\n",
      "obj  0.015460598797395635\n",
      "obj  0.015422987045677737\n",
      "obj  0.015382854969788029\n",
      "obj  0.015370331211060802\n",
      "obj  0.015368652242465712\n",
      "obj  0.01536792857244066\n",
      "obj  0.015367796551813194\n",
      "obj  0.015367767873357604\n",
      "obj  0.015367764765370584\n",
      "v76 d3 f2 t1: original ll 0.0157 auc 0.9657, ensemble ll 0.0155 auc 0.9657\n",
      "running time 9.249372482299805\n",
      "starting model 3 fold 2 target 2\n",
      "obj  0.04318387032862903\n",
      "obj  0.043128091770873685\n",
      "obj  0.043122364218768465\n",
      "obj  0.04307896240978556\n",
      "obj  0.043097832460296186\n",
      "obj  0.04306909731222942\n",
      "obj  0.04307214387163429\n",
      "obj  0.04307035116707285\n",
      "obj  0.04304727783523054\n",
      "obj  0.0430309840079386\n",
      "obj  0.043022293182687\n",
      "obj  0.04301715245142698\n",
      "obj  0.04301657841822036\n",
      "obj  0.04301646882469594\n",
      "obj  0.043016449374235224\n",
      "obj  0.04301644936444788\n",
      "obj  0.04301644592900252\n",
      "obj  0.04301644559267541\n",
      "v76 d3 f2 t2: original ll 0.0386 auc 0.9928, ensemble ll 0.0387 auc 0.9928\n",
      "running time 10.504327297210693\n",
      "starting model 3 fold 2 target 3\n",
      "obj  0.025730156858482924\n",
      "obj  0.025711068038467587\n",
      "obj  0.02574996646283465\n",
      "obj  0.025638979703989682\n",
      "obj  0.02575876666121864\n",
      "obj  0.025718702739587856\n",
      "obj  0.02572402933429702\n",
      "obj  0.02571763491845777\n",
      "obj  0.025630817019899898\n",
      "obj  0.025542745376919897\n",
      "obj  0.025533297930253628\n",
      "obj  0.02553300370388607\n",
      "obj  0.02553262566720473\n",
      "obj  0.025532539742409016\n",
      "obj  0.02553252123747117\n",
      "v76 d3 f2 t3: original ll 0.0252 auc 0.9966, ensemble ll 0.0251 auc 0.9966\n",
      "running time 8.709758758544922\n",
      "starting model 3 fold 2 target 4\n",
      "obj  0.06557067425113065\n",
      "obj  0.06554915310579304\n",
      "obj  0.06551782732310159\n",
      "obj  0.06561329477991737\n",
      "obj  0.06549380497199561\n",
      "obj  0.06548680873987019\n",
      "obj  0.06548820538584028\n",
      "obj  0.06548949875582652\n",
      "obj  0.06546456535452591\n",
      "obj  0.06546447927477733\n",
      "obj  0.06544371134287898\n",
      "obj  0.06544369102957932\n",
      "obj  0.06544044819966618\n",
      "obj  0.06543893632254791\n",
      "obj  0.06543893258495746\n",
      "obj  0.06543884034618598\n",
      "obj  0.0654388329452799\n",
      "obj  0.06543883060063005\n",
      "obj  0.06543882985238425\n",
      "v76 d3 f2 t4: original ll 0.0644 auc 0.9817, ensemble ll 0.0643 auc 0.9817\n",
      "running time 9.869332075119019\n",
      "starting model 3 fold 2 target 5\n",
      "obj  0.08226311457569598\n",
      "obj  0.08218998872581826\n",
      "obj  0.08206266062033578\n",
      "obj  0.08234823720889337\n",
      "obj  0.08199938055055646\n",
      "obj  0.08204691146251854\n",
      "obj  0.08203540413859318\n",
      "obj  0.08203766717898903\n",
      "obj  0.0818805881534236\n",
      "obj  0.08176616222142062\n",
      "obj  0.08172448505531273\n",
      "obj  0.08171400624258743\n",
      "obj  0.08171216728184173\n",
      "obj  0.08171215908937457\n",
      "obj  0.08171178368121276\n",
      "obj  0.08171178266623791\n",
      "obj  0.08171168674850532\n",
      "v76 d3 f2 t5: original ll 0.0771 auc 0.9810, ensemble ll 0.0766 auc 0.9810\n",
      "running time 8.894490718841553\n",
      "total running time 647.1533951759338\n"
     ]
    }
   ],
   "source": [
    "stg = time.time()\n",
    "for fold in range(3):\n",
    "    for ds_idx in range(4):\n",
    "        for target in range(6):\n",
    "            train_ensemble(ids_df, preds_all, fold=fold, target=target, ds_idx=ds_idx, first_step=True)\n",
    "print('total running time', time.time() - stg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pd.read_csv(PATH_WORK/'ensemble'/'stats.v{}'.format(VERSION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train original ll 0.06155 ensemble ll 0.06097\n",
      "valid original ll 0.06155 ensemble ll 0.06107\n"
     ]
    }
   ],
   "source": [
    "agg = stats.loc[stats.ds_idx != -1].groupby('target').mean().sort_index()\n",
    "print('train original ll {:.5f} ensemble ll {:.5f}'\n",
    "      .format((agg.train_loss * class_weights).mean(), (agg.train_loss_ens * class_weights).mean()))\n",
    "print('valid original ll {:.5f} ensemble ll {:.5f}'\n",
    "      .format((agg.valid_loss * class_weights).mean(), (agg.valid_loss_ens * class_weights).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting fold 0 target 0\n",
      "obj  0.09507411612845341\n",
      "obj  0.09535349852500613\n",
      "obj  0.09534116005137871\n",
      "obj  0.09533101174661146\n",
      "obj  0.09532414579988734\n",
      "obj  0.09530798476711709\n",
      "obj  0.09529845005514835\n",
      "obj  0.09528406552931658\n",
      "obj  0.09526236171210575\n",
      "obj  0.09500636154394032\n",
      "obj  0.09496434082558557\n",
      "obj  0.09495164822971938\n",
      "obj  0.09493918088457484\n",
      "obj  0.09492748278327184\n",
      "obj  0.09491764088847596\n",
      "obj  0.09490913444663043\n",
      "obj  0.09490162513672512\n",
      "obj  0.09489334969809254\n",
      "obj  0.09488440576861024\n",
      "obj  0.09487404537493532\n",
      "obj  0.09486315711494987\n",
      "obj  0.09485388312610873\n",
      "obj  0.09484116485942563\n",
      "obj  0.09481774540142596\n",
      "obj  0.09478289957525984\n",
      "obj  0.0947673430784487\n",
      "obj  0.09476612285851009\n",
      "obj  0.09476608966403882\n",
      "obj  0.09476608862702353\n",
      "obj  0.09476608855067337\n",
      "obj  0.09476608855064224\n",
      "obj  0.0947660885506384\n",
      "v76 d-1 f0 t0: original ll 0.0964 auc 0.9871, ensemble ll 0.0963 auc 0.9870\n",
      "running time 11.978947639465332\n",
      "starting fold 0 target 1\n",
      "obj  0.014064049695468308\n",
      "obj  0.01406148763594048\n",
      "obj  0.014064756983048486\n",
      "obj  0.01406253076701888\n",
      "obj  0.014083250565751958\n",
      "obj  0.01406110505712373\n",
      "obj  0.014056482283600957\n",
      "obj  0.014060215541995566\n",
      "obj  0.014012163410645812\n",
      "obj  0.014007130639327516\n",
      "obj  0.014005098919550622\n",
      "obj  0.014004752194298954\n",
      "obj  0.014004648871427572\n",
      "obj  0.014004622606976687\n",
      "v76 d-1 f0 t1: original ll 0.0161 auc 0.9783, ensemble ll 0.0161 auc 0.9780\n",
      "running time 5.770450592041016\n",
      "starting fold 0 target 2\n",
      "obj  0.03889652086088436\n",
      "obj  0.03892843945890641\n",
      "obj  0.03892743233569515\n",
      "obj  0.03892486339057395\n",
      "obj  0.03892315143245734\n",
      "obj  0.03892106471482662\n",
      "obj  0.03891953895289948\n",
      "obj  0.03891826203531413\n",
      "obj  0.03892018626894306\n",
      "obj  0.0389006030779292\n",
      "obj  0.038889733531149384\n",
      "obj  0.03888413500646272\n",
      "obj  0.038881183885517104\n",
      "obj  0.03887955206059472\n",
      "obj  0.038878557709816776\n",
      "obj  0.038877851713713836\n",
      "obj  0.038875715376819144\n",
      "obj  0.038874121841688196\n",
      "obj  0.038872867729066875\n",
      "obj  0.03887180134601449\n",
      "obj  0.03887118606935464\n",
      "obj  0.03886877548625587\n",
      "obj  0.03886714601491705\n",
      "obj  0.038865959941978015\n",
      "obj  0.03886498629039062\n",
      "obj  0.03886415450336465\n",
      "obj  0.03886343312888259\n",
      "obj  0.03886280315833738\n",
      "obj  0.03886225143901574\n",
      "obj  0.038861767442316494\n",
      "obj  0.038860140889335086\n",
      "obj  0.03885862611636351\n",
      "obj  0.03885754087731322\n",
      "obj  0.03885669931969414\n",
      "obj  0.038856019688688126\n",
      "obj  0.038855460919413776\n",
      "obj  0.03885499823602374\n",
      "obj  0.038854614139215995\n",
      "obj  0.03885429754294153\n",
      "obj  0.03885403987807834\n",
      "obj  0.03885383305605144\n",
      "obj  0.038853673005665286\n",
      "obj  0.03885355583147137\n",
      "obj  0.03885296595773327\n",
      "obj  0.03885149781984181\n",
      "obj  0.038850516577437995\n",
      "obj  0.03884984259416912\n",
      "obj  0.03884935899269223\n",
      "obj  0.03884900890068989\n",
      "obj  0.03884875891587197\n",
      "obj  0.03884858767454427\n",
      "obj  0.03884848040753678\n",
      "obj  0.03884842677542066\n",
      "obj  0.03884842033078383\n",
      "obj  0.03884845481944835\n",
      "obj  0.038848526000204636\n",
      "obj  0.038848526000204636\n",
      "obj  0.038848526000204636\n",
      "obj  0.038848526000204636\n",
      "obj  0.038848526000204636\n",
      "obj  0.038848526000204636\n",
      "obj  0.038848526000204636\n",
      "obj  0.038848526000204636\n",
      "obj  0.038848526000204636\n",
      "obj  0.038848526000204636\n",
      "obj  0.038848526000204636\n",
      "obj  0.038848526000204636\n",
      "obj  0.038848526000204636\n",
      "obj  0.038848526000204636\n",
      "obj  0.03884849558775338\n",
      "obj  0.03884849558775338\n",
      "obj  0.03884847359553925\n",
      "obj  0.03884847359553925\n",
      "obj  0.038848463806617085\n",
      "obj  0.03884845920742896\n",
      "obj  0.038848459207428965\n",
      "obj  0.038848456987548775\n",
      "obj  0.03884845921108499\n",
      "obj  0.03884845921108499\n",
      "obj  0.03884845809219485\n",
      "obj  0.03884845809219486\n",
      "obj  0.03884845753833862\n",
      "obj  0.03884845753833862\n",
      "obj  0.03884845726256028\n",
      "obj  0.03884845726256029\n",
      "obj  0.03884845712495866\n",
      "obj  0.03884845712495865\n",
      "obj  0.03884845705622974\n",
      "obj  0.03884845705622974\n",
      "obj  0.03884845702188326\n",
      "obj  0.038848457021883276\n",
      "obj  0.038848457004714544\n",
      "obj  0.038848457004714544\n",
      "obj  0.03884845699613128\n",
      "obj  0.03884845699613128\n",
      "obj  0.03884845699183993\n",
      "obj  0.03884845699183994\n",
      "obj  0.038848456989694344\n",
      "obj  0.038848456989694344\n",
      "obj  0.03884845698862155\n",
      "obj  0.03884845698808517\n",
      "obj  0.03884845698808517\n",
      "obj  0.03884845698781697\n",
      "obj  0.03884845698781697\n",
      "obj  0.038848456987682876\n",
      "obj  0.038848456987682876\n",
      "obj  0.03884845698761582\n",
      "obj  0.03884845698761582\n",
      "obj  0.038848456987582304\n",
      "obj  0.038848456987582304\n",
      "obj  0.038848456987548775\n",
      "obj  0.03884852883205395\n",
      "obj  0.038848528832053954\n",
      "obj  0.038848523888646555\n",
      "obj  0.038848523888646555\n",
      "obj  0.038848486886320734\n",
      "obj  0.038848486886320734\n",
      "obj  0.03884847108045156\n",
      "obj  0.03884847108045156\n",
      "obj  0.03884846382175841\n",
      "obj  0.03884846382175841\n",
      "obj  0.038848460348968133\n",
      "obj  0.038848460348968133\n",
      "obj  0.038848458652151356\n",
      "obj  0.038848458652151356\n",
      "obj  0.03884845781656061\n",
      "obj  0.03884845781656061\n",
      "obj  0.03884845740123192\n",
      "obj  0.03884845740123192\n",
      "obj  0.03884845719418459\n",
      "obj  0.03884845719418459\n",
      "obj  0.03884845709081523\n",
      "obj  0.03884845709081523\n",
      "obj  0.038848457039169164\n",
      "obj  0.03884845703916916\n",
      "obj  0.03884845701335574\n",
      "obj  0.03884845701335574\n",
      "obj  0.03884845700045147\n",
      "obj  0.03884845699399993\n",
      "obj  0.03884845699399993\n",
      "obj  0.03884845699077429\n",
      "obj  0.03884845699077429\n",
      "obj  0.038848456989161534\n",
      "obj  0.03884845698916152\n",
      "obj  0.03884845698835517\n",
      "obj  0.03884845698835516\n",
      "obj  0.038848456987951974\n",
      "obj  0.038848456987750364\n",
      "obj  0.038848456987750364\n",
      "obj  0.03884845698764958\n",
      "obj  0.03884845698764958\n",
      "obj  0.03884845698759917\n",
      "obj  0.03884845698759918\n",
      "obj  0.038848456987548775\n",
      "obj  0.038848528708560745\n",
      "obj  0.038848528708560745\n",
      "obj  0.03884852309972018\n",
      "obj  0.03884848658576378\n",
      "obj  0.03884848658576378\n",
      "obj  0.03884847095283429\n",
      "obj  0.03884847095283429\n",
      "obj  0.038848463763378624\n",
      "obj  0.038848463763378624\n",
      "obj  0.03884846032114318\n",
      "obj  0.03884846032114318\n",
      "obj  0.03884845863865981\n",
      "obj  0.03884845863865981\n",
      "obj  0.038848457809900504\n",
      "obj  0.0388484578099005\n",
      "obj  0.03884845739792329\n",
      "obj  0.03884845739792329\n",
      "obj  0.03884845719253564\n",
      "obj  0.03884845719253565\n",
      "obj  0.03884845708999211\n",
      "obj  0.03884845708999211\n",
      "obj  0.0388484570387579\n",
      "obj  0.0388484570387579\n",
      "obj  0.03884845701315022\n",
      "obj  0.03884845701315022\n",
      "obj  0.03884845700034872\n",
      "obj  0.03884845700034872\n",
      "obj  0.038848456993948545\n",
      "obj  0.038848456993948545\n",
      "obj  0.03884845699074862\n",
      "obj  0.03884845699074862\n",
      "obj  0.03884845698914869\n",
      "obj  0.03884845698914869\n",
      "obj  0.038848456988348726\n",
      "obj  0.03884845698794875\n",
      "obj  0.03884845698794875\n",
      "obj  0.03884845698774876\n",
      "obj  0.03884845698774877\n",
      "obj  0.03884845698764877\n",
      "obj  0.03884845698759876\n",
      "obj  0.03884845698759877\n",
      "obj  0.038848456987548775\n",
      "obj  0.03884852868399831\n",
      "obj  0.03884852868399831\n",
      "obj  0.03884852294346699\n",
      "obj  0.03884852294346699\n",
      "obj  0.03884848652614837\n",
      "obj  0.03884848652614837\n",
      "obj  0.03884847092749121\n",
      "obj  0.03884847092749121\n",
      "obj  0.03884846375177611\n",
      "obj  0.03884846375177611\n",
      "obj  0.03884846031561069\n",
      "obj  0.03884846031561069\n",
      "obj  0.038848458635976565\n",
      "obj  0.038848458635976565\n",
      "obj  0.038848457808575765\n",
      "obj  0.03884845780857577\n",
      "obj  0.03884845739726515\n",
      "obj  0.038848457397265126\n",
      "obj  0.03884845719220763\n",
      "obj  0.03884845719220763\n",
      "obj  0.03884845708982835\n",
      "obj  0.038848457089828356\n",
      "obj  0.03884845703867613\n",
      "obj  0.03884845703867613\n",
      "obj  0.038848457013109336\n",
      "obj  0.038848457013109336\n",
      "obj  0.038848457000328275\n",
      "obj  0.038848457000328275\n",
      "obj  0.038848456993938324\n",
      "obj  0.038848456993938324\n",
      "obj  0.03884845699074351\n",
      "obj  0.0388484569907435\n",
      "obj  0.038848456989146136\n",
      "obj  0.03884845698834745\n",
      "obj  0.038848456988347456\n",
      "obj  0.038848456987948116\n",
      "obj  0.03884845698794811\n",
      "obj  0.038848456987748435\n",
      "obj  0.038848456987748435\n",
      "obj  0.03884845698764862\n",
      "obj  0.03884845698764862\n",
      "obj  0.0388484569875987\n",
      "obj  0.0388484569875987\n",
      "obj  0.038848456987548775\n",
      "obj  0.03884852867909128\n",
      "obj  0.038848528679091276\n",
      "obj  0.03884852291227709\n",
      "obj  0.03884852291227709\n",
      "obj  0.03884848651424496\n",
      "obj  0.038848470922429774\n",
      "obj  0.03884847092242977\n",
      "obj  0.03884846374945852\n",
      "obj  0.038848460314505485\n",
      "obj  0.03884846031450548\n",
      "obj  0.038848458635440536\n",
      "obj  0.038848458635440536\n",
      "obj  0.03884845780831109\n",
      "obj  0.03884845780831109\n",
      "obj  0.038848457397133655\n",
      "obj  0.038848457397133655\n",
      "obj  0.03884845719214209\n",
      "obj  0.03884845719214209\n",
      "obj  0.03884845708979564\n",
      "obj  0.038848457089795646\n",
      "obj  0.038848457038659745\n",
      "obj  0.03884845701310116\n",
      "obj  0.03884845701310116\n",
      "obj  0.038848457000324195\n",
      "obj  0.038848457000324195\n",
      "obj  0.038848456993936305\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obj  0.038848456993936305\n",
      "obj  0.038848456990742485\n",
      "obj  0.038848456990742485\n",
      "obj  0.03884845698914562\n",
      "obj  0.03884845698914562\n",
      "obj  0.03884845698834719\n",
      "obj  0.03884845698834719\n",
      "obj  0.03884845698794799\n",
      "obj  0.038848456987947984\n",
      "obj  0.038848456987748387\n",
      "obj  0.038848456987748387\n",
      "obj  0.03884845698764858\n",
      "obj  0.03884845698764858\n",
      "obj  0.03884845698759868\n",
      "obj  0.03884845698759868\n",
      "v76 d-1 f0 t2: original ll 0.0422 auc 0.9928, ensemble ll 0.0423 auc 0.9928\n",
      "running time 31.989184379577637\n",
      "starting fold 0 target 3\n",
      "obj  0.023644200501472847\n",
      "obj  0.02365128397767065\n",
      "obj  0.023650114938621684\n",
      "obj  0.02364947152285936\n",
      "obj  0.023649884306127245\n",
      "obj  0.023650396865955513\n",
      "obj  0.02364952740574242\n",
      "obj  0.02364781307224471\n",
      "obj  0.02364431614163881\n",
      "obj  0.02362166156563903\n",
      "obj  0.023620606101080837\n",
      "obj  0.023620841782741552\n",
      "obj  0.023620841782741552\n",
      "obj  0.023620841782741552\n",
      "obj  0.023620841782741552\n",
      "obj  0.023620841782741552\n",
      "obj  0.023620841782741552\n",
      "obj  0.023620841782741552\n",
      "obj  0.023620841782741552\n",
      "obj  0.023620841782741552\n",
      "obj  0.023620841782741552\n",
      "obj  0.023620841782741552\n",
      "obj  0.023620841782741552\n",
      "obj  0.023620841782741552\n",
      "obj  0.023620841782741552\n",
      "obj  0.023620747137144817\n",
      "obj  0.023620747137144817\n",
      "obj  0.023620643357141063\n",
      "obj  0.023620643357141063\n",
      "obj  0.023620616423753318\n",
      "obj  0.023620616423753318\n",
      "obj  0.023620609187357443\n",
      "obj  0.02362060918735745\n",
      "obj  0.023620607125232052\n",
      "obj  0.023620607125232052\n",
      "obj  0.023620606483381826\n",
      "obj  0.023620606483381826\n",
      "obj  0.02362060625978423\n",
      "obj  0.02362060625978423\n",
      "obj  0.023620606172320386\n",
      "obj  0.023620606259411838\n",
      "obj  0.023620606259411838\n",
      "obj  0.02362060621383901\n",
      "obj  0.02362060621383901\n",
      "obj  0.023620606192572922\n",
      "obj  0.023620606192572915\n",
      "obj  0.02362060618231996\n",
      "obj  0.023620606177288495\n",
      "obj  0.023620606177288492\n",
      "obj  0.023620606174796527\n",
      "obj  0.023620606174796523\n",
      "obj  0.02362060617355648\n",
      "obj  0.023620606172937937\n",
      "obj  0.023620606172937937\n",
      "obj  0.023620606172629035\n",
      "obj  0.02362060617262903\n",
      "obj  0.02362060617247468\n",
      "obj  0.02362060617247468\n",
      "obj  0.023620606172397522\n",
      "obj  0.023620606172397522\n",
      "obj  0.02362060617235895\n",
      "obj  0.023620606172358952\n",
      "obj  0.023620606172339666\n",
      "obj  0.023620606172339666\n",
      "obj  0.02362060617233002\n",
      "obj  0.023620606172330017\n",
      "obj  0.023620606172339666\n",
      "obj  0.023620606172339666\n",
      "obj  0.023620606172334854\n",
      "obj  0.023620606172334854\n",
      "obj  0.023620606172332435\n",
      "obj  0.023620606172332435\n",
      "obj  0.023620606172330017\n",
      "obj  0.02362084235907661\n",
      "obj  0.02362084235907661\n",
      "obj  0.02362074412593359\n",
      "obj  0.02362074412593359\n",
      "obj  0.023620643443270688\n",
      "obj  0.023620643443270688\n",
      "obj  0.023620616890666992\n",
      "obj  0.023620616890666992\n",
      "obj  0.02362060955340519\n",
      "obj  0.02362060955340519\n",
      "obj  0.023620607368136457\n",
      "obj  0.023620607368136464\n",
      "obj  0.023620606646524357\n",
      "obj  0.023620606378496695\n",
      "obj  0.023620606378496695\n",
      "obj  0.02362060626768033\n",
      "obj  0.023620606218071865\n",
      "obj  0.023620606218071865\n",
      "obj  0.023620606194717612\n",
      "obj  0.023620606194717612\n",
      "obj  0.02362060618340299\n",
      "obj  0.02362060618340299\n",
      "obj  0.023620606177836304\n",
      "obj  0.023620606175075613\n",
      "obj  0.023620606175075613\n",
      "obj  0.023620606173700927\n",
      "obj  0.02362060617370093\n",
      "obj  0.023620606173015004\n",
      "obj  0.023620606173015004\n",
      "obj  0.023620606172672396\n",
      "obj  0.023620606172672396\n",
      "obj  0.023620606172501175\n",
      "obj  0.023620606172501175\n",
      "obj  0.023620606172415598\n",
      "obj  0.023620606172415598\n",
      "obj  0.023620606172372813\n",
      "obj  0.023620606172372813\n",
      "obj  0.023620606172351417\n",
      "obj  0.023620606172351417\n",
      "obj  0.023620606172340717\n",
      "obj  0.02362060617234072\n",
      "obj  0.023620606172335374\n",
      "obj  0.023620606172335377\n",
      "obj  0.023620606172332703\n",
      "obj  0.023620606172332703\n",
      "obj  0.023620606172330017\n",
      "obj  0.02362084215938482\n",
      "obj  0.023620842159384817\n",
      "obj  0.02362074344102141\n",
      "obj  0.023620743441021406\n",
      "obj  0.023620643329408737\n",
      "obj  0.023620643329408737\n",
      "obj  0.023620616890800986\n",
      "obj  0.023620616890800986\n",
      "obj  0.023620609567684574\n",
      "obj  0.023620609567684577\n",
      "obj  0.023620607378831288\n",
      "obj  0.023620607378831288\n",
      "obj  0.023620606652760816\n",
      "obj  0.023620606652760816\n",
      "obj  0.023620606381837227\n",
      "obj  0.023620606381837227\n",
      "obj  0.02362060626940617\n",
      "obj  0.02362060626940617\n",
      "obj  0.023620606218948677\n",
      "obj  0.023620606195159495\n",
      "obj  0.023620606183624798\n",
      "obj  0.023620606183624798\n",
      "obj  0.023620606177947416\n",
      "obj  0.023620606175131224\n",
      "obj  0.02362060617513122\n",
      "obj  0.023620606173728752\n",
      "obj  0.023620606173728752\n",
      "obj  0.02362060617302892\n",
      "obj  0.023620606173028927\n",
      "obj  0.02362060617267935\n",
      "obj  0.023620606172679356\n",
      "obj  0.023620606172504673\n",
      "obj  0.02362060617250467\n",
      "obj  0.023620606172417333\n",
      "obj  0.02362060617241734\n",
      "obj  0.023620606172373684\n",
      "obj  0.023620606172373687\n",
      "obj  0.023620606172351854\n",
      "obj  0.023620606172351854\n",
      "obj  0.02362060617234094\n",
      "obj  0.023620606172340942\n",
      "obj  0.02362060617233549\n",
      "obj  0.02362060617233549\n",
      "obj  0.023620606172332758\n",
      "obj  0.02362060617233276\n",
      "obj  0.023620606172330017\n",
      "obj  0.02362084212157271\n",
      "obj  0.023620743307554707\n",
      "obj  0.023620743307554707\n",
      "obj  0.023620643307503176\n",
      "obj  0.023620643307503176\n",
      "obj  0.02362061689103847\n",
      "obj  0.02362061689103847\n",
      "obj  0.023620609570590225\n",
      "obj  0.023620609570590225\n",
      "obj  0.023620607380981255\n",
      "obj  0.023620607380981255\n",
      "obj  0.023620606654010133\n",
      "obj  0.023620606654010133\n",
      "obj  0.023620606382505467\n",
      "obj  0.023620606382505467\n",
      "obj  0.02362060626975119\n",
      "obj  0.02362060626975119\n",
      "obj  0.023620606219123912\n",
      "obj  0.023620606219123912\n",
      "obj  0.023620606195247792\n",
      "obj  0.023620606195247792\n",
      "obj  0.023620606183669117\n",
      "obj  0.02362060618366912\n",
      "obj  0.02362060617796962\n",
      "obj  0.02362060617796962\n",
      "obj  0.023620606175142327\n",
      "obj  0.023620606175142327\n",
      "obj  0.023620606173734314\n",
      "obj  0.023620606173734314\n",
      "obj  0.023620606173031695\n",
      "obj  0.023620606173031695\n",
      "obj  0.02362060617268074\n",
      "obj  0.02362060617268074\n",
      "obj  0.023620606172505356\n",
      "obj  0.023620606172505356\n",
      "obj  0.023620606172417683\n",
      "obj  0.023620606172417683\n",
      "obj  0.02362060617237385\n",
      "obj  0.02362060617237385\n",
      "obj  0.023620606172351934\n",
      "obj  0.02362060617235193\n",
      "obj  0.023620606172340984\n",
      "obj  0.023620606172340984\n",
      "obj  0.023620606172335502\n",
      "obj  0.023620606172332775\n",
      "obj  0.023620606172332775\n",
      "v76 d-1 f0 t3: original ll 0.0255 auc 0.9965, ensemble ll 0.0261 auc 0.9965\n",
      "running time 14.458370685577393\n",
      "starting fold 0 target 4\n",
      "obj  0.06295320537523548\n",
      "obj  0.06295795820473102\n",
      "obj  0.06294823091737932\n",
      "obj  0.06295226442210552\n",
      "obj  0.06314243768572256\n",
      "obj  0.06314036187075907\n",
      "obj  0.06313714955542234\n",
      "obj  0.06313490007240055\n",
      "obj  0.06296712802733281\n",
      "obj  0.06286308456530267\n",
      "obj  0.06285969330822251\n",
      "obj  0.06285936287709082\n",
      "obj  0.06285935956629274\n",
      "obj  0.0628584552555356\n",
      "obj  0.06285732696371041\n",
      "obj  0.06285731630339639\n",
      "obj  0.06285731630306027\n",
      "v76 d-1 f0 t4: original ll 0.0653 auc 0.9813, ensemble ll 0.0652 auc 0.9814\n",
      "running time 5.986945390701294\n",
      "starting fold 0 target 5\n",
      "obj  0.07741026670951476\n",
      "obj  0.0773928938287676\n",
      "obj  0.07738962795974991\n",
      "obj  0.07740117713791027\n",
      "obj  0.07795096196224648\n",
      "obj  0.07793814689034084\n",
      "obj  0.07793390840357202\n",
      "obj  0.0776549597346062\n",
      "obj  0.07786050689440231\n",
      "obj  0.07765280882250261\n",
      "obj  0.07749349744332389\n",
      "obj  0.07745906071618958\n",
      "obj  0.0774007139504504\n",
      "obj  0.07721977242563742\n",
      "obj  0.07721280401708015\n",
      "obj  0.07721279795718956\n",
      "obj  0.07721279778168778\n",
      "obj  0.07721279777641589\n",
      "obj  0.0772127977622461\n",
      "v76 d-1 f0 t5: original ll 0.0812 auc 0.9790, ensemble ll 0.0810 auc 0.9791\n",
      "running time 6.5645952224731445\n",
      "starting fold 1 target 0\n",
      "obj  0.09466385960462494\n",
      "obj  0.09516596893841994\n",
      "obj  0.09514757772578446\n",
      "obj  0.09512591967342868\n",
      "obj  0.09510893326460425\n",
      "obj  0.09507144301502796\n",
      "obj  0.09505186877776825\n",
      "obj  0.09502884784870844\n",
      "obj  0.09499650636889807\n",
      "obj  0.09489799151103787\n",
      "obj  0.09484342045491413\n",
      "obj  0.09483031985011547\n",
      "obj  0.09481616527017074\n",
      "obj  0.09479919192938258\n",
      "obj  0.09478234195034807\n",
      "obj  0.09476518210345172\n",
      "obj  0.09474565561389817\n",
      "obj  0.09469389542014224\n",
      "obj  0.0946783715852251\n",
      "obj  0.09466404565494342\n",
      "obj  0.0946488259915243\n",
      "obj  0.09463510144409108\n",
      "obj  0.09461846615832764\n",
      "obj  0.09459948771583063\n",
      "obj  0.09458371555261931\n",
      "obj  0.09457118957288552\n",
      "obj  0.0945589255063207\n",
      "obj  0.09454510503758332\n",
      "obj  0.09453429695767711\n",
      "obj  0.09452326836480593\n",
      "obj  0.09449533519699184\n",
      "obj  0.09448219266789858\n",
      "obj  0.09448165273123452\n",
      "obj  0.09448159763435454\n",
      "obj  0.09448159638625833\n",
      "obj  0.09448159630550237\n",
      "obj  0.09448159630222142\n",
      "obj  0.09448159630202074\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obj  0.0944815963020149\n",
      "obj  0.0944815963020132\n",
      "obj  0.09448159630201315\n",
      "v76 d-1 f1 t0: original ll 0.0971 auc 0.9868, ensemble ll 0.0968 auc 0.9869\n",
      "running time 13.513871908187866\n",
      "starting fold 1 target 1\n",
      "obj  0.01528526465995214\n",
      "obj  0.015268399741935492\n",
      "obj  0.01526962240186529\n",
      "obj  0.015269134034227395\n",
      "obj  0.015282755488557743\n",
      "obj  0.015281034107516177\n",
      "obj  0.015280071436972322\n",
      "obj  0.015287101823106527\n",
      "obj  0.015254914518439251\n",
      "obj  0.015240947926139082\n",
      "obj  0.015240761522474618\n",
      "obj  0.015240452490303738\n",
      "obj  0.015240421665482059\n",
      "v76 d-1 f1 t1: original ll 0.0133 auc 0.9631, ensemble ll 0.0133 auc 0.9634\n",
      "running time 4.90322470664978\n",
      "starting fold 1 target 2\n",
      "obj  0.03965667914305073\n",
      "obj  0.039656970762419516\n",
      "obj  0.039649675022433536\n",
      "obj  0.039648749109706796\n",
      "obj  0.03974325103014547\n",
      "obj  0.03973817128298135\n",
      "obj  0.03973494721617907\n",
      "obj  0.03973072661087729\n",
      "obj  0.03964525351326303\n",
      "obj  0.0396149936828302\n",
      "obj  0.03961446329176456\n",
      "obj  0.039614440711448734\n",
      "obj  0.03960994870578889\n",
      "obj  0.039601334833741904\n",
      "obj  0.039599969054679954\n",
      "obj  0.03959995207552729\n",
      "obj  0.039599951996344106\n",
      "obj  0.03959995198548741\n",
      "obj  0.03959995198456804\n",
      "obj  0.03959995198455553\n",
      "obj  0.03959995198455347\n",
      "v76 d-1 f1 t2: original ll 0.0406 auc 0.9925, ensemble ll 0.0406 auc 0.9925\n",
      "running time 7.9694600105285645\n",
      "starting fold 1 target 3\n",
      "obj  0.02479942450463579\n",
      "obj  0.024821874618250334\n",
      "obj  0.024820249298446874\n",
      "obj  0.02481935126061807\n",
      "obj  0.024819810916347686\n",
      "obj  0.024820100421038625\n",
      "obj  0.02481900096870115\n",
      "obj  0.024816762165761157\n",
      "obj  0.02481165925765609\n",
      "obj  0.02477668429836531\n",
      "obj  0.0247742599219245\n",
      "obj  0.024773775640000342\n",
      "obj  0.024772634212853396\n",
      "obj  0.024772023267837204\n",
      "obj  0.024771720417410117\n",
      "obj  0.02476925762419797\n",
      "obj  0.02476665560226389\n",
      "obj  0.024761737738453405\n",
      "obj  0.024756518496946406\n",
      "obj  0.02475499554715564\n",
      "obj  0.024754963710928272\n",
      "obj  0.02475496297140457\n",
      "obj  0.024754962943496158\n",
      "obj  0.024754962939622403\n",
      "obj  0.0247549629395671\n",
      "obj  0.024754962939565747\n",
      "obj  0.024754962939565684\n",
      "obj  0.02475496293956568\n",
      "obj  0.024754962939565684\n",
      "obj  0.02475496293956568\n",
      "obj  0.024754962939565684\n",
      "obj  0.02475496293956568\n",
      "obj  0.024754962939565684\n",
      "obj  0.02475496293956568\n",
      "obj  0.024754962939565684\n",
      "obj  0.02475496293956568\n",
      "obj  0.024754962939565684\n",
      "obj  0.02475496293956568\n",
      "obj  0.024754962939565684\n",
      "obj  0.02475496293956568\n",
      "obj  0.024754962939565684\n",
      "obj  0.02475496293956568\n",
      "obj  0.024754962939565684\n",
      "obj  0.02475496293956568\n",
      "obj  0.024754962939565684\n",
      "obj  0.02475496293956568\n",
      "obj  0.024754962939565684\n",
      "obj  0.02475496293956568\n",
      "obj  0.024754962939565684\n",
      "obj  0.02475496293956568\n",
      "obj  0.024754962939565684\n",
      "obj  0.02475496293956568\n",
      "obj  0.024754962939565684\n",
      "obj  0.02475496293956568\n",
      "obj  0.024754962939565684\n",
      "obj  0.02475496293956568\n",
      "obj  0.024754962939565684\n",
      "obj  0.02475496293956568\n",
      "obj  0.024754962939565684\n",
      "obj  0.02475496293956568\n",
      "obj  0.024754962939565715\n",
      "obj  0.024754962939565715\n",
      "obj  0.02475496293956573\n",
      "v76 d-1 f1 t3: original ll 0.0233 auc 0.9970, ensemble ll 0.0233 auc 0.9970\n",
      "running time 11.29972767829895\n",
      "starting fold 1 target 4\n",
      "obj  0.06405074386071544\n",
      "obj  0.06408264370888339\n",
      "obj  0.06408266522913797\n",
      "obj  0.06408298022256396\n",
      "obj  0.06408382579413342\n",
      "obj  0.06408226683945475\n",
      "obj  0.06408217133902575\n",
      "obj  0.06408032861543225\n",
      "obj  0.06404310676035521\n",
      "obj  0.06398464807062343\n",
      "obj  0.06396981368906425\n",
      "obj  0.06396092172832447\n",
      "obj  0.06395545054620562\n",
      "obj  0.06395197012667637\n",
      "obj  0.06394976280230803\n",
      "obj  0.06394835695189621\n",
      "obj  0.06394758481254664\n",
      "obj  0.06393628021993848\n",
      "obj  0.06393689782040028\n",
      "obj  0.06393689782040028\n",
      "obj  0.06393689782040028\n",
      "obj  0.06393689782040028\n",
      "obj  0.06393689782040028\n",
      "obj  0.06393689782040028\n",
      "obj  0.06393689782040028\n",
      "obj  0.06393689782040028\n",
      "obj  0.06393689782040028\n",
      "obj  0.06393689782040028\n",
      "obj  0.06393689782040028\n",
      "obj  0.06393689782040028\n",
      "obj  0.06393689782040028\n",
      "obj  0.06393689782040028\n",
      "obj  0.06393689782040028\n",
      "obj  0.06393689782040028\n",
      "obj  0.06393660799693199\n",
      "obj  0.06393660799693199\n",
      "obj  0.06393640594459052\n",
      "obj  0.06393640594459052\n",
      "obj  0.06393633298336825\n",
      "obj  0.06393633298336825\n",
      "obj  0.06393630399482396\n",
      "obj  0.06393630399482396\n",
      "obj  0.06393629144449998\n",
      "obj  0.06393629144449998\n",
      "obj  0.06393628566526234\n",
      "obj  0.06393628566526234\n",
      "obj  0.06393628290067302\n",
      "obj  0.06393628154979719\n",
      "obj  0.06393628154979718\n",
      "obj  0.06393628088223441\n",
      "obj  0.06393628055042441\n",
      "obj  0.06393628055042441\n",
      "obj  0.0639362803850125\n",
      "obj  0.0639362803850125\n",
      "obj  0.06393628030242993\n",
      "obj  0.06393628026116945\n",
      "obj  0.06393628026116945\n",
      "obj  0.06393628024054696\n",
      "obj  0.06393628024054697\n",
      "obj  0.0639362802302376\n",
      "obj  0.0639362802302376\n",
      "obj  0.06393628022508342\n",
      "obj  0.06393628022508344\n",
      "obj  0.06393628022250646\n",
      "obj  0.06393628022508342\n",
      "obj  0.06393628022508342\n",
      "obj  0.06393628022379493\n",
      "obj  0.06393628022379493\n",
      "obj  0.06393628022315069\n",
      "obj  0.06393628022315069\n",
      "obj  0.06393628022282857\n",
      "obj  0.0639362802228286\n",
      "obj  0.06393628022266751\n",
      "obj  0.06393628022266751\n",
      "obj  0.06393628022258697\n",
      "obj  0.06393628022258697\n",
      "obj  0.06393628022250646\n",
      "obj  0.06393689652407601\n",
      "obj  0.06393689652407601\n",
      "obj  0.06393660613988675\n",
      "obj  0.0639364053241863\n",
      "obj  0.0639364053241863\n",
      "obj  0.0639363327575863\n",
      "obj  0.0639363327575863\n",
      "obj  0.06393630390503965\n",
      "obj  0.06393630390503965\n",
      "obj  0.06393629140648556\n",
      "obj  0.06393629140648556\n",
      "obj  0.06393628564895792\n",
      "obj  0.06393628564895792\n",
      "obj  0.06393628289416525\n",
      "obj  0.06393628289416524\n",
      "obj  0.06393628154792104\n",
      "obj  0.06393628154792104\n",
      "obj  0.06393628088260714\n",
      "obj  0.06393628055190478\n",
      "obj  0.06393628055190478\n",
      "obj  0.06393628038704255\n",
      "obj  0.06393628038704255\n",
      "obj  0.06393628030473375\n",
      "obj  0.06393628030473375\n",
      "obj  0.0639362802636099\n",
      "obj  0.0639362802636099\n",
      "obj  0.06393628024305563\n",
      "obj  0.06393628024305563\n",
      "obj  0.0639362802327804\n",
      "obj  0.0639362802327804\n",
      "obj  0.06393628022764325\n",
      "obj  0.06393628022764325\n",
      "obj  0.06393628022507483\n",
      "obj  0.06393628022507483\n",
      "obj  0.06393628022379062\n",
      "obj  0.06393628022379061\n",
      "obj  0.06393628022314857\n",
      "obj  0.06393628022282749\n",
      "obj  0.06393628022266698\n",
      "obj  0.06393628022258672\n",
      "obj  0.06393628022258672\n",
      "obj  0.06393628022250646\n",
      "obj  0.06393689626373489\n",
      "obj  0.06393689626373489\n",
      "obj  0.06393660576942249\n",
      "obj  0.06393660576942249\n",
      "obj  0.06393640519987392\n",
      "obj  0.06393633271193222\n",
      "obj  0.0639363327119322\n",
      "obj  0.06393630388654323\n",
      "obj  0.06393630388654323\n",
      "obj  0.0639362913983467\n",
      "obj  0.06393629139834671\n",
      "obj  0.0639362856451692\n",
      "obj  0.0639362856451692\n",
      "obj  0.06393628289234152\n",
      "obj  0.06393628289234152\n",
      "obj  0.06393628154702691\n",
      "obj  0.06393628154702691\n",
      "obj  0.0639362808821645\n",
      "obj  0.0639362808821645\n",
      "obj  0.06393628055168459\n",
      "obj  0.06393628055168459\n",
      "obj  0.06393628038693273\n",
      "obj  0.06393628038693273\n",
      "obj  0.06393628030467889\n",
      "obj  0.06393628030467889\n",
      "obj  0.06393628026358252\n",
      "obj  0.06393628026358252\n",
      "obj  0.06393628024304193\n",
      "obj  0.06393628024304193\n",
      "obj  0.06393628023277356\n",
      "obj  0.06393628023277355\n",
      "obj  0.06393628022763984\n",
      "obj  0.06393628022763984\n",
      "obj  0.0639362802250731\n",
      "obj  0.06393628022378976\n",
      "obj  0.06393628022378976\n",
      "obj  0.06393628022314811\n",
      "obj  0.06393628022314811\n",
      "obj  0.06393628022282728\n",
      "obj  0.06393628022282728\n",
      "obj  0.06393628022266688\n",
      "obj  0.06393628022266688\n",
      "obj  0.06393628022258666\n",
      "obj  0.06393628022258667\n",
      "v76 d-1 f1 t4: original ll 0.0630 auc 0.9810, ensemble ll 0.0631 auc 0.9811\n",
      "running time 15.724650144577026\n",
      "starting fold 1 target 5\n",
      "obj  0.07807655162368311\n",
      "obj  0.07806374692440166\n",
      "obj  0.0780588594631102\n",
      "obj  0.07806868349643732\n",
      "obj  0.07846432888664684\n",
      "obj  0.07845751082002418\n",
      "obj  0.07845426575915776\n",
      "obj  0.07839633278149953\n",
      "obj  0.0783022753234392\n",
      "obj  0.0781310242526549\n",
      "obj  0.07812837203919143\n",
      "obj  0.07808592438601604\n",
      "obj  0.07795731755712804\n",
      "obj  0.07795510006075553\n",
      "obj  0.07795509033429289\n",
      "obj  0.07795506862191993\n",
      "obj  0.07795506832415573\n",
      "v76 d-1 f1 t5: original ll 0.0799 auc 0.9791, ensemble ll 0.0796 auc 0.9792\n",
      "running time 6.383020639419556\n",
      "starting fold 2 target 0\n",
      "obj  0.09666830899448764\n",
      "obj  0.09719609659871899\n",
      "obj  0.0971801109101129\n",
      "obj  0.09716749475321378\n",
      "obj  0.09715844202201998\n",
      "obj  0.09713994248328253\n",
      "obj  0.09712929545165622\n",
      "obj  0.09711496381279247\n",
      "obj  0.09709836389537241\n",
      "obj  0.09706339544657124\n",
      "obj  0.09703705363615854\n",
      "obj  0.0970155677590417\n",
      "obj  0.09699802785270657\n",
      "obj  0.09698102597227207\n",
      "obj  0.09696485016615834\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "obj  0.09694724299835498\n",
      "obj  0.09692854775939869\n",
      "obj  0.09691261177713109\n",
      "obj  0.09689776192219535\n",
      "obj  0.09688477752975173\n",
      "obj  0.09686953652475169\n",
      "obj  0.09685418114723078\n",
      "obj  0.09684177570174622\n",
      "obj  0.09682987844632263\n",
      "obj  0.09681692992830808\n",
      "obj  0.0968044332578605\n",
      "obj  0.09679179807159953\n",
      "obj  0.09677838811254054\n",
      "obj  0.09676520840036909\n",
      "obj  0.096749505964424\n",
      "obj  0.0967355158226414\n",
      "obj  0.0967230527678535\n",
      "obj  0.09671066513776609\n",
      "obj  0.09669638515369106\n",
      "obj  0.09668084111641939\n",
      "obj  0.09666658805647116\n",
      "obj  0.09665470853996583\n",
      "obj  0.09664484428791695\n",
      "obj  0.09663605763374686\n",
      "obj  0.09662694120130698\n",
      "obj  0.09661926522375636\n",
      "obj  0.09661271706369157\n",
      "obj  0.09660704352104513\n",
      "obj  0.09660207798246767\n",
      "obj  0.0965977119666005\n",
      "obj  0.09659211015156632\n",
      "obj  0.09658533357489003\n",
      "obj  0.09657735438445922\n",
      "obj  0.0965696180775532\n",
      "obj  0.09655917611569281\n",
      "obj  0.09652146950718678\n",
      "obj  0.09650695885514765\n",
      "obj  0.09650048504121282\n",
      "obj  0.09649957635150709\n",
      "obj  0.09649956532473411\n",
      "obj  0.0964995652797407\n",
      "obj  0.09649956527937502\n",
      "obj  0.09649956527937219\n",
      "obj  0.09649956527937202\n",
      "v76 d-1 f2 t0: original ll 0.0930 auc 0.9884, ensemble ll 0.0928 auc 0.9884\n",
      "running time 19.701736450195312\n",
      "starting fold 2 target 1\n",
      "obj  0.014396183741972176\n",
      "obj  0.01439909782748712\n",
      "obj  0.014402390819221293\n",
      "obj  0.014399558967294121\n",
      "obj  0.01442417270318113\n",
      "obj  0.014394137433335103\n",
      "obj  0.014391008088018386\n",
      "obj  0.014394118468428186\n",
      "obj  0.014342663915491934\n",
      "obj  0.01433880511655084\n",
      "obj  0.014335752816142208\n",
      "obj  0.014335320440975016\n",
      "obj  0.01433520539081244\n",
      "obj  0.014335187728597518\n",
      "v76 d-1 f2 t1: original ll 0.0151 auc 0.9693, ensemble ll 0.0152 auc 0.9702\n",
      "running time 5.660861492156982\n",
      "starting fold 2 target 2\n",
      "obj  0.04142066096231964\n",
      "obj  0.04141950691718716\n",
      "obj  0.041411741970737\n",
      "obj  0.04141184858188629\n",
      "obj  0.0415403641169471\n",
      "obj  0.041534070727564064\n",
      "obj  0.04152987653300841\n",
      "obj  0.04152511870336332\n",
      "obj  0.04140938689097178\n",
      "obj  0.04138416695381088\n",
      "obj  0.04138397919616544\n",
      "obj  0.04138396994401442\n",
      "obj  0.04138141851857282\n",
      "obj  0.041373992376414\n",
      "obj  0.041373225914561836\n",
      "obj  0.04137322184176393\n",
      "obj  0.04137322182885199\n",
      "obj  0.04137322182882511\n",
      "obj  0.04137322182882506\n",
      "v76 d-1 f2 t2: original ll 0.0373 auc 0.9935, ensemble ll 0.0372 auc 0.9935\n",
      "running time 7.419363021850586\n",
      "starting fold 2 target 3\n",
      "obj  0.02438086143417937\n",
      "obj  0.024400436915855764\n",
      "obj  0.02439921587044549\n",
      "obj  0.02439849118851088\n",
      "obj  0.024398647377341213\n",
      "obj  0.024399530813108035\n",
      "obj  0.024398672729336417\n",
      "obj  0.02439733901696335\n",
      "obj  0.024396377144098518\n",
      "obj  0.0243850421890106\n",
      "obj  0.024383220519591846\n",
      "obj  0.02438208966370119\n",
      "obj  0.024379046291050737\n",
      "obj  0.024373927456214615\n",
      "obj  0.024366091140619675\n",
      "obj  0.024362056819187947\n",
      "obj  0.024361738697513992\n",
      "obj  0.02436173578167788\n",
      "obj  0.024361735678303357\n",
      "obj  0.024361735671390855\n",
      "obj  0.024361735671171805\n",
      "obj  0.024361735671160595\n",
      "obj  0.024361735671159988\n",
      "obj  0.02436173567115998\n",
      "v76 d-1 f2 t3: original ll 0.0241 auc 0.9970, ensemble ll 0.0240 auc 0.9970\n",
      "running time 8.814125299453735\n",
      "starting fold 2 target 4\n",
      "obj  0.06412095248597356\n",
      "obj  0.06417115667292256\n",
      "obj  0.0641685420452325\n",
      "obj  0.06416697996102674\n",
      "obj  0.0641661711943219\n",
      "obj  0.06416479415336378\n",
      "obj  0.06416405454531603\n",
      "obj  0.0641629355602363\n",
      "obj  0.06414976483096446\n",
      "obj  0.06407276235803319\n",
      "obj  0.06403861610719332\n",
      "obj  0.0640352277531286\n",
      "obj  0.06403433753811838\n",
      "obj  0.06403407494065207\n",
      "obj  0.06402641513241308\n",
      "obj  0.06401881738034582\n",
      "obj  0.06401342472235677\n",
      "obj  0.06401211467747402\n",
      "obj  0.06401205560367987\n",
      "obj  0.0640120552893342\n",
      "v76 d-1 f2 t4: original ll 0.0629 auc 0.9828, ensemble ll 0.0628 auc 0.9829\n",
      "running time 7.314190626144409\n",
      "starting fold 2 target 5\n",
      "obj  0.08041136133067063\n",
      "obj  0.0803930849272707\n",
      "obj  0.08038544216605903\n",
      "obj  0.08039144128054102\n",
      "obj  0.08060113717501603\n",
      "obj  0.08059126625087608\n",
      "obj  0.08058943034026268\n",
      "obj  0.08057275075664717\n",
      "obj  0.08027621013579801\n",
      "obj  0.08024228578858189\n",
      "obj  0.0802016525535718\n",
      "obj  0.08020104911674018\n",
      "obj  0.08020104206112326\n",
      "obj  0.08020104177085412\n",
      "obj  0.08020103169069691\n",
      "obj  0.08020101887498769\n",
      "obj  0.08020101885511532\n",
      "v76 d-1 f2 t5: original ll 0.0752 auc 0.9819, ensemble ll 0.0751 auc 0.9820\n",
      "running time 6.39799690246582\n",
      "total running time 192.26241755485535\n"
     ]
    }
   ],
   "source": [
    "stg = time.time()\n",
    "for fold in range(3):\n",
    "    for target in range(6):\n",
    "        train_ensemble(ids_df, preds_all, fold=fold, target=target, ds_idx=-1, first_step=False)\n",
    "print('total running time', time.time() - stg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train original ll 0.05888 ensemble ll 0.05876\n",
      "valid original ll 0.05895 ensemble ll 0.05890\n"
     ]
    }
   ],
   "source": [
    "agg = stats.loc[stats.ds_idx == -1].groupby('target').mean().sort_index()\n",
    "print('train original ll {:.5f} ensemble ll {:.5f}'\n",
    "      .format((agg.train_loss * class_weights).mean(), (agg.train_loss_ens * class_weights).mean()))\n",
    "print('valid original ll {:.5f} ensemble ll {:.5f}'\n",
    "      .format((agg.valid_loss * class_weights).mean(), (agg.valid_loss_ens * class_weights).mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total running time 3.944089412689209\n"
     ]
    }
   ],
   "source": [
    "stg = time.time()\n",
    "preds = np.stack([pickle.load(open(PATH_WORK/'preds_{}_v{}'.format('Densenet161', 72),'rb')),\n",
    "                  pickle.load(open(PATH_WORK/'preds_{}_v{}'.format('Densenet169', 73),'rb')),\n",
    "                  pickle.load(open(PATH_WORK/'preds_{}_v{}'.format('Densenet201', 74),'rb')),\n",
    "                  pickle.load(open(PATH_WORK/'preds_{}_v{}'.format('se_resnext101_32x4d', 75),'rb'))])\n",
    "\n",
    "test_preds_trgt = []\n",
    "for target in range(6):\n",
    "    \n",
    "    test_preds_folds = []\n",
    "    for fold in range(3):\n",
    "        \n",
    "        test_preds = []\n",
    "        for ds_idx in range(4):\n",
    "            model = pickle.load(open(PATH_WORK/'ensemble'/'model.d{}.f{}.t{}.v{}'\n",
    "                                     .format(ds_idx,fold,target,VERSION),'rb'))\n",
    "            X,y,ll_train,auc_train =  getFirstStepX(None, preds[:,fold], TH=model.prior, \n",
    "                                                    powerLow=model.powerLow, powerHigh=model.powerHigh, \n",
    "                                                    fold=fold, target=target, ds_idx=ds_idx, mode='test')\n",
    "            test_preds.append((X*np.expand_dims(model.x, axis=1)).sum(0))\n",
    "        \n",
    "        X = np.stack(test_preds)\n",
    "        model = pickle.load(open(PATH_WORK/'ensemble'/'model.d{}.f{}.t{}.v{}'\n",
    "                                 .format(-1,fold,target,VERSION),'rb'))\n",
    "        test_preds_folds.append((X*np.expand_dims(model.x, axis=1)).sum(0))\n",
    "    \n",
    "    X = np.stack(test_preds_folds).mean(0)\n",
    "    test_preds_trgt.append(X)\n",
    "\n",
    "predictions = np.stack(test_preds_trgt,axis=1)\n",
    "\n",
    "print('total running time', time.time() - stg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78545, 6)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = 0.5* (pickle.load(open(PATH_WORK/'preds_{}_v{}'.format('Densenet169', 51),'rb')) +\n",
    "         pickle.load(open(PATH_WORK/'preds_{}_v{}'.format('Densenet201', 52),'rb')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = (pickle.load(open(PATH_WORK/'preds_{}_v{}'.format('Densenet169', 51),'rb')) +\n",
    "         pickle.load(open(PATH_WORK/'preds_{}_v{}'.format('Densenet201', 52),'rb')) +\n",
    "         pickle.load(open(PATH_WORK/'preds_{}_v{}'.format('se_resnext101_32x4d', 53),'rb'))) / 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.stack([pickle.load(open(PATH_WORK/'preds_{}_v{}'.format('Densenet161', 72),'rb')),\n",
    "                  pickle.load(open(PATH_WORK/'preds_{}_v{}'.format('Densenet169', 73),'rb')),\n",
    "                  pickle.load(open(PATH_WORK/'preds_{}_v{}'.format('Densenet201', 74),'rb')),\n",
    "                  pickle.load(open(PATH_WORK/'preds_{}_v{}'.format('se_resnext101_32x4d', 75),'rb'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.960e-08, 9.020e-10, 9.625e-09, 6.666e-09, 9.995e-09, 1.182e-08],\n",
       "       [2.924e-08, 9.041e-10, 9.693e-09, 6.750e-09, 1.010e-08, 1.153e-08],\n",
       "       [2.927e-08, 1.022e-09, 9.434e-09, 6.728e-09, 9.807e-09, 1.187e-08],\n",
       "       [3.016e-08, 9.835e-10, 9.533e-09, 6.913e-09, 1.039e-08, 1.167e-08]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 217,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.std(2).mean((1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15249, 0.01507, 0.03719, 0.02508, 0.05596, 0.06206],\n",
       "       [0.14765, 0.01445, 0.03652, 0.02455, 0.05567, 0.05837],\n",
       "       [0.14786, 0.01586, 0.0346 , 0.02439, 0.05371, 0.06038],\n",
       "       [0.1466 , 0.01732, 0.03613, 0.02529, 0.05583, 0.05959]])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((- preds.mean((1,2), keepdims=True) * np.log(np.clip(preds,1e-15,1-1e-15)) \n",
    "  - (1 - preds.mean((1,2), keepdims=True)) * np.log(np.clip(1 - preds,1e-15,1-1e-15)))\n",
    " * class_weights).mean((1,2,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.54027, 0.54585, 0.53983, 0.54938])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((- preds.mean((1,2,3), keepdims=True) * np.log(np.clip(preds,1e-15,1-1e-15)) \n",
    "  - (1 - preds.mean((1,2,3), keepdims=True)) * np.log(np.clip(1 - preds,1e-15,1-1e-15)))\n",
    " * class_weights).mean((1,2,3,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = pickle.load(open(PATH_WORK/'preds_{}_v{}'.format('se_resnext101_32x4d', 75),'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 32, 78545, 6)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.quantile(preds,q=0.5,axis=(1)).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = preds.mean((0,1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = scalePreds(predictions, 1.13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_md['pred_any'] = predictions[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x24c3c6e9588>]"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3deVxV953/8dfnXlYBQVYXUEBB0ETUEIhxiVGzJ2afmmamyTRNpp2k7UyXadrOdEnaTrpN2980XTJtOtMlMWsbs0fjnsR9i4IgigKy7/v+/f1xLwYJygUunHsvn+fjwUPuuefg5yT45vA93/P5ijEGpZRSvstmdQFKKaXGlga9Ukr5OA16pZTycRr0Sinl4zTolVLKx/lZXcBA0dHRJjEx0eoylFLKq+zfv7/aGBMz2HseF/SJiYns27fP6jKUUsqriMiZC72nQzdKKeXjNOiVUsrHadArpZSP06BXSikfp0GvlFI+ToNeKaV8nAa9Ukr5OA16NSLGGA4V1/OnXWeoa+m0uhyl1EV43ANTynP19hoOFtfxxoflvPlhGaUN7QD8dttJfvP3l3HJjHCLK1RKDUaDXl1Ub69hf1Edrx8p462j5ZQ3thNgt7EiNZovXzuXqeFBfOWFw9z56/d54s5LuX1RvNUlK6UG0KBXH9M/3N88WkZFYwcBfjZWpsbw9QVprEqLJSzI/9z+r35+GQ//5QD/+txhDhc38M2b0vG366igUp5Cg16dY4xha34VP3orj9yyxnPhftOCaaxOjyM0cPBvl+jQQP7ymWyeePM4v9tZyLHSBr5/+6WkxoWN8xkopQYjnrZmbGZmptGmZuPvQFEdP3zzOLsLa5kZOYnPr5rDDZdOu2C4X8iGw6V87cUjtHX1cNmsKay7PIGbF0wnOMA+RpUrpQBEZL8xJnPQ9zToJ7bT1S384I1c3smpIDo0gC+sTmHd5TMJ8Bv50EtNcwcvHShh/Z5iTlW3EBbox22LZrBybgzJMaEkTAnGT4d2lHIrDXo1qKqmDm757500d3TzTyuS+fSyJEKGeQV/McYY9hTWsn5vMa9/WEZndy8A/nZhVlQIs2NCuGnBdNZmTHfb36nURHWxoNcx+gmqq6eXR545QH1bJy9/binzpk92+98hImQnR5GdHMXjt11CfkUTJyubOVnVwqmqZo6VNvL2sQrKG9p4aMVst//9SikHDfoJ6gnnePzPPpExJiE/UGigH4tnTmHxzCnntnV29/Kvzx3iB28cp6Wjh39Zk4KIjHktSk00GvQT0CuHzvL7nYXcf2WipfPeA/xs/L97FhEcYOcX756gtbObb9yYrmGvlJtp0E8wuWWNfO2lI2QlRvLNm9KtLge7TfjRnQsICbDzPzsKae3s4fFbL8Fm07BXyl006CeQhtYuPvvn/UwO8ueX9y7ymIeabDbhO2vnExzgx2+2naSrp5cf3rlAr+yVchMN+gnCGMOXXzhMaX0b6x+6gtiwIKtLOo+I8LXr52K3wZNbTrIiNYabF+hsHKXcwTMu6dSY25RbyabcCr5y7VwumxVpdTmDEhH+dU0qC+LD+fYrx6hp7rC6JKV8ggb9BNDe1cPjr+WQEhvKp5clWV3ORfnZbfzorgU0tnfx3VdzrC5HKZ+gQT8B/G7HKYpqW/n2LfM9Zlz+YtKmTuaRq1PYcLiUd46VW12OUl7P8//Vq1EprW/jyS0nuX7+VJalRFtdjss+t3I2aVPD+Pe/HaWhtcvqcpTyahr0Pu4Hb+TSa4xHTKUcjgA/Gz++K4Oalk6+97oO4Sg1Ghr0PmzXqRpeO1LGZ6+aTULkJKvLGbZL48P5pxXJvLC/hG35VVaXo5TX0qD3Ud09vXxnwzFmRATzuZXe20fmC6tTmB0Twjde/pD2rh6ry1HKK2nQ+6hn9hRxvLyJ/7g5nSB/7+0FH+Rv51u3zOdsfRtv641ZpUbEpaAXketFJE9ECkTk0UHe/5KI5IjIERF5V0Rm9XvvPhE54fy4z53Fq8G1dnbz03fyWToniuvmT7W6nFFbPieamZGTeGZ3kdWlKOWVhgx6EbEDTwI3APOAe0Rk3oDdDgKZxpgFwIvAj5zHRgLfBrKBLODbIjIFNaZeO1xGQ1sX/7Im1SfaCNhswrqsBHYX1nKyqtnqcpTyOq5c0WcBBcaYU8aYTmA9cGv/HYwxW4wxrc6Xu4C+lojXARuNMbXGmDpgI3C9e0pXF7J+bxFzYkPJnOU7P1PvuiweP5uwfo9e1Ss1XK4E/QyguN/rEue2C3kAeHM4x4rIQyKyT0T2VVXp7IrRyK9o4kBRPesuT/CJq/k+sWFBXDMvjhf3l9DRrTdllRoOV4J+sLQYdP1BEfl7IBP48XCONcY8ZYzJNMZkxsTEuFCSupBn9xQRYLdxx2Lr+syPlXuyZlLX2sXbxyqsLkUpr+JK0JcACf1exwOlA3cSkTXAN4G1xpiO4Ryr3KO9q4e/HjzLtfPjiAwJsLoct1s2J5qEyGCe1ZuySg2LK0G/F0gRkSQRCQDWARv67yAii4Df4gj5yn5vvQ1cKyJTnDdhr3VuU2Pg7WPl1Ld2cU/WTKtLGRM2m7Du8pl8cKqGU3pTVimXDRn0xphu4BEcAZ0LPG+MOSYij4nIWuduPwZCgRdE5JCIbHAeWws8juOHxV7gMec2NQae3VNEQmQwS5KjrC5lzNyd6bgp+9ze4qF3VkoBLi48Yox5A3hjwLZv9ft8zUWOfRp4eqQFKtcUVrew61QtX71urk8vwxcbFsSa9Dhe2F/Cl65NJdDPex8GU2q86JOxPmL93iLsNuHuy3zvJuxA92TPpLalk405elNWKVdo0PuAzu5eXtpfwqq0WGIne9YSgWNh+ZxoZkQE86zOqVfKJRr0PmDz8Qqqmzu5Jyth6J19gM0m3JOVwHsFNRTVtA59gFITnAa9D3h2TzFTJwdxVWqs1aWMm9sWOZ67eydHG50pNRQNei9X1tDG9hNV/F1mPHYfvgk7UPyUScyNC2Pz8cqhd1ZqgtOg93Kbj1diDKxdON3qUsbdqvRY9hTW0tiuSw0qdTEa9F5ua14VMyKCmR0TanUp425VWizdvYadJ6qtLkUpj6ZB78U6u3t5v6CalXNjfKqBmasWJUQQMcmfd3N1+Eapi9Gg92L7TtfS0tnDyrkT5yZsf352G1elxrA1r5Ke3kH77Cml0KD3alvzq/C3C0tm+27Lg6GsSoulpqWTwyX1VpeilMfSoPdiW/MquTwxktBAlzpZ+KSrUmOw24QtOvtGqQvSoPdSpfVt5Fc0s3LuxO7fHzEpgMtmTtFxeqUuQoPeS23Ld6zENVHH5/tblR5LTlkjZQ1tVpeilEfSoPdSW/MqmR4eRErsxJtWOdDqNMcPuy3HdRlKpQajQe+FOrt7ea+ghqvmxk7IaZUDzYkNJX5KMJuPazdLpQajQe+F9p+po7mje8KPz/cREVanxbKzoJr2Ll04XKmBNOi90Lb8KvxswpUTeFrlQKvS42jv6uWDUzVWl6KUx9Gg90Jb8yrJTJxCWJC/1aV4jOykSIL97WzW2TdKfYwGvZcpb2jneHmTzrYZIMjfzrKUaGeTN31KVqn+NOi9zLZ8xxWrjs9/3Kq0WM7Wt5FX0WR1KUp5FA16L7M1r4qpk4OYGxdmdSke52rnbznb83WapVL9adB7ka6eXnaemLjdKocyNTyIpOgQdp+qtboUpTyKBr0XOVhUT5NOq7yo7KRI9pyu1W6WSvWjQe9F3s2twN8uLJ0TbXUpHis7OZKm9m5yyxqtLkUpj6FB70U25VZwRXKUTqu8iOwkx7MFuwt1+EapPhr0XqKwuoWTVS3n+rqowU2PCCYhMpg9hfrglFJ9NOi9xLu5jj4uq9PjLK7E82UnRbGnsJZeHadXCtCg9xobcypImxpGQuQkq0vxeNlJkdS1dnGistnqUpTyCBr0XqC+tZN9Z+pYo1fzLrkiuW+cXodvlAINeq+wNa+Knl7Dmnka9K6InxLM9PAgnU+vlJMGvRfYmFtBTFggC2aEW12KVxARspOj2F1Yo31vlEKD3uN1dveyPa+K1Wmx2Gz6NKyrspMiqW7u5GRVi9WlKGU5DXoPt6ewlqaObp1tM0zZOk6v1Dka9B5uU24FgX42lunTsMOSGDWJ2LBAHadXCg16j2aMYVNuBcvmRBMcYLe6HK+i4/RKfcSloBeR60UkT0QKROTRQd5fISIHRKRbRO4a8F6PiBxyfmxwV+ETQV5FEyV1bTrbZoSykyKpaOzgTE2r1aUoZSm/oXYQETvwJHANUALsFZENxpicfrsVAfcDXxnkS7QZYxa6odYJZ1OO82lYbXswIlckRwKOcfrE6BCLq1HKOq5c0WcBBcaYU8aYTmA9cGv/HYwxp40xR4DeMahxwtqUW0lGfDixk4OsLsUrzY4JJTo0QMfp1YTnStDPAIr7vS5xbnNVkIjsE5FdInLbYDuIyEPOffZVVenqQACVTe0cKq7Xp2FHQUTISorUTpZqwnMl6AebvD2cu1szjTGZwCeBn4vI7I99MWOeMsZkGmMyY2J0UQ2ALccda8PqtMrRyU6K4mx9GyV1Ok6vJi5Xgr4ESOj3Oh4odfUvMMaUOv88BWwFFg2jvglry/EqpoUHkT5N14Ydjey+cXodvlETmCtBvxdIEZEkEQkA1gEuzZ4RkSkiEuj8PBpYCuRc/CjV1dPLewXVXJWqa8OOVmpsGFMm+fPeyWqrS1HKMkMGvTGmG3gEeBvIBZ43xhwTkcdEZC2AiFwuIiXA3cBvReSY8/B0YJ+IHAa2AE8MmK2jBtG3NuxVqTqMNVo2m7AsJYYdJ6p1Pr2asIacXglgjHkDeGPAtm/1+3wvjiGdgce9D1w6yhonnG35ldhtwtIUfRrWHVakRPPq4VJyy5qYN32y1eUoNe70yVgPtC2/istmTmGyrg3rFn2/GW0/oTO61MSkQe9hqpo6OHq2kavm6rCNu8RODiJtahjb8jTo1cSkQe9hdjivOnV83r2uSo1h35laWjq6rS5FqXGnQe9htuVXER0awLxpOpbsTitSY+jqMew6pW2L1cSjQe9BenoN2/OrWJESo4uMuFlm4hSC/e1sy9fhGzXxaNB7kKNnG6hr7dLx+TEQ6GdnyewotmvQqwlIg96DbMuvQgRdZGSMrEiJ5nRNK2dqdHlBNbFo0HuQrXmVLJgRTlRooNWl+KSr5jraPetVvZpoNOg9RH1rJ4eK68+FkXK/xKhJJEQGsy1f2yGoiUWD3kPsLKim1+i0yrEkIqxIieGDk9V0duvSCWri0KD3ENvyqggP9icjPtzqUnzaitQYWjp72H+mzupSlBo3GvQewBjDtvwqlqVE42fX/yVj6crZUfjZRNshqAlFU8UDHC9vorKpQ4dtxkFYkD+LZ03RdghqQtGg9wB9D/Fo0I+Pq1JjyClrpKqpw+pSlBoXGvQeYGteJWlTw4jTRcDHRd8P1B06fKMmCJf60aux09jexb7TdTy4ItnqUiaMedMmExUSwI/fzuNgUT2XJ0WSlRjJ1HD9Qat8kwa9xd47UU13r+FqnT8/bmw24Sd3Z/D0e4W8fKCEP+06A0BCZDD3LUnkM8v1h67yLRr0FtuSV0lYkB+LZ0ZYXcqEcnVaLFenxdLd00tOWSN7Cmt57UgZ//nmcdYunE5smF7dK9+hY/QWMsawJa+KFakxOq3SIn52GwviI/jM8mR+cvcCenoNGw6VWl2WUm6l6WKhY6WOmR86bOMZ5sSGkREfzssHzlpdilJupUFvoa15lYBOq/QkdyyOJ6eskdyyRqtLUcptNOgttCWvigXx4cSEabdKT3FLxnT87cLLB0qsLkUpt9Ggt0hdSycHi+pYqcM2HiUyJICr58byt0OldPdo4zPlGzToLbL9RBW9Bq7W1aQ8zh2L46lq6mBHgbYzVr5Bg94iW/OqiAwJYEG8Tqv0NFenxRAxyX/UN2WNMfx8Uz5Hzza4qTKlRkaD3gI9vY5ulVelxmDXRcA9TqCfnbUZ03nnWDmN7V0j/jqlDe38fNMJfr31pBurU2r4NOgtcKSkntqWTlbqsI3HumNxPB3dvbz5YdmIv8ahonrA0bSuo7vHXaUpNWwa9BbYkleFTWBFiga9p8qIDyc5JoSX9o98+OZQsWNxk+aObnadqnVXaUoNmwa9BbbmVbJo5hSmhARYXYq6ABHhzsXx7DldS1FN64i+xqHieuZNm0ywv52NOeVurlAp12nQj7Oqpg6OlDTobBsvcNuiGYjAXw8O/6q+q6eXD882kJ0cyYrUaDblVGKMGYMqlRqaBv0461tkROfPe74ZEcEsSY7ipQMl9PYOL6TzK5po7+plYUIEa9LjKG9s50OdfaMsokE/zrbkVRIbFsj86ZOtLkW54N7sWRTVtg77qv5QseNG7KKEKaxOj8MmsCmnYixKVGpIGvTjqKfXsCPf0a1SRKdVeoMbLplKRnw4P3knj7ZO12fOHCqqJzIkgITIYCJDAsicFck7GvTKIhr04+hIST2N7d2s0CZmXsNmE75xYzplDe08/V6hy8cdKq5nYULEuR/o18yL43h5E8W1I7uxq9RouBT0InK9iOSJSIGIPDrI+ytE5ICIdIvIXQPeu09ETjg/7nNX4d5ox4lqRGDZnGirS1HDkJ0cxTXz4vj11pNUNw+9oHhTexcFVc0sTPjoqec18+IA2KhX9coCQwa9iNiBJ4EbgHnAPSIyb8BuRcD9wDMDjo0Evg1kA1nAt0VkyujL9k7b86u4dEY4kTqt0us8ekMabV09/GLTiSH3PVLSgDGcF/RJ0SHMiQ1lU64GvRp/rlzRZwEFxphTxphOYD1wa/8djDGnjTFHgIHt/q4DNhpjao0xdcBG4Ho31O11Gtu7OFhcz/IUvZr3RrNjQvlk1kye2VNEQWXzRfftuxGbMaCP0TXz4thdWEtD68jbKig1Eq4E/QyguN/rEuc2V7h0rIg8JCL7RGRfVVWVi1/au3xwsoaeXqNPw3qxL65JIdjfzg/fOn7R/Q4W1ZMcHUL4JP/ztl8zL46eXsMW54IzSo0XV4J+sOkhrk4qdulYY8xTxphMY0xmTIxvBuH2/CpCAuwsmjlhR668XnRoIJ9bOZuNORXsOlUz6D7GmHM3YgdaGB9BdGigx43T/793Twz5w0t5N1eCvgRI6Pc6HnB19eTRHOtTdpyoZsnsKAL8dKKTN3tgWRLTwoP4wRu5gz5Edba+jermDhbO/HjQ22zCmvRYj2ty9k5OOc/uKdInd32YK6mzF0gRkSQRCQDWARtc/PpvA9eKyBTnTdhrndsmlDM1LRTVtuq0Sh8Q5G/nK9fO5UhJw6APUfWNzw92RQ+O4RtPa3JW1dRBfWsXp0fY00d5viGD3hjTDTyCI6BzgeeNMcdE5DERWQsgIpeLSAlwN/BbETnmPLYWeBzHD4u9wGPObRPKdmfbg+U6Pu8Tbl80g4UJEfznm8dpGtCv/nBxPQF+NtKmDv7k89I50QT723nnmGc0OevtNdQ0dwJwsKjO4mrUWHFpHMEY84YxJtUYM9sY833ntm8ZYzY4P99rjIk3xoQYY6KMMfP7Hfu0MWaO8+MPY3Manm37iWripwSTGDXJ6lKUG9hswnfXzqempeNj0y0PFddzyfTJFxyiC/K3c3VaDG8fK6dnmP1zxkJ9WxfdzjoOOvvnK9+jA8ZjrKunlw9O1mjbAx+TkRDBJzIT+N/3T1NQ2QR81LFyYcLFb7jfdOl0qps72V04+A3d8VTV5HgATAQOFusVva/SoB9jh4rrae7oZoXOn/c5X71uLpMC7HxnQw7GGPLKnR0rB7kR29+qtFiC/e28dmTkq1e5S1/QXz4rktyypmH181HeQ4N+jG3Pr8JuE5bM1qD3NVGhgXz52rnsLKjmraPl/TpWXjzogwPsrE6P5a2j5XT3DHzGcHz1tXTom+OvrZR9kwb9GNt+opqFCRGEB/sPvbPyOvdmzyRtahjfez2XXadqiAoJIH5K8JDH3bxgGrUtnZbPvum7ou/rxaM3ZH2TBv0Yqm/t5EiJtj3wZX52G99ZO5+z9W28dqTsvI6VF7NybiwhAXZe/9Dax0qqmjsI9LORGDWJmZGT9Iasj9KgH0M7C6oxBp0/7+OuSI7ilozpwIXnzw8U5G9nzbw43jxaTpeFwzdVTR3EhAUiIiyaGcGBojp9cMoHadCPoR351UwO8mPBjHCrS1Fj7Js3ppOVFMl1l0x1+ZibLp1GfWsX75+0bvZNdXMH0aGBgOPeQmVTB2UN7ZbVo8aGBv0YMcaw40QVS+dE42fX/8y+bmp4EM//0xJS48JcPmZFagyhgX68fsS64Zu+K3rgXB8mHb7xPZpAY6SotpXShnau1EVG1AUE+du5Zl4cbx+roLPbmuGb/kGfPs3xoNdob8i2dfbwD7/frTd2PYgG/Rjp6264JDnS4kqUJ7t5wTQa2rp4r6B63P/u7p5eals7iXEO3QT42bh0RjgHi0d3Rb/vTC07TlTzzO4id5Sp3ECDfozsOlVLdGgAs2NCrS5FebBlKdGEBflZ8vBUbUsnxkC084oeHOP0H55tGNVvGHsLHVNGNx+v9Ig2D0qDfkwYY9h9qobs5Chte6AuKtDPzrXzpvJOTvm4ty6udM6h77uiB8c4fWd3L7lljSP+untO12ITqGnpPPcQmbKWBv0YKK5to7ShnSuSdNhGDe3mBdNoau9m54nxHb6pcj4VG9P/it7ZvmGk4+sd3T0cLKrnjsXx+NmEd3WNXI+gQT8GdjmbVV2RHGVxJcobLJ0TTXiw/7gP3/Q9FRvbL+inhQcRNzlwxFfiR8820NHdy5r0WLKSInUxdA+hQT8G+h6FnxOr4/NqaAF+Nq6fP5V3jpXT2tk9bn9vX9BH9xu6EREWJUwZ8Q3ZPYWO3wQyEyNZkx5HfkUzRbqgieU06MfA7lO1ZCdH6vi8ctnti2fQ0tnDO8fG7wq4urmD0EA/ggPs521fNDOCMzWt1DiHdoZj7+lakmNCiA4NZE26o3+OXtVbT4PezYprWzlb30Z2kg7bKNdlJUYyIyKYlwdZnnCs9J9D31/fg1PDHb7p6TXsPV1LVqLj3tTMqEmkxoVq0HsADXo365s/r+PzajhsNuH2RTPYeaKKysbxaUFQ1dRx3oybPpfOCMduk2E/IZtX3kRTezdZ/SYhrE6PY09hLQ1tXRc5Uo01DXo3211YS2RIACk6Pq+G6fbFM+g18Mqh8WmJUNU8+BV9cICd9Glhw15xau9px/z5yxM/Cvo16XF09xq2OddNVtbQoHezXadqyEqMxGbT8Xk1PLNjQslIiBi34Zvqpg6iQwMGfS87KYoPTtbwpw9Ou/z19pyuZVp40Hn9+BcmRBAVEqDTLC2mQe9GJXWtlNS1cYW2PVAjdMeiGeSWNY7qgSVXtHf10NjePegVPcC/XpPKyrmx/Mcrx/juq8eGfMLVGMPewlouTzx/EoLdJqxKi2XL8UpL2zFPdBr0brTbuVpQto7PqxG6JWM6fjbhr2N8VV89yMNS/YUG+vE/n8rk00uT+MN7p3nwj/to7rjw1M+i2lYqmzq4fJCHBFenx9HY3s2+09rkzCoa9G6061QNEZP8mTuMVrVK9RcZEsDKubG8cujsmPaJ6ZtDf6GgB8fV+Ldumcfjt13Ctvwq7vr1+5TWtw26725nf5vsQYJ+eUo0AX42nX1jIQ16N9pdWKvj82rU7lw8g4rGDt4/OXYtEaqbO4HzH5a6kH+4YhZP3385Z+vauO3J9wYN+72FtURM8mfOIE38QgL9uHJ2FJtyK3T1Koto0LtJaX0bRbWtOq1Sjdqq9FgmB/nx8oGxG75x5Yq+v6tSY3jhc0to6ejmn/9y4GPdLfeeriVz1oUvctakx3GmppWTVc2jK1yNiAa9m+zW/jbKTQL97Ny0YDpvHS2n5SLj4qPRF/RRIa4FPUDa1Mn8+O4MDhXX873Xc85tr2xs53RNK1lJUy547Or0WABeP1I+worVaGjQu8muk7WEB/uTNlXH59Xo3bl4Bm1dPbx9bGyCsaq5nSmT/AnwG14E3HjpNB5cnsQfPzjD35w3jPcMMn9+oGnhwaxKi+U3205SXKu9b8abBr2b7CqsIStJx+eVe1w2awozIyexfk8x7V3u71Nf3dTp0vj8YL52fRpZSZE8+vIRjpc3srewlmB/O5fMCL/ocd+77RLsNuFrLx3RsfpxpkHvBqX1bZypaR10xoFSIyEifGrJLPacrmX5j7bwm20naWp3XxuBCz0V6wo/u41ffnIRk4P8+eyf9rOjoJrFsyLwt188TqZHBPONG9N5/2QNz+zRZQbHkwa9G/RNG1s5N9biSpQveWBZEs88mE3a1DCeePM4S5/YzE/fyaO2pXPUX/tCDc1cFRsWxJP3Lqakro1TVS0XHbbp756sBJbOieI/3zjO2QtM1VTup0HvBhtzKkiODtH+88qtRIQrZ0fzpwey2fDIUq6cHc1/by5g5Y+3UNYwupC8UEOz4bg8MZJv3JgOwLI50S4dIyI8cccCeo3hUR3CGTca9KPU2N7FrlM1XDMvzupSlA9bEB/Bb/7hMl77/DJaOnv43/dPj/hrtXR009bVc96i4CP16WVJ7Pi3q8l08YoeICFyEl+/IY0dJ6p5YV/JqGtQQ9OgH6WteVV09RgNejUuLpkRzvXzp/Ls7qIRT72sGmRR8NFIiJw07GPuzZ7FFcmRPP56DuUN49OWeSLToB+ljTkVRIUEnFusQamx9ullSTS2d/PSgZFdDQ+2KPh4s9mEH965gO4ewxNv5lpWx0ShQT8Knd29bD1eyer0WOw6rVKNk8tmTWFhQgR/eO80vSPohzPcp2LHyqyoEG5bNJ13c7Wz5VhzKehF5HoRyRORAhF5dJD3A0XkOef7u0Uk0bk9UUTaROSQ8+M37i3fWrsLa2jq6OaaeVOtLkVNMA8sS6KwuoXNxyuHfexgi4JbZUVKDE0d3cNetlANz5BBLyJ24EngBmAecI+IzBuw2wNAnTFmDvAz4If93jtpjFno/Pism+r2CBtzKgjyt7k840Apd7nhkqlMDw/i9zsLh31sdXMHNnF0yrTalXOisduEbXm6AtVYco/2GA8AABFzSURBVOWKPgsoMMacMsZ0AuuBWwfscyvwf87PXwRWS//VB3yQMYaNORUsT4khOMBudTlqgvGz27jvykQ+OFXDsdKGYR1b1dRBVGigRww3hgf7syghgu0nLh70B4rq+OoLh+nodt9TwluOV/LW0bIRDX95G1eCfgZQ3O91iXPboPsYY7qBBqCvu1eSiBwUkW0isnywv0BEHhKRfSKyr6rKO36yHz3bSFlDu862UZZZlzWTSQF2nt55eljHuWMOvTutSI3hw7MN1DhvEg/mV1sKeGF/CT/beMItf2d7Vw+PPHOAz/75ALf8cidb8yp9ek6/K0E/2I/9gf9FLrRPGTDTGLMI+BLwjIhM/tiOxjxljMk0xmTGxMS4UJL1NuaUYxNYnaZPwyprhAf7c/dl8bx6uJTKJtenKFY1d7hlDr27XJUagzGws2Dw/vs1zR1szaticpAfT20/yYGi0a9Utfl4JS2dPXxmWRINbV3c/4e9rHtqF/vP+OYqWK4EfQmQ0O91PDBwmfpz+4iIHxAO1BpjOowxNQDGmP3ASSB1tEV7gndyKrhs1hSiPOjKSE08/7g0ia7eXv78wRmXj6n2sCv6S2aEM2WSP9vyB/9t/rUjZXT3Gp6+/3KmhQfzlecP09Y5uiGcDYdKiQ4N5Os3prP5yyt57Nb5nKxq4c5fv88//2X/uaUWfYUrQb8XSBGRJBEJANYBGwbsswG4z/n5XcBmY4wRkRjnzVxEJBlIAU65p3TrFNe2cry8iWt1to2yWGJ0CKvT4vjz7iKXulwaY0bV0Gws2G3C8pQYtudXDzpe/vLBs6RPm0xmYiQ/vmsBp6pb+PHbeSP++xrbu9icV8nNC6ZhtwkBfjY+tSSR7f+2ki9fk8qm3Equ//l2Nub4ztKHQwa9c8z9EeBtIBd43hhzTEQeE5G1zt1+D0SJSAGOIZq+KZgrgCMichjHTdrPGmNq3X0S463vG0DH55UneHB5ErUtnXz5+cNDzkdvaOuiq8d4VNCDY5y+urmD3PLG87afqmrmcHE9dyxy3Ba8ck40n1oyi6ffK2TXqZoR/V0bj1XQ2d3LLRnTzts+KcCPz69O4dVHlhEbFsSDf9zHv7142K1dQ63i0jx6Y8wbxphUY8xsY8z3ndu+ZYzZ4Py83RhztzFmjjEmyxhzyrn9JWPMfGNMhjFmsTHm1bE7lfGzMaeClNhQEqNDrC5FKbKTo/j3m9J5/cMyvrj+4EXD/qM59NZPrexvRYpjivL2/PPH6f928Cw2gbULp5/b9ugNacyKmsRXXzw8ojYQGw6XMiMimMUXeJp97tQw/vbwUh6+ejYv7i/hhl/sYE+hd1+f6pOxw1Tf2sme07V6Na88ymeWJ/PvN6XzxoflfOHZC4e9J7Q/GEzs5CDSp01mW/5HD4AZY/jrobMsnRNN3OSgc9snBfjx07szKKlr4wdvDK99Qk1zBzsLqrklYzoXmwEe4Gfjq9el8cJnl2C3Cf/4hz1jsgDMeNGgH6ZXD5fS02u4br6OzyvP0hf2bx69cNj3XdHHeljQA6xIjWb/mTqanVfp+87UUVzbxu2LBs7mhszESD6zLIm/7C7iX9YfdLlt8xtHy+npNazNmD70zsBlsyL5+g1ptHT2kFvWOPQBHkqDfhh6eg2/31lIRkIEC+IvvmyaUlboH/aPPHOAzu7zw/6jzpVBgx1uqatSY+jqMXxw0jH2/teDZwn2t1/wouqr16Xx8NWzeeNoOat+so1fbDox5GycVw+VMic2lPRprq/tnJEQAcBhL27ToEE/DBtzKjhd08pDy5Mv+mufUlb6zPJk/uPmebx9rIJ7f7frvKvdquYOAuw2Jgf7WVjh4DJnRTIpwM72/Co6unt4/UgZ182PIyRw8Fr7hlfe/dJVrEqL5Web8ln90628cujsoA8/lTW0sed0LWuHGLYZaOrkIGLDAjlcMrwnkD2JBv0w/G7HKeKnBHPdfB2fV57tgWVJ/GLdQo6VNnLjL3awJc8x9u1YFDzAIy9UAvxsXDk7iu0nqthyvJKGti5uXxw/5HEJkZN48t7FPPfQFUwJCeCL6w/x5ecPf+y3mdcOlwFwi4vDNn1EhIUJEXpFPxEcKKpj35k6HliWhN8QiyAr5QluXTiDVz+/jLjJQfzjH/byw7eOU97Y5nE3YvtbkRrDmZpW/ntzAdGhgSydHTX0QU7ZyVFseGQZX7omlZcPnuW+p/fQ0PrR1MgNh0u5dEY4SSOYLZeREMGp6pbzvp430cRy0e92nGJykB9/l5kw9M5KeYjZMaH87eGl3JM1k19vPcl7BTUeHfRXpTpaoBwrbeTWhdOHfVFltwlfWJ3Czz6Rwb4ztdz5m/cprm2lsLqFD882uHwTdqCFznH6I2e986peg94FRTWtvHW0nE9mz7rgeKFSnirI385/3nEpv1i3kJAAOylxrt+IHG+zokKYFeVYmnCw2Tauun1RPH/8dDaVje3c/qv3+a+N+YjAzQMeknLVpc7JF946fKOp5YKn3yvEbhPuvzLR6lKUGrFbF87g2nlTPaI98cXcfVk8uwtrmT/9Y/0Ph2XJ7Che+tyV3P+Hvbx6uJSspEimhQeP6GtNDvJndkwIh4q984asBv0Q6ls7eW5vMbdkTGdquOdNSVNqOLxh7YRHVqXwiJu+VkpcGH99+Eq+uyGHdVmjG3bNSIhge341xhiPvJl9MTp0M4S/7C6irauHB5cnW12KUmoEYsOCePLexSxPGV0L9IUJEVQ3d1Da4HpLaE+hQX8RHd09/O/7p1meEk36tNH9GqmU8m4Z8d774JQG/UU8t7eYqqYOvZpXSpE2LYwAu02D3pecqGjiB2/ksmxONMtTdPFvpSa6QD876dMnc0iD3jc41pM8SGigH//1iQyvu/GilBobC+PD+fBsAz1etqC4Bv0gvvd6DnkVTfz07xYSG6YzbZRSDgtnRtDa2UNBZbPVpQyLBv0Ab35Yxp93FfHQiuRzT+kppRR47w1ZDfp+Supa+dpLR8iID+cr1861uhyllIdJjAphcpAfh0o06L1Sd08vX1x/iF4D/33PYgL89D+NUup8NpuQ4YWdLDXNnJ548zj7z9Tx/dsvYaaz14ZSSg2UER/B8fImr1paUIMe+NXWAn63s5D7r0zk1oUjb6SklPJ9GQkR9PQajpV6T9+bCR/0f951hh+9lcetC6fzrZvnWV2OUsrDZTg7WXpTg7MJHfSvHDrLf7xylNVpsfzk7gxsHt7VTyllvdjJQUwPD/KqB6cmbNBvPl7Bl58/TFZiJE/euxh/XTVKKeUib7shOyHTbVt+FZ/78wHSpoXxu/syCfL3/NatSinPkZEQQVFtK5VN3tHJcsIEfXtXDy/sK2btL3dy39N7iJ8SzP/9YxZhQf5Wl6aU8jJr0uMQgad3nra6FJf49MIjPb2GwupmXthfwvN7i6lr7WJObCiP3TqfOxbHE6rLAiqlRmBObChrM6bzf++f5jPLk4gO9dx1eMGHgr69q4d9p+s4Xt5IXnkTeRVN5Fc00d7Vi03g2nlT+dSSWSyZHaVNypRSo/aF1Sm8eriU3247yTdv8uwZez4T9E3t3fz973cDEB0ayNypoXwyaxZzp4ayPCWG6REjWytSKaUGMzsmlNsWzeCPH5zhweXJxE723AaIPhP00aEBPPvgFaTGhRLl4b9GKaV8wxdXp/DKoVJ+tfUk31k73+pyLshnbsaKCEtmR2nIK6XGzayoEO5aHM8ze4ooa2izupwL8pmgV0opKzyyag69vYZfbTlpdSkXpEGvlFKjkBA5ib+7PIH1e4s4W++ZV/Ua9EopNUoPXz0HQfjl5gKrSxmUS0EvIteLSJ6IFIjIo4O8Hygizznf3y0iif3e+7pze56IXOe+0pVSyjPMiAhmXVYCL+wr5vc7CymqabW6pPMMOetGROzAk8A1QAmwV0Q2GGNy+u32AFBnjJkjIuuAHwKfEJF5wDpgPjAd2CQiqcYY72nkrJRSLnhk1RwOFNXx+Gs5PP5aDqlxoaxOj2NNeiyJUSGEBvkR6Hfxdis9vQb7GDRXdGV6ZRZQYIw5BSAi64Fbgf5BfyvwHefnLwK/FMdTSbcC640xHUChiBQ4v94H7ilfKaU8Q2xYEK99fjlnalrYlFvJu7kV/M/2U/x660c3aQPsNsKC/AgNckRvR1cvHd09dHT30tHdS0Z8OC//81K31+ZK0M8Aivu9LgGyL7SPMaZbRBqAKOf2XQOO/djKHiLyEPAQwMyZM12tXSmlPM6sqBAeWJbEA8uSaGjr4v2CaiqbOmju6KapvZum9i6aO7oBCPKzE+hvI9DPRpC/nfgpY/NgpytBP9jvEcbFfVw5FmPMU8BTAJmZmR97XymlvFF4sD83XDrN6jJcuhlbAiT0ex0PlF5oHxHxA8KBWhePVUopNYZcCfq9QIqIJIlIAI6bqxsG7LMBuM/5+V3AZmOMcW5f55yVkwSkAHvcU7pSSilXDDl04xxzfwR4G7ADTxtjjonIY8A+Y8wG4PfAn5w3W2tx/DDAud/zOG7cdgMP64wbpZQaX+K48PYcmZmZZt++fVaXoZRSXkVE9htjMgd7T5+MVUopH6dBr5RSPk6DXimlfJwGvVJK+TiPuxkrIlXAmVF8iWig2k3lWM2XzgV863x86VxAz8eTuXous4wxMYO94XFBP1oisu9Cd569jS+dC/jW+fjSuYCejydzx7no0I1SSvk4DXqllPJxvhj0T1ldgBv50rmAb52PL50L6Pl4slGfi8+N0SullDqfL17RK6WU6keDXimlfJzPBP1QC5h7OhF5WkQqReRov22RIrJRRE44/5xiZY2uEpEEEdkiIrkickxEvujc7q3nEyQie0TksPN8vuvcniQiu53n85yzjbdXEBG7iBwUkdecr735XE6LyIcickhE9jm3eeX3GoCIRIjIiyJy3PlvaMloz8cngr7fAuY3APOAe5wLk3uT/wWuH7DtUeBdY0wK8K7ztTfoBr5sjEkHrgAedv7/8Nbz6QBWGWMygIXA9SJyBfBD4GfO86kDHrCwxuH6IpDb77U3nwvA1caYhf3mm3vr9xrAL4C3jDFpQAaO/0+jOx9jjNd/AEuAt/u9/jrwdavrGsF5JAJH+73OA6Y5P58G5Fld4wjP6xXgGl84H2AScADHusnVgJ9z+3nfg578gWOlt3eBVcBrOJb89MpzcdZ7GogesM0rv9eAyUAhzoky7jofn7iiZ/AFzD+2CLkXijPGlAE4/4y1uJ5hE5FEYBGwGy8+H+dQxyGgEtgInATqjTHdzl286Xvu58C/Ab3O11F477mAYx3qd0Rkv4g85Nzmrd9ryUAV8Afn0NrvRCSEUZ6PrwS9S4uQq/ElIqHAS8C/GGMara5nNIwxPcaYhTiuhrOA9MF2G9+qhk9EbgYqjTH7+28eZFePP5d+lhpjFuMYun1YRFZYXdAo+AGLgV8bYxYBLbhh2MlXgt5XFyGvEJFpAM4/Ky2ux2Ui4o8j5P9ijHnZudlrz6ePMaYe2Irj3kOEiPQtx+kt33NLgbUichpYj2P45ud457kAYIwpdf5ZCfwVxw9ib/1eKwFKjDG7na9fxBH8ozofXwl6VxYw90b9F12/D8dYt8cTEcGxjnCuMea/+r3lrecTIyIRzs+DgTU4bpBtAe5y7uYV52OM+boxJt4Yk4jj38lmY8y9eOG5AIhIiIiE9X0OXAscxUu/14wx5UCxiMx1blqNY83t0Z2P1Tcf3HgT40YgH8fY6TetrmcE9T8LlAFdOH6qP4Bj7PRd4ITzz0ir63TxXJbh+NX/CHDI+XGjF5/PAuCg83yOAt9ybk8G9gAFwAtAoNW1DvO8VgKvefO5OOs+7Pw41vdv31u/15y1LwT2Ob/f/gZMGe35aAsEpZTycb4ydKOUUuoCNOiVUsrHadArpZSP06BXSikfp0GvlFI+ToNeKaV8nAa9Ukr5uP8PBaQA6Ljk01oAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(test_md[['pos_idx1','pred_any']].groupby('pos_idx1').mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.exp(np.log(preds).mean((0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = 1 / (1 + np.exp(-(np.log(preds/(1-preds)).mean((0,1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = preds.mean((0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.13095, 0.0056 , 0.04341, 0.03077, 0.04752, 0.05518],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1280228 , 0.00678272, 0.04317398, 0.03195811, 0.04593468,\n",
       "       0.05528003], dtype=float32)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_column = np.array([a + '_' + b for a in test_md.SOPInstanceUID for b in all_ich])\n",
    "sub = pd.DataFrame({'ID': id_column, 'Label': predictions.reshape(-1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13141725332644524"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.loc[range(0,len(sub),6), 'Label'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1281835436820984"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.loc[range(0,len(sub),6), 'Label'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sub = pd.read_csv(PATH/'submission_061.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13475628267250392"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_sub.loc[range(0,len(sub),6), 'Label'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(PATH/'sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.994129260901411"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(sub.sort_values('ID').reset_index(drop=True).loc[range(0,len(sub),6), 'Label'], \n",
    "            best_sub.sort_values('ID').reset_index(drop=True).loc[range(0,len(sub),6), 'Label'])[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9944464662920349"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(sub.sort_values('ID').reset_index(drop=True).loc[range(0,len(sub),6), 'Label'], \n",
    "            best_sub.sort_values('ID').reset_index(drop=True).loc[range(0,len(sub),6), 'Label'])[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to RSNA Intracranial Hemorrhage Detection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/17.3M [00:00<?, ?B/s]\n",
      "  0%|          | 8.00k/17.3M [00:00<03:56, 76.7kB/s]\n",
      "  2%|2         | 384k/17.3M [00:00<02:43, 109kB/s]  \n",
      "  4%|4         | 752k/17.3M [00:00<01:53, 153kB/s]\n",
      "  6%|5         | 984k/17.3M [00:00<01:20, 213kB/s]\n",
      "  7%|6         | 1.19M/17.3M [00:00<00:57, 291kB/s]\n",
      "  8%|8         | 1.38M/17.3M [00:00<00:43, 383kB/s]\n",
      "  9%|9         | 1.56M/17.3M [00:02<01:29, 185kB/s]\n",
      " 11%|#1        | 1.91M/17.3M [00:03<01:02, 259kB/s]\n",
      " 13%|#2        | 2.22M/17.3M [00:03<00:44, 357kB/s]\n",
      " 15%|#4        | 2.53M/17.3M [00:03<00:31, 487kB/s]\n",
      " 16%|#6        | 2.84M/17.3M [00:03<00:23, 652kB/s]\n",
      " 18%|#8        | 3.13M/17.3M [00:03<00:17, 853kB/s]\n",
      " 20%|#9        | 3.44M/17.3M [00:03<00:13, 1.09MB/s]\n",
      " 22%|##1       | 3.72M/17.3M [00:03<00:10, 1.32MB/s]\n",
      " 23%|##3       | 4.02M/17.3M [00:03<00:08, 1.60MB/s]\n",
      " 25%|##5       | 4.34M/17.3M [00:03<00:07, 1.89MB/s]\n",
      " 27%|##6       | 4.64M/17.3M [00:03<00:06, 2.11MB/s]\n",
      " 29%|##8       | 4.95M/17.3M [00:04<00:05, 2.34MB/s]\n",
      " 31%|###1      | 5.38M/17.3M [00:04<00:04, 2.73MB/s]\n",
      " 34%|###3      | 5.79M/17.3M [00:04<00:03, 3.07MB/s]\n",
      " 36%|###6      | 6.27M/17.3M [00:04<00:03, 3.48MB/s]\n",
      " 39%|###8      | 6.72M/17.3M [00:04<00:02, 3.76MB/s]\n",
      " 41%|####1     | 7.13M/17.3M [00:04<00:02, 3.73MB/s]\n",
      " 44%|####3     | 7.52M/17.3M [00:04<00:02, 3.49MB/s]\n",
      " 46%|####5     | 7.89M/17.3M [00:04<00:02, 3.33MB/s]\n",
      " 48%|####7     | 8.23M/17.3M [00:04<00:02, 3.17MB/s]\n",
      " 50%|####9     | 8.55M/17.3M [00:05<00:02, 3.12MB/s]\n",
      " 51%|#####1    | 8.87M/17.3M [00:05<00:02, 3.09MB/s]\n",
      " 53%|#####3    | 9.17M/17.3M [00:05<00:02, 3.08MB/s]\n",
      " 55%|#####4    | 9.48M/17.3M [00:05<00:02, 3.05MB/s]\n",
      " 57%|#####6    | 9.77M/17.3M [00:05<00:02, 2.93MB/s]\n",
      " 58%|#####8    | 10.1M/17.3M [00:05<00:02, 3.05MB/s]\n",
      " 60%|######    | 10.4M/17.3M [00:05<00:02, 3.04MB/s]\n",
      " 62%|######1   | 10.7M/17.3M [00:05<00:02, 2.89MB/s]\n",
      " 64%|######3   | 11.0M/17.3M [00:05<00:02, 3.07MB/s]\n",
      " 66%|######5   | 11.3M/17.3M [00:06<00:02, 3.06MB/s]\n",
      " 67%|######7   | 11.6M/17.3M [00:06<00:01, 2.96MB/s]\n",
      " 69%|######9   | 11.9M/17.3M [00:06<00:01, 3.05MB/s]\n",
      " 71%|#######   | 12.2M/17.3M [00:06<00:01, 3.05MB/s]\n",
      " 73%|#######2  | 12.5M/17.3M [00:06<00:01, 3.02MB/s]\n",
      " 74%|#######4  | 12.8M/17.3M [00:06<00:01, 3.04MB/s]\n",
      " 76%|#######6  | 13.1M/17.3M [00:06<00:01, 3.04MB/s]\n",
      " 78%|#######7  | 13.4M/17.3M [00:06<00:01, 3.04MB/s]\n",
      " 79%|#######9  | 13.7M/17.3M [00:06<00:01, 3.00MB/s]\n",
      " 81%|########1 | 14.0M/17.3M [00:06<00:01, 3.01MB/s]\n",
      " 83%|########2 | 14.3M/17.3M [00:07<00:01, 3.02MB/s]\n",
      " 85%|########4 | 14.6M/17.3M [00:07<00:00, 2.93MB/s]\n",
      " 86%|########6 | 14.9M/17.3M [00:07<00:00, 3.03MB/s]\n",
      " 88%|########8 | 15.2M/17.3M [00:07<00:00, 3.03MB/s]\n",
      " 90%|########9 | 15.5M/17.3M [00:07<00:00, 3.00MB/s]\n",
      " 92%|#########1| 15.8M/17.3M [00:07<00:00, 3.04MB/s]\n",
      " 93%|#########3| 16.1M/17.3M [00:07<00:00, 3.03MB/s]\n",
      " 95%|#########5| 16.4M/17.3M [00:07<00:00, 3.04MB/s]\n",
      " 97%|#########6| 16.7M/17.3M [00:07<00:00, 3.03MB/s]\n",
      " 98%|#########8| 17.0M/17.3M [00:08<00:00, 3.02MB/s]\n",
      "100%|##########| 17.3M/17.3M [00:11<00:00, 1.53MB/s]\n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit rsna-intracranial-hemorrhage-detection -f C:/StudioProjects/Hemorrhage/sub.csv -m \"TPU, d161+d169+d201+s101, 32TTA, 3folds, mean, scale 1.13\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
