{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda activate torch-xla-nightly\n",
    "#export XRT_TPU_CONFIG=\"tpu_worker;0;$10.0.101.2:8470\"\n",
    "#git init\n",
    "#git remote add origin https://github.com/nosound2/RSNA-Hemorrhage\n",
    "#git pull origin master\n",
    "#git config remote.origin.push HEAD\n",
    "#gcloud config set compute/zone europe-west4-a\n",
    "#gcloud auth login\n",
    "#gcloud config set project endless-empire-239015\n",
    "#pip install kaggle\n",
    "#mkdir .kaggle\n",
    "#gsutil cp gs://recursion-double-strand/kaggle-keys/kaggle.json ~/.kaggle\n",
    "#chmod 600 /home/zahar_chikishev/.kaggle/kaggle.json\n",
    "#kaggle competitions download rsna-intracranial-hemorrhage-detection -f stage_1_train.csv\n",
    "#sudo apt install unzip\n",
    "#unzip stage_1_train.csv.zip\n",
    "#kaggle kernels output xhlulu/rsna-generate-metadata-csvs -p .\n",
    "#gsutil cp gs://rsna-hemorrhage/yuvals/* .\n",
    "\n",
    "#export XRT_TPU_CONFIG=\"tpu_worker;0;10.0.101.2:8470\"; conda activate torch-xla-nightly; jupyter notebook\n",
    "\n",
    "# 35.204.242.164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 22\n",
    "CLOUD_SINGLE = False\n",
    "MIXUP = False\n",
    "DATA_SET = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "from matplotlib import patches, patheffects\n",
    "import time\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error,log_loss\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "import pdb\n",
    "\n",
    "import scipy as sp\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "\n",
    "CLOUD = not torch.cuda.is_available()\n",
    "\n",
    "if not CLOUD:\n",
    "    torch.cuda.current_device()\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as D\n",
    "import torch.nn.functional as F\n",
    "import torch.utils as U\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "from torchvision import models as M\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if CLOUD:\n",
    "    PATH = Path('/home/zahar_chikishev')\n",
    "    PATH_WORK = Path('/home/zahar_chikishev/running')\n",
    "else:\n",
    "    PATH = Path('C:/StudioProjects/Hemorrhage')\n",
    "    PATH_WORK = Path('C:/StudioProjects/Hemorrhage/running')\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import seaborn as sn\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "\n",
    "all_ich = ['any','epidural','intraparenchymal','intraventricular','subarachnoid','subdural']\n",
    "class_weights = 6.0*np.array([2,1,1,1,1,1])/7.0\n",
    "\n",
    "if CLOUD:\n",
    "    import torch_xla\n",
    "    import torch_xla.distributed.data_parallel as dp\n",
    "    import torch_xla.utils as xu\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    \n",
    "    from typing import Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CLOUD:\n",
    "    device = xm.xla_device()\n",
    "    #device = 'cpu'\n",
    "    MAX_DEVICES = 1 if CLOUD_SINGLE else 8\n",
    "    bs = 100\n",
    "else:\n",
    "    device = 'cuda'\n",
    "    #device = 'cpu'\n",
    "    MAX_DEVICES = 1\n",
    "    bs = 10\n",
    "\n",
    "if CLOUD and (not CLOUD_SINGLE):\n",
    "    devices = xm.get_xla_supported_devices(max_devices=MAX_DEVICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2351\n",
    "\n",
    "def setSeeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "setSeeds(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_cat, cols_float = pickle.load(open(PATH_WORK/'covs','rb'))\n",
    "meta_cols = cols_cat + cols_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    filename = PATH_WORK/'indexes_file.pkl'\n",
    "    all_idx, train_ids, val_ids = pickle.load(open(filename,'rb'))\n",
    "\n",
    "    train_md = pd.read_csv(PATH_WORK/'train_md.csv').sort_values(['SeriesInstanceUID','pos_idx'])\n",
    "    train_md['img_id'] = train_md.SOPInstanceUID.str.split('_').apply(lambda x: x[1])\n",
    "\n",
    "    trn_data = train_md.loc[train_md.img_id.isin(all_idx[train_ids])].reset_index(drop=True)\n",
    "    val_data = train_md.loc[train_md.img_id.isin(all_idx[val_ids])].reset_index(drop=True)\n",
    "\n",
    "    assert len(trn_data.SeriesInstanceUID.unique()) + len(val_data.SeriesInstanceUID.unique()) \\\n",
    "        == len(train_md.SeriesInstanceUID.unique())\n",
    "\n",
    "    assert len(trn_data.PatientID.unique()) + len(val_data.PatientID.unique()) \\\n",
    "        >= len(train_md.PatientID.unique())\n",
    "\n",
    "    ids_df = pd.DataFrame(all_idx, columns = ['img_id'])\n",
    "    ids_df = ids_df.join(train_md[['img_id','SeriesInstanceUID','pos_idx']].set_index('img_id'), on = 'img_id')\n",
    "\n",
    "    assert len(ids_df.SeriesInstanceUID.unique()) == 19530\n",
    "    \n",
    "    pickle.dump((trn_data,val_data,ids_df), open(PATH_WORK/'train.post.processed.1','wb'))\n",
    "else:\n",
    "    trn_data,val_data,ids_df = pickle.load(open(PATH_WORK/'train.post.processed.1','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    test_md = pd.read_csv(PATH_WORK/'test_md.csv').sort_values(['SeriesInstanceUID','pos_idx'])\n",
    "    test_md['img_id'] = test_md.SOPInstanceUID.str.split('_').apply(lambda x: x[1])\n",
    "\n",
    "    filename = PATH_WORK/'test_indexes.pkl'\n",
    "    test_ids = pickle.load(open(filename,'rb'))\n",
    "\n",
    "    test_ids_df = pd.DataFrame(test_ids, columns = ['img_id'])\n",
    "    test_ids_df = test_ids_df.join(test_md[['img_id','SeriesInstanceUID','pos_idx']].set_index('img_id'), on = 'img_id')\n",
    "\n",
    "    assert len(test_ids_df.SeriesInstanceUID.unique()) == 2214\n",
    "    \n",
    "    pickle.dump((test_md,test_ids_df), open(PATH_WORK/'test.post.processed.1','wb'))\n",
    "else:\n",
    "    test_md,test_ids_df = pickle.load(open(PATH_WORK/'test.post.processed.1','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    print(pd.concat([test_md[meta_cols].mean(0),\n",
    "                     trn_data[meta_cols].mean(0),\n",
    "                     val_data[meta_cols].mean(0)], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. \n",
    "model_Densenet161_3_vehrsion_basic_classifier_type_features_train_split_2.pkl\n",
    "10/7/19, 4:14:13 PM UTC+3\t\n",
    "model_Densenet161_3_vehrsion_basic_classifier_type_features_test_split_2.pkl\n",
    "10/7/19, 5:05:17 PM UTC+3\t\n",
    "indexes_file.pkl\n",
    "10/7/19, 5:34:42 PM UTC+3\t\n",
    "test_indexes.pkl \n",
    "10/9/19, 6:36:35 PM UTC+3\t\n",
    "\n",
    "2. \n",
    "model_Densenet169_3_vehrsion_basic_classifier_wso_type_features_test_tta_split_2.pkl\n",
    "10/10/19, 6:31:59 PM UTC+3\t\n",
    "model_Densenet169_3_vehrsion_basic_classifier_wso_type_features_train_tta_split_2.pkl\n",
    "10/10/19, 5:56:16 PM UTC+3\t\n",
    "\n",
    "model_Densenet161_3_vehrsion_basic_classifier_wso2_type_features_test_tta_split_2.pkl\n",
    "10/10/19, 3:07:20 PM UTC+3\t\n",
    "model_Densenet161_3_vehrsion_basic_classifier_wso2_type_features_train_tta_split_2.pkl\n",
    "10/10/19, 1:36:36 PM UTC+3\t\n",
    "\n",
    "4.\n",
    "model_Densenet161_3_version_classifier_splits_type_features_test_split_0.pkl\n",
    "10/14/19, 12:53:08 PM UTC+3\t\n",
    "model_Densenet161_3_version_classifier_splits_type_features_test_split_1.pkl\n",
    "10/14/19, 2:09:27 PM UTC+3\t\n",
    "model_Densenet161_3_version_classifier_splits_type_features_test_split_2.pkl\n",
    "10/14/19, 3:17:16 PM UTC+3\t\n",
    "\n",
    "5.\n",
    "train_dedup.csv\n",
    "10/14/19, 11:39:18 AM UTC+3\t\n",
    "model_Densenet169_3_version_classifier_splits_type_features_train_tta_split_2.pkl\n",
    "10/13/19, 2:43:34 PM UTC+3\t\n",
    "model_Densenet169_3_version_classifier_splits_type_features_train_tta_split_0.pkl\n",
    "10/14/19, 6:09:28 PM UTC+3\t\n",
    "model_Densenet169_3_version_classifier_splits_type_features_train_tta_split_1.pkl\n",
    "10/14/19, 8:09:27 PM UTC+3\t\n",
    "PID_splits.pkl\n",
    "10/14/19, 11:34:06 AM UTC+3\t\n",
    "model_Densenet169_3_version_classifier_splits_type_features_test_split_0.pkl\n",
    "10/13/19, 3:34:21 PM UTC+3\t\n",
    "model_Densenet169_3_version_classifier_splits_type_features_test_split_1.pkl\n",
    "10/13/19, 4:36:25 PM UTC+3\t\n",
    "model_Densenet169_3_version_classifier_splits_type_features_test_split_2.pkl\n",
    "10/13/19, 4:05:32 PM UTC+3\t\n",
    "\n",
    "\n",
    "I finished uploading all densenet 169 folds for test and train.\n",
    "(Be aware, fold 0 gives worse scores then 2).\n",
    "I uploaded the train dataset I used (without duplicates) it is called 'train_dedup'\n",
    "Also loaded the splits data into PID_split.\n",
    "It is a tuple. The first variable is a numpy array with the unique PIDs in train_df.\n",
    "The 2nd variable is a double list, with the indices of the train and validation for the 3 splits. \n",
    "The indices are for the PID numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_SET == 1:\n",
    "    feat_sz = 2208\n",
    "elif DATA_SET == 2:\n",
    "    feat_sz = 208\n",
    "else: assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_SET == 2\n",
    "if False:\n",
    "    train_dedup = pd.read_csv(PATH_WORK/'yuval'/'train_dedup.csv')\n",
    "    pids, folding = pickle.load(open(PATH_WORK/'yuval'/'PID_splits.pkl','rb'))\n",
    "    \n",
    "    assert len(pids) == 17079\n",
    "    assert len(np.unique(pids)) == 17079\n",
    "\n",
    "    for fol in folding:\n",
    "        assert len(fol[0]) + len(fol[1]) == 17079\n",
    "\n",
    "    assert len(folding[0][1]) + len(folding[1][1]) + len(folding[2][1]) == 17079\n",
    "\n",
    "    assert len(train_dedup.PID.unique()) == 17079\n",
    "\n",
    "    train_dedup['fold'] = np.nan\n",
    "\n",
    "    for fold in range(3):\n",
    "        train_dedup.loc[train_dedup.PID.isin(pids[folding[fold][1]]),'fold'] = fold\n",
    "\n",
    "    assert train_dedup.fold.isnull().sum() == 0\n",
    "\n",
    "    train_md = pd.read_csv(PATH_WORK/'train_md.csv').sort_values(['SeriesInstanceUID','pos_idx'])\n",
    "    train_md['img_id'] = train_md.SOPInstanceUID.str.split('_').apply(lambda x: x[1])\n",
    "\n",
    "    ids_df = train_dedup[['fold','PatientID']]\n",
    "    ids_df.columns = ['fold','img_id']\n",
    "\n",
    "    ids_df = ids_df.join(train_md.set_index('img_id'), on = 'img_id')\n",
    "    \n",
    "    pickle.dump(ids_df, open(PATH_WORK/'features/densenet169_v3/train/train.ids.df','wb'))\n",
    "    \n",
    "    test_md, test_ids_df = pickle.load(open(PATH_WORK/'test.post.processed.1','rb'))\n",
    "    test_ids_df = test_ids_df[['img_id']].join(test_md.set_index('img_id'), on = 'img_id')\n",
    "    \n",
    "    pickle.dump(test_ids_df, open(PATH_WORK/'features/densenet169_v3/test/test.ids.df','wb'))\n",
    "    \n",
    "    for fold in range(3):\n",
    "        filename = PATH_WORK/'yuval'/\\\n",
    "            'model_Densenet169_3_version_classifier_splits_type_features_train_tta_split_{}.pkl'\\\n",
    "            .format(fold)\n",
    "        feats = pickle.load(open(filename,'rb'))\n",
    "        assert len(feats) == 4*len(ids_df)\n",
    "\n",
    "        for i in range(4):\n",
    "            feats_sub1 = feats[torch.BoolTensor(np.arange(len(feats))%4 == i)]\n",
    "            feats_sub2 = feats_sub1[torch.BoolTensor(train_dedup.fold != fold)]\n",
    "            pickle.dump(feats_sub2, open(PATH_WORK/'features/densenet169_v3/train/train.f{}.a{}'\n",
    "                                         .format(fold,i),'wb'))\n",
    "\n",
    "            feats_sub2 = feats_sub1[torch.BoolTensor(train_dedup.fold == fold)]\n",
    "            pickle.dump(feats_sub2, open(PATH_WORK/'features/densenet169_v3/train/valid.f{}.a{}'\n",
    "                                         .format(fold,i),'wb'))\n",
    "            \n",
    "            if i==0:\n",
    "                black_feats = feats_sub1[torch.BoolTensor(ids_df.img_id == '006d4432e')].squeeze()\n",
    "                pickle.dump(black_feats, open(PATH_WORK/'features/densenet169_v3/train/black.f{}'\n",
    "                                              .format(fold),'wb'))\n",
    "    \n",
    "    for fold in range(3):\n",
    "        filename = PATH_WORK/'yuval'/\\\n",
    "            'model_Densenet169_3_version_classifier_splits_type_features_test_split_{}.pkl'\\\n",
    "            .format(fold)\n",
    "        feats = pickle.load(open(filename,'rb'))\n",
    "\n",
    "        for i in range(8):\n",
    "            feats_sub = feats[torch.BoolTensor(np.arange(len(feats))%8 == i)]\n",
    "            pickle.dump(feats_sub, open(PATH_WORK/'features/densenet169_v3/test/test.f{}.a{}'\n",
    "                                        .format(fold,i),'wb'))\n",
    "            assert len(feats_sub) == len(test_md)\n",
    "else:\n",
    "    ids_df = pickle.load(open(PATH_WORK/'features/densenet169_v3/train/train.ids.df','rb'))\n",
    "    test_ids_df = pickle.load(open(PATH_WORK/'features/densenet169_v3/test/test.ids.df','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATA_SET == 1\n",
    "if False:\n",
    "    filename = PATH_WORK/'model_Densenet161_3_vehrsion_basic_classifier_type_features_train_split_2.pkl'\n",
    "    feats = pickle.load(open(filename,'rb'))\n",
    "\n",
    "    for series_id in tqdm(ids_df.SeriesInstanceUID.unique()):\n",
    "        mask = torch.BoolTensor(ids_df.SeriesInstanceUID.values == series_id)\n",
    "        feats_id = feats[mask]\n",
    "        pickle.dump(feats_id, open(PATH_WORK/'features/densenet161_v3/train/{}'.format(series_id),'wb'))\n",
    "\n",
    "\n",
    "    filename = PATH_WORK/'model_Densenet161_3_vehrsion_basic_classifier_type_features_test_split_2.pkl'\n",
    "    feats = pickle.load(open(filename,'rb'))\n",
    "\n",
    "    for series_id in tqdm(test_ids_df.SeriesInstanceUID.unique()):\n",
    "        mask = torch.BoolTensor(test_ids_df.SeriesInstanceUID.values == series_id)\n",
    "        feats_id = feats[mask]\n",
    "        pickle.dump(feats_id, open(PATH_WORK/'features/densenet161_v3/test/{}'.format(series_id),'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = PATH_WORK/'features/densenet161_v3/train/ID_000a935543'\n",
    "#feats1 = pickle.load(open(path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_black = '006d4432e'\n",
    "\n",
    "if DATA_SET == 1:\n",
    "    path = PATH_WORK/'features/densenet161_v3/train/ID_992b567eb6'\n",
    "    black_feats = pickle.load(open(path,'rb'))[41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSNA_DataSet(D.Dataset):\n",
    "    def __init__(self, metadata, ids_df, mode='train', bs=None, dataset=DATA_SET, fold=0, anum=0):\n",
    "        \n",
    "        super(RSNA_DataSet, self).__init__()\n",
    "        \n",
    "        if metadata is not None:\n",
    "            md = metadata.copy()\n",
    "            md = md.reset_index(drop=True)\n",
    "        else:\n",
    "            if mode == 'train':\n",
    "                md = ids_df.loc[ids_df.fold != fold].copy().reset_index(drop=True)\n",
    "            elif mode == 'valid':\n",
    "                md = ids_df.loc[ids_df.fold == fold].copy().reset_index(drop=True)\n",
    "            else:\n",
    "                md = ids_df.copy().reset_index(drop=True)\n",
    "        \n",
    "        series = md.SeriesInstanceUID.unique()\n",
    "        \n",
    "        samples_add = 0\n",
    "        if (mode != 'train') and not DATA_SMALL:\n",
    "            batch_num = -((-len(series))//(bs*MAX_DEVICES))\n",
    "            samples_add = batch_num*bs*MAX_DEVICES - len(series)\n",
    "            print('adding dummy serieses', samples_add)\n",
    "        \n",
    "        #self.records = df.to_records(index=False)\n",
    "        self.mode = mode\n",
    "        self.real = np.concatenate([np.repeat(True,len(series)),np.repeat(False,samples_add)])\n",
    "        self.series = np.concatenate([series, random.sample(list(series),samples_add)])\n",
    "        self.metadata = md\n",
    "        self.ids_df = ids_df\n",
    "        self.dataset = dataset\n",
    "        self.fold = fold\n",
    "        self.anum = anum\n",
    "        \n",
    "        print('DataSet', dataset, mode, 'size', len(self.series), 'fold', fold, 'anum', anum)\n",
    "        \n",
    "        if self.dataset == 2:\n",
    "            folder = 'test' if self.mode == 'test' else 'train'\n",
    "            path = PATH_WORK/'features/densenet169_v3/{}/{}.f{}.a{}'.format(folder,self.mode,self.fold,self.anum)\n",
    "            feats = pickle.load(open(path,'rb'))\n",
    "            self.feats = feats\n",
    "            assert len(feats) == len(md)\n",
    "            \n",
    "            path = PATH_WORK/'features/densenet169_v3/train/black.f{}'.format(fold)\n",
    "            self.black_feats = pickle.load(open(path,'rb')).squeeze()\n",
    "        elif self.dataset == 1:\n",
    "            self.black_feats = black_feats\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        series_id = self.series[index]\n",
    "        df = self.metadata.loc[self.metadata.SeriesInstanceUID == series_id].reset_index(drop=True)\n",
    "        \n",
    "        if self.dataset == 1:\n",
    "            folder = 'test' if self.mode == 'test' else 'train'\n",
    "            ids_df_sub = self.ids_df.loc[self.ids_df.SeriesInstanceUID.values == series_id]\n",
    "            \n",
    "            path = PATH_WORK/'features/densenet161_v3/{}/{}'.format(folder,series_id)\n",
    "            feats = pickle.load(open(path,'rb'))\n",
    "            \n",
    "            if feats.shape[0] > len(df):\n",
    "                mask_dup = ~ids_df_sub.img_id.duplicated().values\n",
    "                ids_df_sub = ids_df_sub.loc[mask_dup]\n",
    "                feats = feats[torch.BoolTensor(mask_dup)]\n",
    "            \n",
    "            assert feats.shape[0] == len(df)\n",
    "            assert len(ids_df_sub) == len(df)\n",
    "            assert np.all(ids_df_sub.img_id.isin(df.img_id).values)\n",
    "        elif self.dataset == 2:\n",
    "            feats = self.feats[torch.BoolTensor(self.metadata.SeriesInstanceUID.values == series_id)]\n",
    "            ids_df_sub = df\n",
    "        else: assert False\n",
    "        \n",
    "        order = np.argsort(ids_df_sub.pos_idx.values)\n",
    "        if self.dataset == 1:\n",
    "            assert np.all(ids_df_sub.img_id.values[order] == df.img_id.values)\n",
    "        feats = feats[torch.LongTensor(order)]\n",
    "        \n",
    "        feats = torch.cat([feats, torch.Tensor(df[meta_cols].values)], dim=1)\n",
    "        target = torch.Tensor(df[all_ich].values)\n",
    "        \n",
    "        PAD = 4+9\n",
    "        \n",
    "        offset = np.random.randint(0, 61 - feats.shape[0])\n",
    "        #offset = 0\n",
    "        top_pad = PAD + offset\n",
    "        if top_pad > 0:\n",
    "            dummy_row = torch.cat([self.black_feats, torch.Tensor(df.head(1)[meta_cols].values).squeeze()])\n",
    "            feats = torch.cat([dummy_row.repeat(top_pad,1), feats], dim=0)\n",
    "            if offset > 0:\n",
    "                target = torch.cat([torch.zeros((offset, len(all_ich))), target], dim=0)\n",
    "        bot_pad = 60 - len(df) - offset + PAD\n",
    "        if bot_pad > 0:\n",
    "            dummy_row = torch.cat([self.black_feats, torch.Tensor(df.tail(1)[meta_cols].values).squeeze()])\n",
    "            feats = torch.cat([feats, dummy_row.repeat(bot_pad,1)], dim=0)\n",
    "            if (60 - len(df) - offset) > 0:\n",
    "                target = torch.cat([target, torch.zeros((60 - len(df) - offset, len(all_ich)))], dim=0)\n",
    "        \n",
    "        assert feats.shape[0] == (60 + 2*PAD)\n",
    "        assert target.shape[0] == 60\n",
    "        \n",
    "        feats = feats.transpose(1,0)\n",
    "        \n",
    "        idx = index\n",
    "        if not self.real[index]: idx = -1\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            return feats, target\n",
    "        else:\n",
    "            return feats, target, idx, offset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.series) if not DATA_SMALL else int(0.01*len(self.series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCurrentBatch(fold=0):\n",
    "    sel_batch = None\n",
    "    for filename in os.listdir(PATH_WORK/'models'):\n",
    "        splits = filename.split('.')\n",
    "        if int(splits[2][1]) != fold: continue\n",
    "        if int(splits[3][1:]) != VERSION: continue\n",
    "        if sel_batch is None:\n",
    "            sel_batch = int(splits[1][1:])\n",
    "        else:\n",
    "            sel_batch = max(sel_batch, int(splits[1][1:]))\n",
    "    return sel_batch\n",
    "\n",
    "def modelFileName(fold=0, batch = 1, return_last = False, return_next = False):\n",
    "    sel_batch = batch\n",
    "    if return_last or return_next:\n",
    "        sel_batch = getCurrentBatch(fold)\n",
    "        if return_last and sel_batch is None:\n",
    "            return None\n",
    "        if return_next:\n",
    "            if sel_batch is None: sel_batch = 1\n",
    "            else: sel_batch += 1\n",
    "    \n",
    "    return 'model.b{}.f{}.v{}'.format(sel_batch, fold, VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCEWithLogitsLoss(nn.Module):\n",
    "    def __init__(self, weight=None):\n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "    \n",
    "    def forward(self, input, target, batch_weights = None):\n",
    "        loss = (torch.log(1+torch.exp(input)) - target*input)*self.weight\n",
    "        if batch_weights is not None:\n",
    "            loss = batch_weights*loss\n",
    "        return loss.mean()\n",
    "        \n",
    "        #return F.binary_cross_entropy_with_logits(input.squeeze(), target,\n",
    "        #                                          self.weight,\n",
    "        #                                          pos_weight=self.pos_weight,\n",
    "        #                                          reduction=self.reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatProduct(nn.Module):\n",
    "    def __init__(self, in_feature, out_feature):\n",
    "        super(FeatProduct, self).__init__()\n",
    "        self.in_feature = in_feature\n",
    "        self.out_feature = out_feature\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_feature, in_feature))\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_feature))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        nn.init.uniform_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = F.linear(x, self.weight) + self.bias\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_drop_lin(n_in:int, n_out:int, bn:bool=True, p:float=0., actn=None):\n",
    "    \"Sequence of batchnorm (if `bn`), dropout (with `p`) and linear (`n_in`,`n_out`) layers followed by `actn`.\"\n",
    "    layers = [nn.BatchNorm1d(n_in)] if bn else []\n",
    "    if p != 0: layers.append(nn.Dropout(p))\n",
    "    layers.append(nn.Linear(n_in, n_out))\n",
    "    if actn is not None: layers.append(actn)\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularModel(nn.Module):\n",
    "    \"Basic model for tabular data.\"\n",
    "    def __init__(self, n_cont:int, out_sz:int, layers, ps=None,\n",
    "                 emb_drop:float=0., use_bn:bool=True, bn_final:bool=False, feat_sz=2208, fc_drop_p=0.3):\n",
    "        super().__init__()\n",
    "        self.bn_cont = nn.BatchNorm1d(feat_sz + n_cont)\n",
    "        self.n_cont = n_cont\n",
    "        sizes = self.get_sizes(layers, out_sz)\n",
    "        actns = [nn.ReLU(inplace=True) for _ in range(len(sizes)-2)] + [None]\n",
    "        layers = []\n",
    "        for i,(n_in,n_out,dp,act) in enumerate(zip(sizes[:-1],sizes[1:],[0.]+ps,actns)):\n",
    "            layers += bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act)\n",
    "        if bn_final: layers.append(nn.BatchNorm1d(sizes[-1]))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.feat_product = FeatProduct(feat_sz + n_cont, 20)\n",
    "        self.fc_drop = nn.Dropout(p=fc_drop_p)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2D_1 = nn.Conv2d(1,32,(feat_sz + n_cont,1))\n",
    "        self.conv2D_2 = nn.Conv2d(1,32,(feat_sz + n_cont,5))#,padding=(0,2)\n",
    "        self.bn_cont1 = nn.BatchNorm1d(64)\n",
    "        self.conv1D_1 = nn.Conv1d(64,32,3)#,padding=1\n",
    "        self.conv1D_3 = nn.Conv1d(64,32,5,dilation=5)\n",
    "        self.conv1D_2 = nn.Conv1d(64,6,3)#,padding=1\n",
    "        self.bn_cont2 = nn.BatchNorm1d(64)\n",
    "        self.bn_cont3 = nn.BatchNorm1d(6)\n",
    "\n",
    "    def get_sizes(self, layers, out_sz):\n",
    "        return [1200] + layers + [out_sz]\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        x = self.bn_cont(x) # bs,2208,60\n",
    "        x = self.fc_drop(x)\n",
    "        x = x.reshape(x.shape[0],1,x.shape[1],x.shape[2]) # bs,1,2208,60\n",
    "        x = torch.cat([self.conv2D_1(x[:,:,:,2:(-2)]).squeeze(), \n",
    "                       self.conv2D_2(x).squeeze()], dim=1) # bs,64,60\n",
    "        x = self.relu(x)\n",
    "        x = self.bn_cont1(x)\n",
    "        x = self.fc_drop(x)\n",
    "        #x = self.conv1D_1(x)\n",
    "        x = torch.cat([self.conv1D_1(x[:,:,9:(-9)]), \n",
    "                       self.conv1D_3(x)], dim=1) # bs,64,60\n",
    "        x = self.relu(x)\n",
    "        x = self.bn_cont2(x)\n",
    "        x = self.fc_drop(x)\n",
    "        x = self.conv1D_2(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.bn_cont3(x) # bs,6,60\n",
    "        #x = self.fc_drop(x)\n",
    "        #x = self.feat_product(x)\n",
    "        #x = x.reshape(x.shape[0],-1)\n",
    "        #x = self.layers(x)\n",
    "        #x = x.reshape(x.shape[0],60,6)\n",
    "        x = x.transpose(1,2) # bs,60,6\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_fn(model, loader, device, context = None):\n",
    "    \n",
    "    if CLOUD and (not CLOUD_SINGLE):\n",
    "        tlen = len(loader._loader._loader)\n",
    "        OUT_LOSS = 1000\n",
    "        OUT_TIME = 4\n",
    "        generator = loader\n",
    "        device_num = int(str(device)[-1])\n",
    "        dataset = loader._loader._loader.dataset\n",
    "    else:\n",
    "        tlen = len(loader)\n",
    "        OUT_LOSS = 10\n",
    "        OUT_TIME = 1\n",
    "        generator = enumerate(loader)\n",
    "        device_num = 1\n",
    "        dataset = loader.dataset\n",
    "    \n",
    "    #print('Start training {}'.format(device), 'batches', tlen)\n",
    "    \n",
    "    criterion = BCEWithLogitsLoss(weight = torch.Tensor(class_weights).to(device))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(0.9, 0.99))\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    if CLOUD:\n",
    "        tracker = xm.RateTracker()\n",
    "\n",
    "    tloss = 0\n",
    "    tloss_count = 0\n",
    "    \n",
    "    st = time.time()\n",
    "    mixup_collected = False\n",
    "    for i, (x, y) in generator:\n",
    "        if (not CLOUD) or CLOUD_SINGLE:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "        \n",
    "        if MIXUP:\n",
    "            if mixup_collected:\n",
    "                lambd = np.random.beta(0.4, 0.4, y.size(0))\n",
    "                lambd = torch.Tensor(lambd).to(device)[:,None,None]\n",
    "                #shuffle = torch.randperm(y.size(0)).to(device)\n",
    "                x = lambd * x + (1-lambd) * x_mix #x[shuffle]\n",
    "                mixup_collected = False\n",
    "            else:\n",
    "                x_mix = x\n",
    "                y_mix = y\n",
    "                mixup_collected = True\n",
    "                continue\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        \n",
    "        if MIXUP:\n",
    "            loss = criterion(output, y, lambd) + criterion(output, y_mix, 1-lambd) #y[shuffle]\n",
    "            del x_mix, y_mix\n",
    "        else:\n",
    "            loss = criterion(output, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        tloss += len(y)*loss.cpu().detach().item()\n",
    "        tloss_count += len(y)\n",
    "        \n",
    "        if CLOUD or CLOUD_SINGLE:\n",
    "            xm.optimizer_step(optimizer)\n",
    "            if CLOUD_SINGLE:\n",
    "                xm.mark_step()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        \n",
    "        if CLOUD:\n",
    "            tracker.add(len(y))\n",
    "        \n",
    "        st_passed = time.time() - st\n",
    "        if (i+1)%OUT_TIME == 0 and device_num == 1:\n",
    "            #print(torch_xla._XLAC._xla_metrics_report())\n",
    "            print('Batch {} device: {} time passed: {:.3f} time per batch: {:.3f}'\n",
    "                .format(i+1, device, st_passed, st_passed/(i+1)))\n",
    "        \n",
    "        del loss, output, y, x\n",
    "    \n",
    "    return tloss, tloss_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def val_loop_fn(model, loader, device, context = None):\n",
    "    \n",
    "    if CLOUD and (not CLOUD_SINGLE):\n",
    "        tlen = len(loader._loader._loader)\n",
    "        OUT_LOSS = 1000\n",
    "        OUT_TIME = 1\n",
    "        generator = loader\n",
    "        device_num = int(str(device)[-1])\n",
    "    else:\n",
    "        tlen = len(loader)\n",
    "        OUT_LOSS = 1000\n",
    "        OUT_TIME = 10\n",
    "        generator = enumerate(loader)\n",
    "        device_num = 1\n",
    "    \n",
    "    #print('Start validating {}'.format(device), 'batches', tlen)\n",
    "    \n",
    "    st = time.time()\n",
    "    model.eval()\n",
    "    \n",
    "    results = []\n",
    "    indices = []\n",
    "    offsets = []\n",
    "    \n",
    "    for i, (x, y, idx, offset) in generator:\n",
    "        \n",
    "        if (not CLOUD) or CLOUD_SINGLE:\n",
    "            x = x.to(device)\n",
    "        \n",
    "        output = torch.sigmoid(model(x))\n",
    "        \n",
    "        mask = (idx >= 0)\n",
    "        results.append(output[mask].cpu().detach().numpy())\n",
    "        indices.append(idx[mask].cpu().detach().numpy())\n",
    "        offsets.append(offset[mask].cpu().detach().numpy())\n",
    "        \n",
    "        st_passed = time.time() - st\n",
    "        if (i+1)%OUT_TIME == 0 and device_num == 1:\n",
    "            print('Batch {} device: {} time passed: {:.3f} time per batch: {:.3f}'\n",
    "                  .format(i+1, device, st_passed, st_passed/(i+1)))\n",
    "        \n",
    "        del output, y, x, idx, offset\n",
    "    \n",
    "    results = np.concatenate(results)\n",
    "    indices = np.concatenate(indices)\n",
    "    offsets = np.concatenate(offsets)\n",
    "    \n",
    "    return results, indices, offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_loop_fn(model, loader, device, context = None):\n",
    "    \n",
    "    if CLOUD and (not CLOUD_SINGLE):\n",
    "        tlen = len(loader._loader._loader)\n",
    "        OUT_LOSS = 1000\n",
    "        OUT_TIME = 100\n",
    "        generator = loader\n",
    "        device_num = int(str(device)[-1])\n",
    "    else:\n",
    "        tlen = len(loader)\n",
    "        OUT_LOSS = 1000\n",
    "        OUT_TIME = 10\n",
    "        generator = enumerate(loader)\n",
    "        device_num = 1\n",
    "    \n",
    "    #print('Start testing {}'.format(device), 'batches', tlen)\n",
    "    \n",
    "    st = time.time()\n",
    "    model.eval()\n",
    "    \n",
    "    results = []\n",
    "    indices = []\n",
    "    offsets = []\n",
    "    \n",
    "    for i, (x, y, idx, offset) in generator:\n",
    "        \n",
    "        if (not CLOUD) or CLOUD_SINGLE:\n",
    "            x = x.to(device)\n",
    "        \n",
    "        output = torch.sigmoid(model(x))\n",
    "        \n",
    "        mask = (idx >= 0)\n",
    "        results.append(output[mask].cpu().detach().numpy())\n",
    "        indices.append(idx[mask].cpu().detach().numpy())\n",
    "        offsets.append(offset[mask].cpu().detach().numpy())\n",
    "        \n",
    "        st_passed = time.time() - st\n",
    "        if (i+1)%OUT_TIME == 0 and device_num == 1:\n",
    "            print('B{} -> time passed: {:.3f} time per batch: {:.3f}'.format(i+1, st_passed, st_passed/(i+1)))\n",
    "        \n",
    "        del output, x, y, idx, offset\n",
    "    \n",
    "    return np.concatenate(results), np.concatenate(indices), np.concatenate(offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(weight=None, load_model=True, epochs=1, bs=100):\n",
    "    \n",
    "    fold = 0\n",
    "    \n",
    "    cur_epoch = getCurrentBatch()\n",
    "    if cur_epoch is None: cur_epoch = 0\n",
    "    print('completed epochs:', cur_epoch, 'starting now:', epochs)\n",
    "    \n",
    "    setSeeds(SEED + cur_epoch)\n",
    "    \n",
    "    if DATA_SET == 1:\n",
    "        trn_ds = RSNA_DataSet(trn_data, ids_df, mode='train', bs=bs)\n",
    "        val_ds = RSNA_DataSet(val_data, ids_df, mode='valid', bs=bs)\n",
    "    elif DATA_SET == 2:\n",
    "        trn_ds = RSNA_DataSet(None, ids_df, mode='train', bs=bs)\n",
    "        val_ds = RSNA_DataSet(None, ids_df, mode='valid', bs=bs)\n",
    "    else: assert False\n",
    "    \n",
    "    loader = D.DataLoader(trn_ds, num_workers=16 if CLOUD else 0, batch_size=bs, shuffle=True, drop_last=True)\n",
    "    loader_val = D.DataLoader(val_ds, num_workers=16 if CLOUD else 0, batch_size=bs, shuffle=True)\n",
    "    #tst_ds = RSNA_DataSet(test_md, test_ids_df, mode='test')\n",
    "    print('dataset train:', len(trn_ds), 'valid:', len(val_ds), 'loader train:', len(loader), 'valid:', len(loader_val))\n",
    "    \n",
    "    model = TabularModel(n_cont = len(meta_cols), out_sz=360, layers=[500,200], ps=[0.5,0.5], bn_final=True, \n",
    "                         feat_sz=feat_sz)\n",
    "    \n",
    "    model_file_name = modelFileName(return_last=True)\n",
    "    if model_file_name is not None:\n",
    "        print('loading model', model_file_name)\n",
    "        state_dict = torch.load(PATH_WORK/'models'/model_file_name)\n",
    "        model.load_state_dict(state_dict)\n",
    "    else:\n",
    "        print('starting from scratch')\n",
    "    \n",
    "    if (not CLOUD) or CLOUD_SINGLE:\n",
    "        model = model.to(device)\n",
    "    else:\n",
    "        model_parallel = dp.DataParallel(model, device_ids=devices)\n",
    "    \n",
    "    for i in range(cur_epoch+1, cur_epoch+epochs+1):\n",
    "        st = time.time()\n",
    "\n",
    "        if CLOUD and (not CLOUD_SINGLE):\n",
    "            results = model_parallel(train_loop_fn, loader)\n",
    "            tloss, tloss_count = np.stack(results).sum(0)\n",
    "            state_dict = model_parallel._models[0].state_dict()\n",
    "        else:\n",
    "            tloss, tloss_count = train_loop_fn(model, loader, device)\n",
    "            state_dict = model.state_dict()\n",
    "        \n",
    "        state_dict = {k:v.to('cpu') for k,v in state_dict.items()}\n",
    "        tr_ll = tloss / tloss_count\n",
    "        \n",
    "        train_time = time.time()-st\n",
    "        \n",
    "        model_file_name = modelFileName(return_next=True)\n",
    "        if not DATA_SMALL:\n",
    "            torch.save(state_dict, PATH_WORK/'models'/model_file_name)\n",
    "        \n",
    "        st = time.time()\n",
    "        if CLOUD and (not CLOUD_SINGLE):\n",
    "            results = model_parallel(val_loop_fn, loader_val)\n",
    "            predictions = np.concatenate([results[i][0] for i in range(MAX_DEVICES)])\n",
    "            indices = np.concatenate([results[i][1] for i in range(MAX_DEVICES)])\n",
    "            offsets = np.concatenate([results[i][2] for i in range(MAX_DEVICES)])\n",
    "        else:\n",
    "            predictions, indices, offsets = val_loop_fn(model, loader_val, device)\n",
    "        \n",
    "        if DATA_SET == 1:\n",
    "            loc_data = val_data.copy()\n",
    "        else:\n",
    "            loc_data = ids_df.loc[ids_df.fold == fold].copy().reset_index(drop=True)\n",
    "        \n",
    "        if DATA_SMALL:\n",
    "            val_sz = int(0.01*len(loc_data.SeriesInstanceUID.unique()))\n",
    "            val_series = loc_data.SeriesInstanceUID.unique()[:val_sz]\n",
    "            loc_data = loc_data.loc[loc_data.SeriesInstanceUID.isin(val_series)]\n",
    "        \n",
    "        predictions = predictions[np.argsort(indices)]\n",
    "        offsets = offsets[np.argsort(indices)]\n",
    "        assert len(predictions) == len(loc_data.SeriesInstanceUID.unique())\n",
    "        assert len(predictions) == len(offsets)\n",
    "        assert np.all(indices[np.argsort(indices)] == np.array(range(len(predictions))))\n",
    "        \n",
    "        valid_time = time.time()-st\n",
    "        \n",
    "        val_results = np.zeros((len(loc_data),6))\n",
    "        for k, series in enumerate(loc_data.SeriesInstanceUID.unique()):\n",
    "            mask = loc_data.SeriesInstanceUID == series\n",
    "            assert (offsets[k] + mask.sum()) <= 60\n",
    "            val_results[mask] = predictions[k,offsets[k]:(offsets[k] + mask.sum())]\n",
    "        \n",
    "        lls = [log_loss(loc_data[all_ich[k]].values, val_results[:,k], eps=1e-8, labels=[0,1]) for k in range(6)]\n",
    "        ll = (class_weights * np.array(lls)).mean()\n",
    "        cor = np.corrcoef(loc_data.loc[:,all_ich].values.reshape(-1), val_results.reshape(-1))[0,1]\n",
    "\n",
    "        print('epoch {}, train ll: {:.4f}, val ll: {:.4f}, cor: {:.4f}'.format(i, tr_ll, ll, cor))\n",
    "        valid_time = time.time()-st\n",
    "\n",
    "        epoch_stats = pd.DataFrame([[i, 0, tr_ll, ll, cor, lls[0], lls[1], lls[2], lls[3], lls[4], lls[5],\n",
    "                                     len(trn_ds), len(val_ds), bs, train_time, valid_time,\n",
    "                                     learning_rate, weight_decay]], \n",
    "                                   columns = \n",
    "                                    ['epoch','fold','train_loss','val_loss','cor',\n",
    "                                     'any','epidural','intraparenchymal','intraventricular','subarachnoid','subdural',\n",
    "                                     'train_sz','val_sz','bs','train_time','valid_time','lr','wd'\n",
    "                                     ])\n",
    "\n",
    "        stats_filename = PATH_WORK/'stats.f{}.v{}'.format(0,VERSION)\n",
    "        if stats_filename.is_file():\n",
    "            epoch_stats = pd.concat([pd.read_csv(stats_filename), epoch_stats], sort=False)\n",
    "        #if not DATA_SMALL:\n",
    "        epoch_stats.to_csv(stats_filename, index=False)\n",
    "    \n",
    "    return model, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch 22 device: xla:1 time passed: 277.972 time per batch: 12.635 - 16 cores / 8 workers\n",
    "#Batch 22 device: xla:1 time passed: 209.280 time per batch: 9.513  - 16 cores / 16 workers\n",
    "#Batch 22 device: xla:1 time passed: 213.209 time per batch: 9.691  - 16 cores / 32 workers\n",
    "#Batch 22 device: xla:1 time passed: 275.780 time per batch: 12.535 - 16 cores / 8 workers\n",
    "#Batch 22 device: xla:1 time passed: 208.826 time per batch: 9.492  - 16 cores / 16 workers\n",
    "#Batch 22 device: xla:1 time passed: 245.750 time per batch: 11.170 - 16 cores / 12 workers\n",
    "#Batch 22 device: xla:1 time passed: 374.876 time per batch: 17.040 - 16 cores / 8 workers\n",
    "#Batch 22 device: xla:1 time passed: 400.221 time per batch: 18.192 - 8 cores / 8 workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10x0.02, 3x0.002, 6x0.0002 (+10x0.0002)\n",
    "# back to orig best\n",
    "# epoch 19, train ll: 0.0389, val ll: 0.0602, cor: 0.8424\n",
    "# epoch 29, train ll: 0.0378, val ll: 0.0593, cor: 0.8448\n",
    "\n",
    "# 3x0.05, 5x0.02, 3x0.002, 6x0.0002\n",
    "# reduced all sizes\n",
    "# epoch 17, train ll: 0.0407, val ll: 0.0620, cor: 0.8371\n",
    "\n",
    "# 3x0.05, 5x0.02, 3x0.002, 6x0.0002\n",
    "# 32 to 64 for conv2D reduce before, w dilated\n",
    "# epoch 17, train ll: 0.0379, val ll: 0.0589, cor: 0.8460\n",
    "\n",
    "# 3x0.1, 5x0.02, 3x0.002, 6x0.0002\n",
    "# dilated, 3xTTA\n",
    "# epoch 17, train ll: 0.0364, val ll: 0.0582, cor: 0.8454, LB 0.066\n",
    "\n",
    "# train ll: 0.0354, val ll: 0.0577, cor: 0.8462, LB 0.065"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-cycle\n",
    "# copy latest model to GS code\n",
    "# add performance tracking, search for bottlenecks\n",
    "# smaller bs\n",
    "# keep the data in memory\n",
    "# improve black image meta data\n",
    "# freeze bias approach?\n",
    "# pseudo-labelling?\n",
    "# normalize metadata outliers\n",
    "# try GCP fast guide connecting\n",
    "\n",
    "# Yuval: zoom in, squish, perspective wraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epochs: 0 starting now: 3\n",
      "DataSet train 2\n",
      "DataSet valid 2\n",
      "dataset train: 130 valid: 64 loader train: 13 valid: 7\n",
      "starting from scratch\n",
      "Batch 2 device: cuda time passed: 2.394 time per batch: 1.197\n",
      "Batch 4 device: cuda time passed: 5.252 time per batch: 1.313\n",
      "Batch 6 device: cuda time passed: 7.909 time per batch: 1.318\n",
      "Batch 8 device: cuda time passed: 10.715 time per batch: 1.339\n",
      "Batch 10 device: cuda time passed: 13.676 time per batch: 1.368\n",
      "Batch 12 device: cuda time passed: 16.369 time per batch: 1.364\n",
      "epoch 1, train ll: 0.7019, val ll: 0.7032, cor: 0.1309\n",
      "Batch 2 device: cuda time passed: 2.965 time per batch: 1.483\n",
      "Batch 4 device: cuda time passed: 5.636 time per batch: 1.409\n",
      "Batch 6 device: cuda time passed: 8.233 time per batch: 1.372\n",
      "Batch 8 device: cuda time passed: 10.772 time per batch: 1.347\n",
      "Batch 10 device: cuda time passed: 13.325 time per batch: 1.332\n",
      "Batch 12 device: cuda time passed: 15.908 time per batch: 1.326\n",
      "epoch 2, train ll: 0.6679, val ll: 0.6908, cor: 0.3522\n",
      "Batch 2 device: cuda time passed: 2.459 time per batch: 1.229\n",
      "Batch 4 device: cuda time passed: 5.055 time per batch: 1.264\n",
      "Batch 6 device: cuda time passed: 7.520 time per batch: 1.253\n",
      "Batch 8 device: cuda time passed: 10.021 time per batch: 1.253\n",
      "Batch 10 device: cuda time passed: 13.097 time per batch: 1.310\n",
      "Batch 12 device: cuda time passed: 15.881 time per batch: 1.323\n",
      "epoch 3, train ll: 0.6346, val ll: 0.6474, cor: 0.3811\n"
     ]
    }
   ],
   "source": [
    "DATA_SMALL = True\n",
    "MIXUP = True\n",
    "learning_rate = 0.002\n",
    "weight_decay = 1e-3\n",
    "model, predictions = train_one(epochs=3, bs=bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>fold</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>cor</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "      <th>train_sz</th>\n",
       "      <th>val_sz</th>\n",
       "      <th>bs</th>\n",
       "      <th>train_time</th>\n",
       "      <th>valid_time</th>\n",
       "      <th>lr</th>\n",
       "      <th>wd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.268860</td>\n",
       "      <td>0.143166</td>\n",
       "      <td>0.613321</td>\n",
       "      <td>0.163072</td>\n",
       "      <td>0.043452</td>\n",
       "      <td>0.119038</td>\n",
       "      <td>0.088011</td>\n",
       "      <td>0.204062</td>\n",
       "      <td>0.221455</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>366.598575</td>\n",
       "      <td>55.069581</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.163124</td>\n",
       "      <td>0.621400</td>\n",
       "      <td>0.279981</td>\n",
       "      <td>1.371310</td>\n",
       "      <td>0.041597</td>\n",
       "      <td>0.409983</td>\n",
       "      <td>0.280379</td>\n",
       "      <td>0.337232</td>\n",
       "      <td>0.537986</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>157.264272</td>\n",
       "      <td>29.822798</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.121807</td>\n",
       "      <td>0.216327</td>\n",
       "      <td>0.506429</td>\n",
       "      <td>0.331250</td>\n",
       "      <td>0.025227</td>\n",
       "      <td>0.253565</td>\n",
       "      <td>0.086670</td>\n",
       "      <td>0.267595</td>\n",
       "      <td>0.218730</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>156.105009</td>\n",
       "      <td>26.639628</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.055151</td>\n",
       "      <td>0.159198</td>\n",
       "      <td>0.640255</td>\n",
       "      <td>0.259493</td>\n",
       "      <td>0.016848</td>\n",
       "      <td>0.186059</td>\n",
       "      <td>0.071142</td>\n",
       "      <td>0.168241</td>\n",
       "      <td>0.153109</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>158.559280</td>\n",
       "      <td>26.512727</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046670</td>\n",
       "      <td>0.132980</td>\n",
       "      <td>0.701180</td>\n",
       "      <td>0.218808</td>\n",
       "      <td>0.015350</td>\n",
       "      <td>0.134875</td>\n",
       "      <td>0.061654</td>\n",
       "      <td>0.129225</td>\n",
       "      <td>0.152139</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>159.042082</td>\n",
       "      <td>28.155369</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046544</td>\n",
       "      <td>0.081856</td>\n",
       "      <td>0.793531</td>\n",
       "      <td>0.133274</td>\n",
       "      <td>0.015343</td>\n",
       "      <td>0.065370</td>\n",
       "      <td>0.044277</td>\n",
       "      <td>0.083408</td>\n",
       "      <td>0.098045</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>160.890759</td>\n",
       "      <td>28.551072</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042350</td>\n",
       "      <td>0.076822</td>\n",
       "      <td>0.802252</td>\n",
       "      <td>0.121926</td>\n",
       "      <td>0.014430</td>\n",
       "      <td>0.076024</td>\n",
       "      <td>0.038554</td>\n",
       "      <td>0.068125</td>\n",
       "      <td>0.096769</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>157.371435</td>\n",
       "      <td>28.277143</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.041801</td>\n",
       "      <td>0.068393</td>\n",
       "      <td>0.822944</td>\n",
       "      <td>0.109251</td>\n",
       "      <td>0.014549</td>\n",
       "      <td>0.058177</td>\n",
       "      <td>0.035710</td>\n",
       "      <td>0.066958</td>\n",
       "      <td>0.084854</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>158.508394</td>\n",
       "      <td>26.840242</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038437</td>\n",
       "      <td>0.060963</td>\n",
       "      <td>0.838129</td>\n",
       "      <td>0.098239</td>\n",
       "      <td>0.014021</td>\n",
       "      <td>0.046781</td>\n",
       "      <td>0.031096</td>\n",
       "      <td>0.059095</td>\n",
       "      <td>0.079272</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>159.288100</td>\n",
       "      <td>26.728020</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037768</td>\n",
       "      <td>0.059423</td>\n",
       "      <td>0.841837</td>\n",
       "      <td>0.096557</td>\n",
       "      <td>0.013188</td>\n",
       "      <td>0.045602</td>\n",
       "      <td>0.030030</td>\n",
       "      <td>0.058118</td>\n",
       "      <td>0.075906</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>159.072762</td>\n",
       "      <td>28.434797</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037344</td>\n",
       "      <td>0.059671</td>\n",
       "      <td>0.842345</td>\n",
       "      <td>0.098072</td>\n",
       "      <td>0.012431</td>\n",
       "      <td>0.045787</td>\n",
       "      <td>0.029444</td>\n",
       "      <td>0.057719</td>\n",
       "      <td>0.076169</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>158.250142</td>\n",
       "      <td>27.065008</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036959</td>\n",
       "      <td>0.058352</td>\n",
       "      <td>0.844836</td>\n",
       "      <td>0.095667</td>\n",
       "      <td>0.012542</td>\n",
       "      <td>0.045106</td>\n",
       "      <td>0.028727</td>\n",
       "      <td>0.056887</td>\n",
       "      <td>0.073870</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>159.215915</td>\n",
       "      <td>26.805205</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036706</td>\n",
       "      <td>0.058233</td>\n",
       "      <td>0.845160</td>\n",
       "      <td>0.095445</td>\n",
       "      <td>0.012548</td>\n",
       "      <td>0.045051</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.056732</td>\n",
       "      <td>0.073839</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>157.341968</td>\n",
       "      <td>26.664258</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036681</td>\n",
       "      <td>0.058124</td>\n",
       "      <td>0.845519</td>\n",
       "      <td>0.095233</td>\n",
       "      <td>0.012649</td>\n",
       "      <td>0.044803</td>\n",
       "      <td>0.028464</td>\n",
       "      <td>0.056771</td>\n",
       "      <td>0.073717</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>159.055664</td>\n",
       "      <td>27.114434</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036489</td>\n",
       "      <td>0.058256</td>\n",
       "      <td>0.845202</td>\n",
       "      <td>0.095650</td>\n",
       "      <td>0.012617</td>\n",
       "      <td>0.044774</td>\n",
       "      <td>0.028461</td>\n",
       "      <td>0.056715</td>\n",
       "      <td>0.073924</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>158.027945</td>\n",
       "      <td>27.053142</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036589</td>\n",
       "      <td>0.058325</td>\n",
       "      <td>0.845258</td>\n",
       "      <td>0.095791</td>\n",
       "      <td>0.012605</td>\n",
       "      <td>0.044762</td>\n",
       "      <td>0.028428</td>\n",
       "      <td>0.056738</td>\n",
       "      <td>0.074158</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>156.406418</td>\n",
       "      <td>28.478744</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036371</td>\n",
       "      <td>0.058204</td>\n",
       "      <td>0.845376</td>\n",
       "      <td>0.095510</td>\n",
       "      <td>0.012744</td>\n",
       "      <td>0.044829</td>\n",
       "      <td>0.028270</td>\n",
       "      <td>0.056709</td>\n",
       "      <td>0.073857</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>157.756094</td>\n",
       "      <td>28.209587</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  fold  train_loss  val_loss       cor       any  epidural  \\\n",
       "0       1     0    0.268860  0.143166  0.613321  0.163072  0.043452   \n",
       "1       2     0    0.163124  0.621400  0.279981  1.371310  0.041597   \n",
       "2       3     0    0.121807  0.216327  0.506429  0.331250  0.025227   \n",
       "3       4     0    0.055151  0.159198  0.640255  0.259493  0.016848   \n",
       "4       5     0    0.046670  0.132980  0.701180  0.218808  0.015350   \n",
       "5       6     0    0.046544  0.081856  0.793531  0.133274  0.015343   \n",
       "6       7     0    0.042350  0.076822  0.802252  0.121926  0.014430   \n",
       "7       8     0    0.041801  0.068393  0.822944  0.109251  0.014549   \n",
       "8       9     0    0.038437  0.060963  0.838129  0.098239  0.014021   \n",
       "9      10     0    0.037768  0.059423  0.841837  0.096557  0.013188   \n",
       "10     11     0    0.037344  0.059671  0.842345  0.098072  0.012431   \n",
       "11     12     0    0.036959  0.058352  0.844836  0.095667  0.012542   \n",
       "12     13     0    0.036706  0.058233  0.845160  0.095445  0.012548   \n",
       "13     14     0    0.036681  0.058124  0.845519  0.095233  0.012649   \n",
       "14     15     0    0.036489  0.058256  0.845202  0.095650  0.012617   \n",
       "15     16     0    0.036589  0.058325  0.845258  0.095791  0.012605   \n",
       "16     17     0    0.036371  0.058204  0.845376  0.095510  0.012744   \n",
       "\n",
       "    intraparenchymal  intraventricular  subarachnoid  subdural  train_sz  \\\n",
       "0           0.119038          0.088011      0.204062  0.221455     17577   \n",
       "1           0.409983          0.280379      0.337232  0.537986     17577   \n",
       "2           0.253565          0.086670      0.267595  0.218730     17577   \n",
       "3           0.186059          0.071142      0.168241  0.153109     17577   \n",
       "4           0.134875          0.061654      0.129225  0.152139     17577   \n",
       "5           0.065370          0.044277      0.083408  0.098045     17577   \n",
       "6           0.076024          0.038554      0.068125  0.096769     17577   \n",
       "7           0.058177          0.035710      0.066958  0.084854     17577   \n",
       "8           0.046781          0.031096      0.059095  0.079272     17577   \n",
       "9           0.045602          0.030030      0.058118  0.075906     17577   \n",
       "10          0.045787          0.029444      0.057719  0.076169     17577   \n",
       "11          0.045106          0.028727      0.056887  0.073870     17577   \n",
       "12          0.045051          0.028569      0.056732  0.073839     17577   \n",
       "13          0.044803          0.028464      0.056771  0.073717     17577   \n",
       "14          0.044774          0.028461      0.056715  0.073924     17577   \n",
       "15          0.044762          0.028428      0.056738  0.074158     17577   \n",
       "16          0.044829          0.028270      0.056709  0.073857     17577   \n",
       "\n",
       "    val_sz   bs  train_time  valid_time      lr      wd  \n",
       "0     2400  100  366.598575   55.069581  0.1000  0.0001  \n",
       "1     2400  100  157.264272   29.822798  0.1000  0.0001  \n",
       "2     2400  100  156.105009   26.639628  0.1000  0.0001  \n",
       "3     2400  100  158.559280   26.512727  0.0200  0.0001  \n",
       "4     2400  100  159.042082   28.155369  0.0200  0.0001  \n",
       "5     2400  100  160.890759   28.551072  0.0200  0.0001  \n",
       "6     2400  100  157.371435   28.277143  0.0200  0.0001  \n",
       "7     2400  100  158.508394   26.840242  0.0200  0.0001  \n",
       "8     2400  100  159.288100   26.728020  0.0020  0.0001  \n",
       "9     2400  100  159.072762   28.434797  0.0020  0.0001  \n",
       "10    2400  100  158.250142   27.065008  0.0020  0.0001  \n",
       "11    2400  100  159.215915   26.805205  0.0002  0.0001  \n",
       "12    2400  100  157.341968   26.664258  0.0002  0.0001  \n",
       "13    2400  100  159.055664   27.114434  0.0002  0.0001  \n",
       "14    2400  100  158.027945   27.053142  0.0002  0.0001  \n",
       "15    2400  100  156.406418   28.478744  0.0002  0.0001  \n",
       "16    2400  100  157.756094   28.209587  0.0002  0.0001  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(PATH_WORK/'stats.f{}.v{}'.format(0,VERSION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>fold</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>cor</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "      <th>train_sz</th>\n",
       "      <th>val_sz</th>\n",
       "      <th>bs</th>\n",
       "      <th>train_time</th>\n",
       "      <th>valid_time</th>\n",
       "      <th>lr</th>\n",
       "      <th>wd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.245414</td>\n",
       "      <td>0.368845</td>\n",
       "      <td>0.457550</td>\n",
       "      <td>0.514073</td>\n",
       "      <td>0.044114</td>\n",
       "      <td>0.229571</td>\n",
       "      <td>0.092236</td>\n",
       "      <td>0.201881</td>\n",
       "      <td>0.985966</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>160.251015</td>\n",
       "      <td>31.356178</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.111711</td>\n",
       "      <td>0.425409</td>\n",
       "      <td>0.517751</td>\n",
       "      <td>0.835668</td>\n",
       "      <td>0.052879</td>\n",
       "      <td>0.300094</td>\n",
       "      <td>0.288142</td>\n",
       "      <td>0.345089</td>\n",
       "      <td>0.320320</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.015386</td>\n",
       "      <td>29.556167</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.092080</td>\n",
       "      <td>0.172839</td>\n",
       "      <td>0.584343</td>\n",
       "      <td>0.221234</td>\n",
       "      <td>0.019638</td>\n",
       "      <td>0.128909</td>\n",
       "      <td>0.129850</td>\n",
       "      <td>0.149150</td>\n",
       "      <td>0.339859</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>154.879500</td>\n",
       "      <td>27.140476</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.113210</td>\n",
       "      <td>0.566921</td>\n",
       "      <td>0.309114</td>\n",
       "      <td>1.084074</td>\n",
       "      <td>0.027198</td>\n",
       "      <td>0.492600</td>\n",
       "      <td>0.448893</td>\n",
       "      <td>0.417427</td>\n",
       "      <td>0.414179</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>156.678311</td>\n",
       "      <td>28.908540</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.142060</td>\n",
       "      <td>0.122733</td>\n",
       "      <td>0.687616</td>\n",
       "      <td>0.178393</td>\n",
       "      <td>0.048400</td>\n",
       "      <td>0.098498</td>\n",
       "      <td>0.110666</td>\n",
       "      <td>0.111207</td>\n",
       "      <td>0.133571</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>154.641192</td>\n",
       "      <td>26.106112</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.165164</td>\n",
       "      <td>0.171820</td>\n",
       "      <td>0.681821</td>\n",
       "      <td>0.330669</td>\n",
       "      <td>0.017591</td>\n",
       "      <td>0.091962</td>\n",
       "      <td>0.113049</td>\n",
       "      <td>0.157081</td>\n",
       "      <td>0.161722</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>154.669446</td>\n",
       "      <td>25.977148</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.108488</td>\n",
       "      <td>0.175178</td>\n",
       "      <td>0.442986</td>\n",
       "      <td>0.314731</td>\n",
       "      <td>0.015936</td>\n",
       "      <td>0.149549</td>\n",
       "      <td>0.089253</td>\n",
       "      <td>0.148018</td>\n",
       "      <td>0.194026</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.439858</td>\n",
       "      <td>25.644034</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046603</td>\n",
       "      <td>0.110731</td>\n",
       "      <td>0.735936</td>\n",
       "      <td>0.190573</td>\n",
       "      <td>0.015797</td>\n",
       "      <td>0.076381</td>\n",
       "      <td>0.056579</td>\n",
       "      <td>0.118125</td>\n",
       "      <td>0.127090</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.412679</td>\n",
       "      <td>26.197765</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.043568</td>\n",
       "      <td>0.075700</td>\n",
       "      <td>0.808988</td>\n",
       "      <td>0.121208</td>\n",
       "      <td>0.015132</td>\n",
       "      <td>0.055995</td>\n",
       "      <td>0.040890</td>\n",
       "      <td>0.080520</td>\n",
       "      <td>0.094944</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>154.755002</td>\n",
       "      <td>26.382353</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.041196</td>\n",
       "      <td>0.074663</td>\n",
       "      <td>0.810396</td>\n",
       "      <td>0.121195</td>\n",
       "      <td>0.014108</td>\n",
       "      <td>0.056875</td>\n",
       "      <td>0.035701</td>\n",
       "      <td>0.075549</td>\n",
       "      <td>0.098022</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>154.682014</td>\n",
       "      <td>26.325155</td>\n",
       "      <td>0.0100</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038925</td>\n",
       "      <td>0.062498</td>\n",
       "      <td>0.835334</td>\n",
       "      <td>0.101864</td>\n",
       "      <td>0.013212</td>\n",
       "      <td>0.046009</td>\n",
       "      <td>0.030019</td>\n",
       "      <td>0.062721</td>\n",
       "      <td>0.081797</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.851308</td>\n",
       "      <td>26.180528</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038245</td>\n",
       "      <td>0.060230</td>\n",
       "      <td>0.840434</td>\n",
       "      <td>0.098347</td>\n",
       "      <td>0.012866</td>\n",
       "      <td>0.044839</td>\n",
       "      <td>0.029394</td>\n",
       "      <td>0.059918</td>\n",
       "      <td>0.077898</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>154.850655</td>\n",
       "      <td>26.117462</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037875</td>\n",
       "      <td>0.059834</td>\n",
       "      <td>0.841174</td>\n",
       "      <td>0.098087</td>\n",
       "      <td>0.012779</td>\n",
       "      <td>0.044849</td>\n",
       "      <td>0.029316</td>\n",
       "      <td>0.059304</td>\n",
       "      <td>0.076416</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>384.280921</td>\n",
       "      <td>51.336847</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037421</td>\n",
       "      <td>0.059395</td>\n",
       "      <td>0.842651</td>\n",
       "      <td>0.097142</td>\n",
       "      <td>0.012808</td>\n",
       "      <td>0.044485</td>\n",
       "      <td>0.029056</td>\n",
       "      <td>0.058803</td>\n",
       "      <td>0.076331</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.777497</td>\n",
       "      <td>29.887910</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037143</td>\n",
       "      <td>0.058922</td>\n",
       "      <td>0.844032</td>\n",
       "      <td>0.096749</td>\n",
       "      <td>0.012895</td>\n",
       "      <td>0.043866</td>\n",
       "      <td>0.028514</td>\n",
       "      <td>0.058257</td>\n",
       "      <td>0.075425</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.482482</td>\n",
       "      <td>25.653671</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.063199</td>\n",
       "      <td>0.062034</td>\n",
       "      <td>0.837447</td>\n",
       "      <td>0.101087</td>\n",
       "      <td>0.012745</td>\n",
       "      <td>0.047637</td>\n",
       "      <td>0.032370</td>\n",
       "      <td>0.061437</td>\n",
       "      <td>0.077872</td>\n",
       "      <td>17577</td>\n",
       "      <td>2000</td>\n",
       "      <td>100</td>\n",
       "      <td>169.736084</td>\n",
       "      <td>29.725267</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.061176</td>\n",
       "      <td>0.060760</td>\n",
       "      <td>0.838443</td>\n",
       "      <td>0.099033</td>\n",
       "      <td>0.012716</td>\n",
       "      <td>0.045007</td>\n",
       "      <td>0.033106</td>\n",
       "      <td>0.058764</td>\n",
       "      <td>0.077659</td>\n",
       "      <td>17577</td>\n",
       "      <td>2000</td>\n",
       "      <td>100</td>\n",
       "      <td>158.700440</td>\n",
       "      <td>24.708474</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.060958</td>\n",
       "      <td>0.062885</td>\n",
       "      <td>0.841032</td>\n",
       "      <td>0.104076</td>\n",
       "      <td>0.012513</td>\n",
       "      <td>0.046202</td>\n",
       "      <td>0.032725</td>\n",
       "      <td>0.061478</td>\n",
       "      <td>0.079124</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>153.454062</td>\n",
       "      <td>28.075624</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0.060142</td>\n",
       "      <td>0.062561</td>\n",
       "      <td>0.840940</td>\n",
       "      <td>0.103543</td>\n",
       "      <td>0.012825</td>\n",
       "      <td>0.046356</td>\n",
       "      <td>0.033186</td>\n",
       "      <td>0.060847</td>\n",
       "      <td>0.077625</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>154.643816</td>\n",
       "      <td>26.342926</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.060683</td>\n",
       "      <td>0.063580</td>\n",
       "      <td>0.840973</td>\n",
       "      <td>0.106396</td>\n",
       "      <td>0.013263</td>\n",
       "      <td>0.045586</td>\n",
       "      <td>0.031670</td>\n",
       "      <td>0.060354</td>\n",
       "      <td>0.081400</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>154.788163</td>\n",
       "      <td>26.137916</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.060733</td>\n",
       "      <td>0.060299</td>\n",
       "      <td>0.842699</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.012535</td>\n",
       "      <td>0.044882</td>\n",
       "      <td>0.030581</td>\n",
       "      <td>0.057817</td>\n",
       "      <td>0.077322</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>154.736595</td>\n",
       "      <td>29.023877</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0.060171</td>\n",
       "      <td>0.062357</td>\n",
       "      <td>0.839801</td>\n",
       "      <td>0.102981</td>\n",
       "      <td>0.012760</td>\n",
       "      <td>0.045732</td>\n",
       "      <td>0.033905</td>\n",
       "      <td>0.060712</td>\n",
       "      <td>0.077428</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>154.789902</td>\n",
       "      <td>26.088672</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>0.060118</td>\n",
       "      <td>0.060522</td>\n",
       "      <td>0.841712</td>\n",
       "      <td>0.099508</td>\n",
       "      <td>0.012758</td>\n",
       "      <td>0.044703</td>\n",
       "      <td>0.030725</td>\n",
       "      <td>0.060239</td>\n",
       "      <td>0.076210</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.973188</td>\n",
       "      <td>26.107710</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>0</td>\n",
       "      <td>0.060056</td>\n",
       "      <td>0.060459</td>\n",
       "      <td>0.843309</td>\n",
       "      <td>0.099756</td>\n",
       "      <td>0.012853</td>\n",
       "      <td>0.045013</td>\n",
       "      <td>0.030690</td>\n",
       "      <td>0.059043</td>\n",
       "      <td>0.076100</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>154.747613</td>\n",
       "      <td>26.102819</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.060182</td>\n",
       "      <td>0.060651</td>\n",
       "      <td>0.843138</td>\n",
       "      <td>0.100404</td>\n",
       "      <td>0.012770</td>\n",
       "      <td>0.045022</td>\n",
       "      <td>0.030943</td>\n",
       "      <td>0.058453</td>\n",
       "      <td>0.076564</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.069272</td>\n",
       "      <td>28.767607</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0.058318</td>\n",
       "      <td>0.059742</td>\n",
       "      <td>0.843963</td>\n",
       "      <td>0.098442</td>\n",
       "      <td>0.012746</td>\n",
       "      <td>0.044305</td>\n",
       "      <td>0.030169</td>\n",
       "      <td>0.058106</td>\n",
       "      <td>0.075987</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.616217</td>\n",
       "      <td>26.113737</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0.058986</td>\n",
       "      <td>0.059739</td>\n",
       "      <td>0.843590</td>\n",
       "      <td>0.098584</td>\n",
       "      <td>0.012919</td>\n",
       "      <td>0.044461</td>\n",
       "      <td>0.029744</td>\n",
       "      <td>0.058144</td>\n",
       "      <td>0.075737</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>154.627858</td>\n",
       "      <td>26.179941</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>0</td>\n",
       "      <td>0.059261</td>\n",
       "      <td>0.059380</td>\n",
       "      <td>0.844253</td>\n",
       "      <td>0.097980</td>\n",
       "      <td>0.012621</td>\n",
       "      <td>0.044263</td>\n",
       "      <td>0.029754</td>\n",
       "      <td>0.057572</td>\n",
       "      <td>0.075489</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.360457</td>\n",
       "      <td>26.128672</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>0.059228</td>\n",
       "      <td>0.059909</td>\n",
       "      <td>0.842914</td>\n",
       "      <td>0.098742</td>\n",
       "      <td>0.012802</td>\n",
       "      <td>0.044614</td>\n",
       "      <td>0.029972</td>\n",
       "      <td>0.058185</td>\n",
       "      <td>0.076303</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.530201</td>\n",
       "      <td>27.551405</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037146</td>\n",
       "      <td>0.059396</td>\n",
       "      <td>0.843380</td>\n",
       "      <td>0.098268</td>\n",
       "      <td>0.012643</td>\n",
       "      <td>0.044284</td>\n",
       "      <td>0.028572</td>\n",
       "      <td>0.057151</td>\n",
       "      <td>0.076586</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>154.486103</td>\n",
       "      <td>26.230717</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036045</td>\n",
       "      <td>0.057565</td>\n",
       "      <td>0.846713</td>\n",
       "      <td>0.094500</td>\n",
       "      <td>0.012647</td>\n",
       "      <td>0.043381</td>\n",
       "      <td>0.027538</td>\n",
       "      <td>0.056336</td>\n",
       "      <td>0.074055</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.204885</td>\n",
       "      <td>26.453111</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>32</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035795</td>\n",
       "      <td>0.057748</td>\n",
       "      <td>0.846156</td>\n",
       "      <td>0.094808</td>\n",
       "      <td>0.013045</td>\n",
       "      <td>0.043539</td>\n",
       "      <td>0.027406</td>\n",
       "      <td>0.056319</td>\n",
       "      <td>0.074310</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.244213</td>\n",
       "      <td>27.856316</td>\n",
       "      <td>0.0005</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>33</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035666</td>\n",
       "      <td>0.057728</td>\n",
       "      <td>0.846278</td>\n",
       "      <td>0.095015</td>\n",
       "      <td>0.012587</td>\n",
       "      <td>0.043213</td>\n",
       "      <td>0.027224</td>\n",
       "      <td>0.056258</td>\n",
       "      <td>0.074786</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.400090</td>\n",
       "      <td>26.334645</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>34</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035420</td>\n",
       "      <td>0.057729</td>\n",
       "      <td>0.846230</td>\n",
       "      <td>0.094790</td>\n",
       "      <td>0.012786</td>\n",
       "      <td>0.043387</td>\n",
       "      <td>0.027166</td>\n",
       "      <td>0.056169</td>\n",
       "      <td>0.075013</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>155.614933</td>\n",
       "      <td>26.507466</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  fold  train_loss  val_loss       cor       any  epidural  \\\n",
       "0       1     0    0.245414  0.368845  0.457550  0.514073  0.044114   \n",
       "1       2     0    0.111711  0.425409  0.517751  0.835668  0.052879   \n",
       "2       3     0    0.092080  0.172839  0.584343  0.221234  0.019638   \n",
       "3       4     0    0.113210  0.566921  0.309114  1.084074  0.027198   \n",
       "4       5     0    0.142060  0.122733  0.687616  0.178393  0.048400   \n",
       "5       6     0    0.165164  0.171820  0.681821  0.330669  0.017591   \n",
       "6       7     0    0.108488  0.175178  0.442986  0.314731  0.015936   \n",
       "7       8     0    0.046603  0.110731  0.735936  0.190573  0.015797   \n",
       "8       9     0    0.043568  0.075700  0.808988  0.121208  0.015132   \n",
       "9      10     0    0.041196  0.074663  0.810396  0.121195  0.014108   \n",
       "10     11     0    0.038925  0.062498  0.835334  0.101864  0.013212   \n",
       "11     12     0    0.038245  0.060230  0.840434  0.098347  0.012866   \n",
       "12     13     0    0.037875  0.059834  0.841174  0.098087  0.012779   \n",
       "13     14     0    0.037421  0.059395  0.842651  0.097142  0.012808   \n",
       "14     15     0    0.037143  0.058922  0.844032  0.096749  0.012895   \n",
       "15     16     0    0.063199  0.062034  0.837447  0.101087  0.012745   \n",
       "16     17     0    0.061176  0.060760  0.838443  0.099033  0.012716   \n",
       "17     18     0    0.060958  0.062885  0.841032  0.104076  0.012513   \n",
       "18     19     0    0.060142  0.062561  0.840940  0.103543  0.012825   \n",
       "19     20     0    0.060683  0.063580  0.840973  0.106396  0.013263   \n",
       "20     21     0    0.060733  0.060299  0.842699  0.099477  0.012535   \n",
       "21     22     0    0.060171  0.062357  0.839801  0.102981  0.012760   \n",
       "22     23     0    0.060118  0.060522  0.841712  0.099508  0.012758   \n",
       "23     24     0    0.060056  0.060459  0.843309  0.099756  0.012853   \n",
       "24     25     0    0.060182  0.060651  0.843138  0.100404  0.012770   \n",
       "25     26     0    0.058318  0.059742  0.843963  0.098442  0.012746   \n",
       "26     27     0    0.058986  0.059739  0.843590  0.098584  0.012919   \n",
       "27     28     0    0.059261  0.059380  0.844253  0.097980  0.012621   \n",
       "28     29     0    0.059228  0.059909  0.842914  0.098742  0.012802   \n",
       "29     30     0    0.037146  0.059396  0.843380  0.098268  0.012643   \n",
       "30     31     0    0.036045  0.057565  0.846713  0.094500  0.012647   \n",
       "31     32     0    0.035795  0.057748  0.846156  0.094808  0.013045   \n",
       "32     33     0    0.035666  0.057728  0.846278  0.095015  0.012587   \n",
       "33     34     0    0.035420  0.057729  0.846230  0.094790  0.012786   \n",
       "\n",
       "    intraparenchymal  intraventricular  subarachnoid  subdural  train_sz  \\\n",
       "0           0.229571          0.092236      0.201881  0.985966     17577   \n",
       "1           0.300094          0.288142      0.345089  0.320320     17577   \n",
       "2           0.128909          0.129850      0.149150  0.339859     17577   \n",
       "3           0.492600          0.448893      0.417427  0.414179     17577   \n",
       "4           0.098498          0.110666      0.111207  0.133571     17577   \n",
       "5           0.091962          0.113049      0.157081  0.161722     17577   \n",
       "6           0.149549          0.089253      0.148018  0.194026     17577   \n",
       "7           0.076381          0.056579      0.118125  0.127090     17577   \n",
       "8           0.055995          0.040890      0.080520  0.094944     17577   \n",
       "9           0.056875          0.035701      0.075549  0.098022     17577   \n",
       "10          0.046009          0.030019      0.062721  0.081797     17577   \n",
       "11          0.044839          0.029394      0.059918  0.077898     17577   \n",
       "12          0.044849          0.029316      0.059304  0.076416     17577   \n",
       "13          0.044485          0.029056      0.058803  0.076331     17577   \n",
       "14          0.043866          0.028514      0.058257  0.075425     17577   \n",
       "15          0.047637          0.032370      0.061437  0.077872     17577   \n",
       "16          0.045007          0.033106      0.058764  0.077659     17577   \n",
       "17          0.046202          0.032725      0.061478  0.079124     17577   \n",
       "18          0.046356          0.033186      0.060847  0.077625     17577   \n",
       "19          0.045586          0.031670      0.060354  0.081400     17577   \n",
       "20          0.044882          0.030581      0.057817  0.077322     17577   \n",
       "21          0.045732          0.033905      0.060712  0.077428     17577   \n",
       "22          0.044703          0.030725      0.060239  0.076210     17577   \n",
       "23          0.045013          0.030690      0.059043  0.076100     17577   \n",
       "24          0.045022          0.030943      0.058453  0.076564     17577   \n",
       "25          0.044305          0.030169      0.058106  0.075987     17577   \n",
       "26          0.044461          0.029744      0.058144  0.075737     17577   \n",
       "27          0.044263          0.029754      0.057572  0.075489     17577   \n",
       "28          0.044614          0.029972      0.058185  0.076303     17577   \n",
       "29          0.044284          0.028572      0.057151  0.076586     17577   \n",
       "30          0.043381          0.027538      0.056336  0.074055     17577   \n",
       "31          0.043539          0.027406      0.056319  0.074310     17577   \n",
       "32          0.043213          0.027224      0.056258  0.074786     17577   \n",
       "33          0.043387          0.027166      0.056169  0.075013     17577   \n",
       "\n",
       "    val_sz   bs  train_time  valid_time      lr      wd  \n",
       "0     2400  100  160.251015   31.356178  0.1000  0.0001  \n",
       "1     2400  100  155.015386   29.556167  0.1000  0.0001  \n",
       "2     2400  100  154.879500   27.140476  0.1000  0.0001  \n",
       "3     2400  100  156.678311   28.908540  0.1000  0.0001  \n",
       "4     2400  100  154.641192   26.106112  0.1000  0.0001  \n",
       "5     2400  100  154.669446   25.977148  0.1000  0.0001  \n",
       "6     2400  100  155.439858   25.644034  0.1000  0.0001  \n",
       "7     2400  100  155.412679   26.197765  0.0100  0.0001  \n",
       "8     2400  100  154.755002   26.382353  0.0100  0.0001  \n",
       "9     2400  100  154.682014   26.325155  0.0100  0.0001  \n",
       "10    2400  100  155.851308   26.180528  0.0020  0.0001  \n",
       "11    2400  100  154.850655   26.117462  0.0020  0.0001  \n",
       "12    2400  100  384.280921   51.336847  0.0020  0.0001  \n",
       "13    2400  100  155.777497   29.887910  0.0020  0.0001  \n",
       "14    2400  100  155.482482   25.653671  0.0020  0.0001  \n",
       "15    2000  100  169.736084   29.725267  0.0020  0.0001  \n",
       "16    2000  100  158.700440   24.708474  0.0020  0.0001  \n",
       "17    2400  100  153.454062   28.075624  0.0020  0.0001  \n",
       "18    2400  100  154.643816   26.342926  0.0020  0.0001  \n",
       "19    2400  100  154.788163   26.137916  0.0020  0.0001  \n",
       "20    2400  100  154.736595   29.023877  0.0020  0.0001  \n",
       "21    2400  100  154.789902   26.088672  0.0020  0.0001  \n",
       "22    2400  100  155.973188   26.107710  0.0020  0.0001  \n",
       "23    2400  100  154.747613   26.102819  0.0005  0.0001  \n",
       "24    2400  100  155.069272   28.767607  0.0005  0.0001  \n",
       "25    2400  100  155.616217   26.113737  0.0005  0.0001  \n",
       "26    2400  100  154.627858   26.179941  0.0005  0.0001  \n",
       "27    2400  100  155.360457   26.128672  0.0005  0.0001  \n",
       "28    2400  100  155.530201   27.551405  0.0005  0.0001  \n",
       "29    2400  100  154.486103   26.230717  0.0005  0.0001  \n",
       "30    2400  100  155.204885   26.453111  0.0005  0.0001  \n",
       "31    2400  100  155.244213   27.856316  0.0005  0.0001  \n",
       "32    2400  100  155.400090   26.334645  0.0002  0.0001  \n",
       "33    2400  100  155.614933   26.507466  0.0002  0.0001  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(PATH_WORK/'stats.f{}.v{}'.format(0,VERSION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.0739044 , 0.00253213, 0.02575298, 0.01846603, 0.0246274 ,\n",
       "       0.03151604], dtype=float32)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.mean((0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0021f22e48>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(predictions.mean(0)[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_one(bs = 100, add_seed = 0, fold = 0):\n",
    "    st = time.time()\n",
    "\n",
    "    cur_epoch = getCurrentBatch(fold=fold)\n",
    "    if cur_epoch is None: cur_epoch = 0\n",
    "    print('completed epochs:', cur_epoch)\n",
    "\n",
    "    model = TabularModel(n_cont = len(meta_cols), out_sz=360, layers=[500,200], ps=[0.5,0.5], bn_final=True, \n",
    "                         feat_sz=feat_sz)\n",
    "    \n",
    "    model_file_name = modelFileName(return_last=True, fold=fold)\n",
    "    if model_file_name is not None:\n",
    "        print('loading model', model_file_name)\n",
    "        state_dict = torch.load(PATH_WORK/'models'/model_file_name)\n",
    "        model.load_state_dict(state_dict)\n",
    "    \n",
    "    if (not CLOUD) or CLOUD_SINGLE:\n",
    "        model = model.to(device)\n",
    "    else:\n",
    "        model_parallel = dp.DataParallel(model, device_ids=devices)\n",
    "\n",
    "    setSeeds(SEED + cur_epoch + add_seed)\n",
    "\n",
    "    tst_ds = RSNA_DataSet(None, test_ids_df, mode='test', bs=bs, fold=fold)\n",
    "    loader_tst = D.DataLoader(tst_ds, num_workers=8 if CLOUD else 0, batch_size=bs, shuffle=False)\n",
    "    print('dataset test:', len(tst_ds), 'loader test:', len(loader_tst))\n",
    "\n",
    "    if CLOUD and (not CLOUD_SINGLE):\n",
    "        results = model_parallel(test_loop_fn, loader_tst)\n",
    "        predictions = np.concatenate([results[i][0] for i in range(MAX_DEVICES)])\n",
    "        indices = np.concatenate([results[i][1] for i in range(MAX_DEVICES)])\n",
    "        offsets = np.concatenate([results[i][2] for i in range(MAX_DEVICES)])\n",
    "    else:\n",
    "        predictions, indices, offsets = test_loop_fn(model, loader_tst, device)\n",
    "\n",
    "    predictions = predictions[np.argsort(indices)]\n",
    "    offsets = offsets[np.argsort(indices)]\n",
    "    assert len(predictions) == len(test_md.SeriesInstanceUID.unique())\n",
    "    assert np.all(indices[np.argsort(indices)] == np.array(range(len(predictions))))\n",
    "    \n",
    "    print('test processing time:', time.time() - st)\n",
    "    \n",
    "    return predictions, offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epochs: 0\n",
      "adding dummy serieses 86\n",
      "DataSet test 2\n",
      "dataset test: 2300 loader test: 23\n",
      "B10 -> time passed: 19.897 time per batch: 1.990\n",
      "B20 -> time passed: 37.963 time per batch: 1.898\n",
      "test processing time: 43.615331172943115\n"
     ]
    }
   ],
   "source": [
    "DATA_SMALL = False\n",
    "predictions, offsets = inference_one(fold = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4793942 , 0.45832056, 0.6021465 , 0.4322197 , 0.5268149 ,\n",
       "       0.5183584 ], dtype=float32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.mean((0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07497942, 0.00332458, 0.02651889, 0.01697854, 0.02606766,\n",
       "       0.03189934], dtype=float32)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.mean((0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.07497869, 0.00332539, 0.02651942, 0.01697802, 0.02606644,\n",
       "       0.03189924], dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.mean((0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame()\n",
    "for i, series_id in enumerate(test_md.SeriesInstanceUID.unique()):\n",
    "    df = test_md.loc[test_md.SeriesInstanceUID == series_id]\n",
    "    id_column = [a + '_' + b for a in df.SOPInstanceUID for b in all_ich]\n",
    "    assert (offsets[i] + len(df)) <= 60\n",
    "    data_sub = pd.DataFrame({'ID':np.array(id_column), \n",
    "                             'Label':predictions[i,offsets[i]:(offsets[i] + len(df))].reshape(-1)})\n",
    "    sub = pd.concat([sub,data_sub], axis=0, sort=False)\n",
    "\n",
    "sub = sub.reset_index(drop=True)\n",
    "\n",
    "assert len(sub) == 6*len(test_md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = sub.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub2 = sub.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.Label = (sub.Label + sub1.Label + sub2.Label)/3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_76233fb91_any</td>\n",
       "      <td>0.004707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_76233fb91_epidural</td>\n",
       "      <td>0.000312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_76233fb91_intraparenchymal</td>\n",
       "      <td>0.000656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_76233fb91_intraventricular</td>\n",
       "      <td>0.000672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_76233fb91_subarachnoid</td>\n",
       "      <td>0.002877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              ID     Label\n",
       "0               ID_76233fb91_any  0.004707\n",
       "1          ID_76233fb91_epidural  0.000312\n",
       "2  ID_76233fb91_intraparenchymal  0.000656\n",
       "3  ID_76233fb91_intraventricular  0.000672\n",
       "4      ID_76233fb91_subarachnoid  0.002877"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12678736462225704"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.loc[range(0,len(sub),6), 'Label'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sub = pd.read_csv(PATH/'submission_061.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13475628267250275"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_sub.loc[range(0,len(sub),6), 'Label'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(PATH/'sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9829018879124783"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(sub.sort_values('ID').reset_index(drop=True).loc[range(0,len(sub),6), 'Label'], \n",
    "            best_sub.sort_values('ID').reset_index(drop=True).loc[range(0,len(sub),6), 'Label'])[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 20.8M/20.8M [00:05<00:00, 4.23MB/s]\n",
      "Successfully submitted to RSNA Intracranial Hemorrhage Detection"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit rsna-intracranial-hemorrhage-detection -f ~/sub.csv -m \"TPU, dilated + 3xTTA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
