{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda activate torch-xla-nightly\n",
    "#export XRT_TPU_CONFIG=\"tpu_worker;0;$10.0.101.2:8470\"\n",
    "#git init\n",
    "#git remote add origin https://github.com/nosound2/RSNA-Hemorrhage\n",
    "#git pull origin master\n",
    "#git config remote.origin.push HEAD\n",
    "#gcloud config set compute/zone europe-west4-a\n",
    "#gcloud auth login\n",
    "#gcloud config set project endless-empire-239015\n",
    "#pip install kaggle\n",
    "#mkdir .kaggle\n",
    "#gsutil cp gs://recursion-double-strand/kaggle-keys/kaggle.json ~/.kaggle\n",
    "#chmod 600 /home/zahar_chikishev/.kaggle/kaggle.json\n",
    "#kaggle competitions download rsna-intracranial-hemorrhage-detection -f stage_1_train.csv\n",
    "#sudo apt install unzip\n",
    "#unzip stage_1_train.csv.zip\n",
    "#kaggle kernels output xhlulu/rsna-generate-metadata-csvs -p .\n",
    "#gsutil cp gs://rsna-hemorrhage/yuvals/* .\n",
    "\n",
    "#export XRT_TPU_CONFIG=\"tpu_worker;0;10.0.101.2:8470\"; conda activate torch-xla-nightly; jupyter notebook\n",
    "\n",
    "# 35.204.242.164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 38\n",
    "CLOUD_SINGLE = False\n",
    "MIXUP = False\n",
    "DATA_SET = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "from matplotlib import patches, patheffects\n",
    "import time\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error,log_loss,roc_auc_score\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "import pdb\n",
    "\n",
    "import scipy as sp\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "\n",
    "CLOUD = not torch.cuda.is_available()\n",
    "\n",
    "if not CLOUD:\n",
    "    torch.cuda.current_device()\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as D\n",
    "import torch.nn.functional as F\n",
    "import torch.utils as U\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "from torchvision import models as M\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if CLOUD:\n",
    "    PATH = Path('/home/zahar_chikishev')\n",
    "    PATH_WORK = Path('/home/zahar_chikishev/running')\n",
    "else:\n",
    "    PATH = Path('C:/StudioProjects/Hemorrhage')\n",
    "    PATH_WORK = Path('C:/StudioProjects/Hemorrhage/running')\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import seaborn as sn\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "\n",
    "all_ich = ['any','epidural','intraparenchymal','intraventricular','subarachnoid','subdural']\n",
    "class_weights = 6.0*np.array([2,1,1,1,1,1])/7.0\n",
    "\n",
    "if CLOUD:\n",
    "    import torch_xla\n",
    "    import torch_xla.distributed.data_parallel as dp\n",
    "    import torch_xla.utils as xu\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    \n",
    "    from typing import Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_black = '006d4432e'\n",
    "\n",
    "if CLOUD:\n",
    "    device = xm.xla_device()\n",
    "    #device = 'cpu'\n",
    "    MAX_DEVICES = 1 if CLOUD_SINGLE else 8\n",
    "    bs = 32\n",
    "else:\n",
    "    device = 'cuda'\n",
    "    #device = 'cpu'\n",
    "    MAX_DEVICES = 1\n",
    "    bs = 16\n",
    "\n",
    "if CLOUD and (not CLOUD_SINGLE):\n",
    "    devices = xm.get_xla_supported_devices(max_devices=MAX_DEVICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2351\n",
    "\n",
    "def setSeeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "setSeeds(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_cat, cols_float = pickle.load(open(PATH_WORK/'covs','rb'))\n",
    "meta_cols = cols_cat + cols_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_SET == 1:\n",
    "    if False:\n",
    "        filename = PATH_WORK/'indexes_file.pkl'\n",
    "        all_idx, train_ids, val_ids = pickle.load(open(filename,'rb'))\n",
    "\n",
    "        train_md = pd.read_csv(PATH_WORK/'train_md.csv').sort_values(['SeriesInstanceUID','pos_idx'])\n",
    "        train_md['img_id'] = train_md.SOPInstanceUID.str.split('_').apply(lambda x: x[1])\n",
    "\n",
    "        ids_df = pd.DataFrame(all_idx, columns = ['img_id'])\n",
    "        ids_df = ids_df.join(train_md.set_index('img_id'), on = 'img_id')\n",
    "        \n",
    "        assert len(ids_df.SeriesInstanceUID.unique()) == 19530\n",
    "        \n",
    "        trn_data = ids_df.loc[ids_df.img_id.isin(all_idx[train_ids])].reset_index(drop=True)\n",
    "        val_data = ids_df.loc[ids_df.img_id.isin(all_idx[val_ids])].reset_index(drop=True)\n",
    "\n",
    "        assert len(trn_data.SeriesInstanceUID.unique()) + len(val_data.SeriesInstanceUID.unique()) \\\n",
    "            == len(train_md.SeriesInstanceUID.unique())\n",
    "\n",
    "        assert len(trn_data.PatientID.unique()) + len(val_data.PatientID.unique()) \\\n",
    "            >= len(train_md.PatientID.unique())\n",
    "\n",
    "        pickle.dump((trn_data,val_data), open(PATH_WORK/'train.post.processed.1','wb'))\n",
    "    else:\n",
    "        trn_data,val_data = pickle.load(open(PATH_WORK/'train.post.processed.1','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_SET == 1:\n",
    "    if False:\n",
    "        test_md = pd.read_csv(PATH_WORK/'test_md.csv').sort_values(['SeriesInstanceUID','pos_idx'])\n",
    "        test_md['img_id'] = test_md.SOPInstanceUID.str.split('_').apply(lambda x: x[1])\n",
    "\n",
    "        filename = PATH_WORK/'test_indexes.pkl'\n",
    "        test_ids = pickle.load(open(filename,'rb'))\n",
    "\n",
    "        test_ids_df = pd.DataFrame(test_ids, columns = ['img_id'])\n",
    "        test_md = test_ids_df.join(test_md.set_index('img_id'), on = 'img_id')\n",
    "\n",
    "        assert len(test_md.SeriesInstanceUID.unique()) == 2214\n",
    "\n",
    "        pickle.dump(test_md, open(PATH_WORK/'test.post.processed.1','wb'))\n",
    "    else:\n",
    "        test_md = pickle.load(open(PATH_WORK/'test.post.processed.1','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    print(pd.concat([test_md[meta_cols].mean(0),\n",
    "                     trn_data[meta_cols].mean(0),\n",
    "                     val_data[meta_cols].mean(0)], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. \n",
    "model_Densenet161_3_vehrsion_basic_classifier_type_features_train_split_2.pkl\n",
    "10/7/19, 4:14:13 PM UTC+3\t\n",
    "model_Densenet161_3_vehrsion_basic_classifier_type_features_test_split_2.pkl\n",
    "10/7/19, 5:05:17 PM UTC+3\t\n",
    "indexes_file.pkl\n",
    "10/7/19, 5:34:42 PM UTC+3\t\n",
    "test_indexes.pkl \n",
    "10/9/19, 6:36:35 PM UTC+3\t\n",
    "\n",
    "2. \n",
    "model_Densenet169_3_vehrsion_basic_classifier_wso_type_features_test_tta_split_2.pkl\n",
    "10/10/19, 6:31:59 PM UTC+3\t\n",
    "model_Densenet169_3_vehrsion_basic_classifier_wso_type_features_train_tta_split_2.pkl\n",
    "10/10/19, 5:56:16 PM UTC+3\t\n",
    "\n",
    "model_Densenet161_3_vehrsion_basic_classifier_wso2_type_features_test_tta_split_2.pkl\n",
    "10/10/19, 3:07:20 PM UTC+3\t\n",
    "model_Densenet161_3_vehrsion_basic_classifier_wso2_type_features_train_tta_split_2.pkl\n",
    "10/10/19, 1:36:36 PM UTC+3\t\n",
    "\n",
    "4.\n",
    "model_Densenet161_3_version_classifier_splits_type_features_test_split_0.pkl\n",
    "10/14/19, 12:53:08 PM UTC+3\t\n",
    "model_Densenet161_3_version_classifier_splits_type_features_test_split_1.pkl\n",
    "10/14/19, 2:09:27 PM UTC+3\t\n",
    "model_Densenet161_3_version_classifier_splits_type_features_test_split_2.pkl\n",
    "10/14/19, 3:17:16 PM UTC+3\t\n",
    "\n",
    "5.\n",
    "train_dedup.csv\n",
    "10/14/19, 11:39:18 AM UTC+3\t\n",
    "model_Densenet169_3_version_classifier_splits_type_features_train_tta_split_2.pkl\n",
    "10/13/19, 2:43:34 PM UTC+3\t\n",
    "model_Densenet169_3_version_classifier_splits_type_features_train_tta_split_0.pkl\n",
    "10/14/19, 6:09:28 PM UTC+3\t\n",
    "model_Densenet169_3_version_classifier_splits_type_features_train_tta_split_1.pkl\n",
    "10/14/19, 8:09:27 PM UTC+3\t\n",
    "PID_splits.pkl\n",
    "10/14/19, 11:34:06 AM UTC+3\t\n",
    "model_Densenet169_3_version_classifier_splits_type_features_test_split_0.pkl\n",
    "10/13/19, 3:34:21 PM UTC+3\t\n",
    "model_Densenet169_3_version_classifier_splits_type_features_test_split_1.pkl\n",
    "10/13/19, 4:36:25 PM UTC+3\t\n",
    "model_Densenet169_3_version_classifier_splits_type_features_test_split_2.pkl\n",
    "10/13/19, 4:05:32 PM UTC+3\t\n",
    "\n",
    "\n",
    "I finished uploading all densenet 169 folds for test and train.\n",
    "(Be aware, fold 0 gives worse scores then 2).\n",
    "I uploaded the train dataset I used (without duplicates) it is called 'train_dedup'\n",
    "Also loaded the splits data into PID_split.\n",
    "It is a tuple. The first variable is a numpy array with the unique PIDs in train_df.\n",
    "The 2nd variable is a double list, with the indices of the train and validation for the 3 splits. \n",
    "The indices are for the PID numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_SET == 1:\n",
    "    feat_sz = 2208\n",
    "elif DATA_SET == 2:\n",
    "    feat_sz = 208\n",
    "else: assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_SET == 2:\n",
    "    if False:\n",
    "        train_dedup = pd.read_csv(PATH_WORK/'yuval'/'train_dedup.csv')\n",
    "        pids, folding = pickle.load(open(PATH_WORK/'yuval'/'PID_splits.pkl','rb'))\n",
    "\n",
    "        assert len(pids) == 17079\n",
    "        assert len(np.unique(pids)) == 17079\n",
    "\n",
    "        for fol in folding:\n",
    "            assert len(fol[0]) + len(fol[1]) == 17079\n",
    "\n",
    "        assert len(folding[0][1]) + len(folding[1][1]) + len(folding[2][1]) == 17079\n",
    "\n",
    "        assert len(train_dedup.PID.unique()) == 17079\n",
    "\n",
    "        train_dedup['fold'] = np.nan\n",
    "\n",
    "        for fold in range(3):\n",
    "            train_dedup.loc[train_dedup.PID.isin(pids[folding[fold][1]]),'fold'] = fold\n",
    "\n",
    "        assert train_dedup.fold.isnull().sum() == 0\n",
    "\n",
    "        train_md = pd.read_csv(PATH_WORK/'train_md.csv').sort_values(['SeriesInstanceUID','pos_idx'])\n",
    "        train_md['img_id'] = train_md.SOPInstanceUID.str.split('_').apply(lambda x: x[1])\n",
    "\n",
    "        ids_df = train_dedup[['fold','PatientID']]\n",
    "        ids_df.columns = ['fold','img_id']\n",
    "\n",
    "        ids_df = ids_df.join(train_md.set_index('img_id'), on = 'img_id')\n",
    "\n",
    "        pickle.dump(ids_df, open(PATH_WORK/'features/densenet169_v3/train/train.ids.df','wb'))\n",
    "\n",
    "        test_md = pickle.load(open(PATH_WORK/'test.post.processed.1','rb'))\n",
    "\n",
    "        pickle.dump(test_md, open(PATH_WORK/'features/densenet169_v3/test/test.ids.df','wb'))\n",
    "\n",
    "        for fold in range(3):\n",
    "            filename = PATH_WORK/'yuval'/\\\n",
    "                'model_Densenet169_3_version_classifier_splits_type_features_train_tta_split_{}.pkl'\\\n",
    "                .format(fold)\n",
    "            feats = pickle.load(open(filename,'rb'))\n",
    "            assert len(feats) == 4*len(ids_df)\n",
    "\n",
    "            for i in range(4):\n",
    "                feats_sub1 = feats[torch.BoolTensor(np.arange(len(feats))%4 == i)]\n",
    "                feats_sub2 = feats_sub1[torch.BoolTensor(train_dedup.fold != fold)]\n",
    "                pickle.dump(feats_sub2, open(PATH_WORK/'features/densenet169_v3/train/train.f{}.a{}'\n",
    "                                             .format(fold,i),'wb'))\n",
    "\n",
    "                feats_sub2 = feats_sub1[torch.BoolTensor(train_dedup.fold == fold)]\n",
    "                pickle.dump(feats_sub2, open(PATH_WORK/'features/densenet169_v3/train/valid.f{}.a{}'\n",
    "                                             .format(fold,i),'wb'))\n",
    "\n",
    "                if i==0:\n",
    "                    black_feats = feats_sub1[torch.BoolTensor(ids_df.img_id == all_black)].squeeze()\n",
    "                    pickle.dump(black_feats, open(PATH_WORK/'features/densenet169_v3/train/black.f{}'\n",
    "                                                  .format(fold),'wb'))\n",
    "\n",
    "        for fold in range(3):\n",
    "            filename = PATH_WORK/'yuval'/\\\n",
    "                'model_Densenet169_3_version_classifier_splits_type_features_test_split_{}.pkl'\\\n",
    "                .format(fold)\n",
    "            feats = pickle.load(open(filename,'rb'))\n",
    "\n",
    "            for i in range(8):\n",
    "                feats_sub = feats[torch.BoolTensor(np.arange(len(feats))%8 == i)]\n",
    "                pickle.dump(feats_sub, open(PATH_WORK/'features/densenet169_v3/test/test.f{}.a{}'\n",
    "                                            .format(fold,i),'wb'))\n",
    "                assert len(feats_sub) == len(test_md)\n",
    "    else:\n",
    "        ids_df = pickle.load(open(PATH_WORK/'features/densenet169_v3/train/train.ids.df','rb'))\n",
    "        test_md = pickle.load(open(PATH_WORK/'features/densenet169_v3/test/test.ids.df','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_SET == 1:\n",
    "    if False:\n",
    "        filename = PATH_WORK/'model_Densenet161_3_vehrsion_basic_classifier_type_features_train_split_2.pkl'\n",
    "        feats = pickle.load(open(filename,'rb'))\n",
    "\n",
    "        for series_id in tqdm(ids_df.SeriesInstanceUID.unique()):\n",
    "            mask = torch.BoolTensor(ids_df.SeriesInstanceUID.values == series_id)\n",
    "            feats_id = feats[mask]\n",
    "            pickle.dump(feats_id, open(PATH_WORK/'features/densenet161_v3/train/{}'.format(series_id),'wb'))\n",
    "\n",
    "\n",
    "        filename = PATH_WORK/'model_Densenet161_3_vehrsion_basic_classifier_type_features_test_split_2.pkl'\n",
    "        feats = pickle.load(open(filename,'rb'))\n",
    "\n",
    "        for series_id in tqdm(test_md.SeriesInstanceUID.unique()):\n",
    "            mask = torch.BoolTensor(test_md.SeriesInstanceUID.values == series_id)\n",
    "            feats_id = feats[mask]\n",
    "            pickle.dump(feats_id, open(PATH_WORK/'features/densenet161_v3/test/{}'.format(series_id),'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = PATH_WORK/'features/densenet161_v3/train/ID_000a935543'\n",
    "#feats1 = pickle.load(open(path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_SET == 1:\n",
    "    path = PATH_WORK/'features/densenet161_v3/train/ID_992b567eb6'\n",
    "    black_feats = pickle.load(open(path,'rb'))[41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSNA_DataSet(D.Dataset):\n",
    "    def __init__(self, metadata, mode='train', bs=None, dataset=DATA_SET, fold=0):\n",
    "        \n",
    "        super(RSNA_DataSet, self).__init__()\n",
    "        \n",
    "        if dataset == 1:\n",
    "            md = metadata.copy()\n",
    "            md = md.reset_index(drop=True)\n",
    "        else:\n",
    "            if mode == 'train':\n",
    "                md = metadata.loc[metadata.fold != fold].copy().reset_index(drop=True)\n",
    "            elif mode == 'valid':\n",
    "                md = metadata.loc[metadata.fold == fold].copy().reset_index(drop=True)\n",
    "            else:\n",
    "                md = metadata.copy().reset_index(drop=True)\n",
    "        \n",
    "        series = np.sort(md.SeriesInstanceUID.unique())\n",
    "        md = md.set_index('SeriesInstanceUID', drop=True)\n",
    "        \n",
    "        samples_add = 0\n",
    "        if (mode != 'train') and not DATA_SMALL:\n",
    "            batch_num = -((-len(series))//(bs*MAX_DEVICES))\n",
    "            samples_add = batch_num*bs*MAX_DEVICES - len(series)\n",
    "            print('adding dummy serieses', samples_add)\n",
    "        \n",
    "        #self.records = df.to_records(index=False)\n",
    "        self.mode = mode\n",
    "        self.real = np.concatenate([np.repeat(True,len(series)),np.repeat(False,samples_add)])\n",
    "        self.series = np.concatenate([series, random.sample(list(series),samples_add)])\n",
    "        self.metadata = md\n",
    "        self.dataset = dataset\n",
    "        self.fold = fold\n",
    "        \n",
    "        print('DataSet', dataset, mode, 'size', len(self.series), 'fold', fold)\n",
    "        \n",
    "        if self.dataset == 2:\n",
    "            path = PATH_WORK/'features/densenet169_v3/train/black.f{}'.format(fold)\n",
    "            self.black_feats = pickle.load(open(path,'rb')).squeeze()\n",
    "            \n",
    "            if mode == 'valid':\n",
    "                self.setFeats(0)\n",
    "            \n",
    "        elif self.dataset == 1:\n",
    "            self.black_feats = black_feats\n",
    "    \n",
    "    def setFeats(self, anum):\n",
    "        if self.dataset == 1: return\n",
    "        print('setFeats, augmentation', anum)\n",
    "        self.anum = anum\n",
    "        folder = 'test' if self.mode == 'test' else 'train'\n",
    "        path = PATH_WORK/'features/densenet169_v3/{}/{}.f{}.a{}'.format(folder,self.mode,self.fold,anum)\n",
    "        feats = pickle.load(open(path,'rb'))\n",
    "        self.feats = feats\n",
    "        assert len(feats) == len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        series_id = self.series[index]\n",
    "        #df = self.metadata.loc[self.metadata.SeriesInstanceUID == series_id].reset_index(drop=True)\n",
    "        df = self.metadata.loc[series_id].reset_index(drop=True)\n",
    "        \n",
    "        if self.dataset == 1:\n",
    "            folder = 'test' if self.mode == 'test' else 'train'\n",
    "            path = PATH_WORK/'features/densenet161_v3/{}/{}'.format(folder,series_id)\n",
    "            feats = pickle.load(open(path,'rb'))\n",
    "            \n",
    "            if feats.shape[0] > len(df.img_id.unique()):\n",
    "                mask_dup = ~df.img_id.duplicated().values\n",
    "                df = df.loc[mask_dup]\n",
    "                feats = feats[torch.BoolTensor(mask_dup)]\n",
    "            \n",
    "            assert feats.shape[0] == len(df)\n",
    "        elif self.dataset == 2:\n",
    "            feats = self.feats[torch.BoolTensor(self.metadata.index.values == series_id)]\n",
    "        else: assert False\n",
    "        \n",
    "        order = np.argsort(df.pos_idx.values)\n",
    "        df = df.sort_values(['pos_idx'])\n",
    "        feats = feats[torch.LongTensor(order)]\n",
    "        \n",
    "        feats = torch.cat([feats, torch.Tensor(df[meta_cols].values)], dim=1)\n",
    "        target = torch.Tensor(df[all_ich].values)\n",
    "        \n",
    "        PAD = 4+9\n",
    "        \n",
    "        offset = np.random.randint(0, 61 - feats.shape[0])\n",
    "        #offset = 0\n",
    "        top_pad = PAD + offset\n",
    "        if top_pad > 0:\n",
    "            dummy_row = torch.cat([self.black_feats, torch.Tensor(df.head(1)[meta_cols].values).squeeze()])\n",
    "            feats = torch.cat([dummy_row.repeat(top_pad,1), feats], dim=0)\n",
    "            if offset > 0:\n",
    "                target = torch.cat([torch.zeros((offset, len(all_ich))), target], dim=0)\n",
    "        bot_pad = 60 - len(df) - offset + PAD\n",
    "        if bot_pad > 0:\n",
    "            dummy_row = torch.cat([self.black_feats, torch.Tensor(df.tail(1)[meta_cols].values).squeeze()])\n",
    "            feats = torch.cat([feats, dummy_row.repeat(bot_pad,1)], dim=0)\n",
    "            if (60 - len(df) - offset) > 0:\n",
    "                target = torch.cat([target, torch.zeros((60 - len(df) - offset, len(all_ich)))], dim=0)\n",
    "        \n",
    "        assert feats.shape[0] == (60 + 2*PAD)\n",
    "        assert target.shape[0] == 60\n",
    "        \n",
    "        feats = feats.transpose(1,0)\n",
    "        \n",
    "        idx = index\n",
    "        if not self.real[index]: idx = -1\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            return feats, target\n",
    "        else:\n",
    "            return feats, target, idx, offset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.series) if not DATA_SMALL else int(0.01*len(self.series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCurrentBatch(fold=0):\n",
    "    sel_batch = None\n",
    "    for filename in os.listdir(PATH_WORK/'models'):\n",
    "        splits = filename.split('.')\n",
    "        if int(splits[2][1]) != fold: continue\n",
    "        if int(splits[3][1:]) != VERSION: continue\n",
    "        if sel_batch is None:\n",
    "            sel_batch = int(splits[1][1:])\n",
    "        else:\n",
    "            sel_batch = max(sel_batch, int(splits[1][1:]))\n",
    "    return sel_batch\n",
    "\n",
    "def modelFileName(fold=0, batch = 1, return_last = False, return_next = False):\n",
    "    sel_batch = batch\n",
    "    if return_last or return_next:\n",
    "        sel_batch = getCurrentBatch(fold)\n",
    "        if return_last and sel_batch is None:\n",
    "            return None\n",
    "        if return_next:\n",
    "            if sel_batch is None: sel_batch = 1\n",
    "            else: sel_batch += 1\n",
    "    \n",
    "    return 'model.b{}.f{}.v{}'.format(sel_batch, fold, VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCEWithLogitsLoss(nn.Module):\n",
    "    def __init__(self, weight=None):\n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "    \n",
    "    def forward(self, input, target, batch_weights = None):\n",
    "        loss = (torch.log(1+torch.exp(input)) - target*input)*self.weight\n",
    "        if batch_weights is not None:\n",
    "            loss = batch_weights*loss\n",
    "        return loss.mean()\n",
    "        \n",
    "        #return F.binary_cross_entropy_with_logits(input.squeeze(), target,\n",
    "        #                                          self.weight,\n",
    "        #                                          pos_weight=self.pos_weight,\n",
    "        #                                          reduction=self.reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatProduct(nn.Module):\n",
    "    def __init__(self, in_feature, out_feature):\n",
    "        super(FeatProduct, self).__init__()\n",
    "        self.in_feature = in_feature\n",
    "        self.out_feature = out_feature\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_feature, in_feature))\n",
    "        self.bias = nn.Parameter(torch.Tensor(out_feature))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "        nn.init.uniform_(self.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = F.linear(x, self.weight) + self.bias\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_drop_lin(n_in:int, n_out:int, bn:bool=True, p:float=0., actn=None):\n",
    "    \"Sequence of batchnorm (if `bn`), dropout (with `p`) and linear (`n_in`,`n_out`) layers followed by `actn`.\"\n",
    "    layers = [nn.BatchNorm1d(n_in)] if bn else []\n",
    "    if p != 0: layers.append(nn.Dropout(p))\n",
    "    layers.append(nn.Linear(n_in, n_out))\n",
    "    if actn is not None: layers.append(actn)\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularModel(nn.Module):\n",
    "    \"Basic model for tabular data.\"\n",
    "    def __init__(self, n_cont:int, out_sz:int, layers, ps=None,\n",
    "                 emb_drop:float=0., use_bn:bool=True, bn_final:bool=False, feat_sz=2208, fc_drop_p=0.3):\n",
    "        super().__init__()\n",
    "        self.bn_cont = nn.BatchNorm1d(feat_sz + n_cont)\n",
    "        self.n_cont = n_cont\n",
    "        sizes = self.get_sizes(layers, out_sz)\n",
    "        actns = [nn.ReLU(inplace=True) for _ in range(len(sizes)-2)] + [None]\n",
    "        layers = []\n",
    "        for i,(n_in,n_out,dp,act) in enumerate(zip(sizes[:-1],sizes[1:],[0.]+ps,actns)):\n",
    "            layers += bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act)\n",
    "        if bn_final: layers.append(nn.BatchNorm1d(sizes[-1]))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.feat_product = FeatProduct(feat_sz + n_cont, 20)\n",
    "        self.fc_drop = nn.Dropout(p=fc_drop_p)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2D_1 = nn.Conv2d(1,32,(feat_sz + n_cont,1))\n",
    "        self.conv2D_2 = nn.Conv2d(1,32,(feat_sz + n_cont,5))#,padding=(0,2)\n",
    "        self.bn_cont1 = nn.BatchNorm1d(64)\n",
    "        self.conv1D_1 = nn.Conv1d(64,32,3)#,padding=1\n",
    "        self.conv1D_3 = nn.Conv1d(64,32,5,dilation=5)\n",
    "        self.conv1D_2 = nn.Conv1d(64,6,3)#,padding=1\n",
    "        self.bn_cont2 = nn.BatchNorm1d(64)\n",
    "        self.bn_cont3 = nn.BatchNorm1d(6)\n",
    "\n",
    "    def get_sizes(self, layers, out_sz):\n",
    "        return [1200] + layers + [out_sz]\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        x = self.bn_cont(x) # bs,2208,60\n",
    "        x = self.fc_drop(x)\n",
    "        x = x.reshape(x.shape[0],1,x.shape[1],x.shape[2]) # bs,1,2208,60\n",
    "        x = torch.cat([self.conv2D_1(x[:,:,:,2:(-2)]).squeeze(), \n",
    "                       self.conv2D_2(x).squeeze()], dim=1) # bs,64,60\n",
    "        x = self.relu(x)\n",
    "        x = self.bn_cont1(x)\n",
    "        x = self.fc_drop(x)\n",
    "        #x = self.conv1D_1(x)\n",
    "        x = torch.cat([self.conv1D_1(x[:,:,9:(-9)]), \n",
    "                       self.conv1D_3(x)], dim=1) # bs,64,60\n",
    "        x = self.relu(x)\n",
    "        x = self.bn_cont2(x)\n",
    "        x = self.fc_drop(x)\n",
    "        x = self.conv1D_2(x)\n",
    "        #x = self.relu(x)\n",
    "        #x = self.bn_cont3(x) # bs,6,60\n",
    "        #x = self.fc_drop(x)\n",
    "        #x = self.feat_product(x)\n",
    "        #x = x.reshape(x.shape[0],-1)\n",
    "        #x = self.layers(x)\n",
    "        #x = x.reshape(x.shape[0],60,6)\n",
    "        x = x.transpose(1,2) # bs,60,6\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_fn(model, loader, device, context = None):\n",
    "    \n",
    "    if CLOUD and (not CLOUD_SINGLE):\n",
    "        tlen = len(loader._loader._loader)\n",
    "        OUT_LOSS = 1000\n",
    "        OUT_TIME = 4\n",
    "        generator = loader\n",
    "        device_num = int(str(device)[-1])\n",
    "        dataset = loader._loader._loader.dataset\n",
    "    else:\n",
    "        tlen = len(loader)\n",
    "        OUT_LOSS = 10\n",
    "        OUT_TIME = 1\n",
    "        generator = enumerate(loader)\n",
    "        device_num = 1\n",
    "        dataset = loader.dataset\n",
    "    \n",
    "    #print('Start training {}'.format(device), 'batches', tlen)\n",
    "    \n",
    "    criterion = BCEWithLogitsLoss(weight = torch.Tensor(class_weights).to(device))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(0.9, 0.99))\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    if CLOUD:\n",
    "        tracker = xm.RateTracker()\n",
    "\n",
    "    tloss = 0\n",
    "    tloss_count = 0\n",
    "    \n",
    "    st = time.time()\n",
    "    mixup_collected = False\n",
    "    for i, (x, y) in generator:\n",
    "        if (not CLOUD) or CLOUD_SINGLE:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "        \n",
    "        if MIXUP:\n",
    "            if mixup_collected:\n",
    "                lambd = np.random.beta(0.4, 0.4, y.size(0))\n",
    "                lambd = torch.Tensor(lambd).to(device)[:,None,None]\n",
    "                #shuffle = torch.randperm(y.size(0)).to(device)\n",
    "                x = lambd * x + (1-lambd) * x_mix #x[shuffle]\n",
    "                mixup_collected = False\n",
    "            else:\n",
    "                x_mix = x\n",
    "                y_mix = y\n",
    "                mixup_collected = True\n",
    "                continue\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        \n",
    "        if MIXUP:\n",
    "            loss = criterion(output, y, lambd) + criterion(output, y_mix, 1-lambd) #y[shuffle]\n",
    "            del x_mix, y_mix\n",
    "        else:\n",
    "            loss = criterion(output, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        tloss += len(y)*loss.cpu().detach().item()\n",
    "        tloss_count += len(y)\n",
    "        \n",
    "        if CLOUD or CLOUD_SINGLE:\n",
    "            xm.optimizer_step(optimizer)\n",
    "            if CLOUD_SINGLE:\n",
    "                xm.mark_step()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        \n",
    "        if CLOUD:\n",
    "            tracker.add(len(y))\n",
    "        \n",
    "        st_passed = time.time() - st\n",
    "        if (i+1)%OUT_TIME == 0 and device_num == 1:\n",
    "            #print(torch_xla._XLAC._xla_metrics_report())\n",
    "            print('Batch {} device: {} time passed: {:.3f} time per batch: {:.3f}'\n",
    "                .format(i+1, device, st_passed, st_passed/(i+1)))\n",
    "        \n",
    "        del loss, output, y, x\n",
    "    \n",
    "    return tloss, tloss_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def val_loop_fn(model, loader, device, context = None):\n",
    "    \n",
    "    if CLOUD and (not CLOUD_SINGLE):\n",
    "        tlen = len(loader._loader._loader)\n",
    "        OUT_LOSS = 1000\n",
    "        OUT_TIME = 4\n",
    "        generator = loader\n",
    "        device_num = int(str(device)[-1])\n",
    "    else:\n",
    "        tlen = len(loader)\n",
    "        OUT_LOSS = 1000\n",
    "        OUT_TIME = 10\n",
    "        generator = enumerate(loader)\n",
    "        device_num = 1\n",
    "    \n",
    "    #print('Start validating {}'.format(device), 'batches', tlen)\n",
    "    \n",
    "    st = time.time()\n",
    "    model.eval()\n",
    "    \n",
    "    results = []\n",
    "    indices = []\n",
    "    offsets = []\n",
    "    \n",
    "    for i, (x, y, idx, offset) in generator:\n",
    "        \n",
    "        if (not CLOUD) or CLOUD_SINGLE:\n",
    "            x = x.to(device)\n",
    "        \n",
    "        output = torch.sigmoid(model(x))\n",
    "        \n",
    "        mask = (idx >= 0)\n",
    "        results.append(output[mask].cpu().detach().numpy())\n",
    "        indices.append(idx[mask].cpu().detach().numpy())\n",
    "        offsets.append(offset[mask].cpu().detach().numpy())\n",
    "        \n",
    "        st_passed = time.time() - st\n",
    "        if (i+1)%OUT_TIME == 0 and device_num == 1:\n",
    "            print('Batch {} device: {} time passed: {:.3f} time per batch: {:.3f}'\n",
    "                  .format(i+1, device, st_passed, st_passed/(i+1)))\n",
    "        \n",
    "        del output, y, x, idx, offset\n",
    "    \n",
    "    results = np.concatenate(results)\n",
    "    indices = np.concatenate(indices)\n",
    "    offsets = np.concatenate(offsets)\n",
    "    \n",
    "    return results, indices, offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_loop_fn(model, loader, device, context = None):\n",
    "    \n",
    "    if CLOUD and (not CLOUD_SINGLE):\n",
    "        tlen = len(loader._loader._loader)\n",
    "        OUT_LOSS = 1000\n",
    "        OUT_TIME = 100\n",
    "        generator = loader\n",
    "        device_num = int(str(device)[-1])\n",
    "    else:\n",
    "        tlen = len(loader)\n",
    "        OUT_LOSS = 1000\n",
    "        OUT_TIME = 10\n",
    "        generator = enumerate(loader)\n",
    "        device_num = 1\n",
    "    \n",
    "    #print('Start testing {}'.format(device), 'batches', tlen)\n",
    "    \n",
    "    st = time.time()\n",
    "    model.eval()\n",
    "    \n",
    "    results = []\n",
    "    indices = []\n",
    "    offsets = []\n",
    "    \n",
    "    for i, (x, y, idx, offset) in generator:\n",
    "        \n",
    "        if (not CLOUD) or CLOUD_SINGLE:\n",
    "            x = x.to(device)\n",
    "        \n",
    "        output = torch.sigmoid(model(x))\n",
    "        \n",
    "        mask = (idx >= 0)\n",
    "        results.append(output[mask].cpu().detach().numpy())\n",
    "        indices.append(idx[mask].cpu().detach().numpy())\n",
    "        offsets.append(offset[mask].cpu().detach().numpy())\n",
    "        \n",
    "        st_passed = time.time() - st\n",
    "        if (i+1)%OUT_TIME == 0 and device_num == 1:\n",
    "            print('B{} -> time passed: {:.3f} time per batch: {:.3f}'.format(i+1, st_passed, st_passed/(i+1)))\n",
    "        \n",
    "        del output, x, y, idx, offset\n",
    "    \n",
    "    return np.concatenate(results), np.concatenate(indices), np.concatenate(offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(weight=None, load_model=True, epochs=1, bs=100, fold=0):\n",
    "    \n",
    "    st0 = time.time()\n",
    "    \n",
    "    cur_epoch = getCurrentBatch(fold=fold)\n",
    "    if cur_epoch is None: cur_epoch = 0\n",
    "    print('completed epochs:', cur_epoch, 'starting now:', epochs)\n",
    "    \n",
    "    setSeeds(SEED + cur_epoch)\n",
    "    \n",
    "    if DATA_SET == 1:\n",
    "        trn_ds = RSNA_DataSet(trn_data, mode='train', bs=bs, fold=fold)\n",
    "        val_ds = RSNA_DataSet(val_data, mode='valid', bs=bs, fold=fold)\n",
    "    elif DATA_SET == 2:\n",
    "        trn_ds = RSNA_DataSet(ids_df, mode='train', bs=bs, fold=fold)\n",
    "        val_ds = RSNA_DataSet(ids_df, mode='valid', bs=bs, fold=fold)\n",
    "    else: assert False\n",
    "    \n",
    "    loader = D.DataLoader(trn_ds, num_workers=16 if (CLOUD and not CLOUD_SINGLE) else 0, batch_size=bs, \n",
    "                          shuffle=True, drop_last=True)\n",
    "    loader_val = D.DataLoader(val_ds, num_workers=16 if (CLOUD and not CLOUD_SINGLE) else 0, batch_size=bs, \n",
    "                              shuffle=True)\n",
    "    print('dataset train:', len(trn_ds), 'valid:', len(val_ds), 'loader train:', len(loader), 'valid:', len(loader_val))\n",
    "    \n",
    "    model = TabularModel(n_cont = len(meta_cols), out_sz=360, layers=[500,200], ps=[0.5,0.5], bn_final=True, \n",
    "                         feat_sz=feat_sz)\n",
    "    \n",
    "    model_file_name = modelFileName(return_last=True, fold=fold)\n",
    "    if model_file_name is not None:\n",
    "        print('loading model', model_file_name)\n",
    "        state_dict = torch.load(PATH_WORK/'models'/model_file_name)\n",
    "        model.load_state_dict(state_dict)\n",
    "    else:\n",
    "        print('starting from scratch')\n",
    "    \n",
    "    if (not CLOUD) or CLOUD_SINGLE:\n",
    "        model = model.to(device)\n",
    "    else:\n",
    "        model_parallel = dp.DataParallel(model, device_ids=devices)\n",
    "        \n",
    "    loc_data = val_ds.metadata.copy()\n",
    "    #if DATA_SET == 1:\n",
    "    #    loc_data = val_data.copy()\n",
    "    #else:\n",
    "    #    loc_data = ids_df.loc[ids_df.fold == fold].copy().reset_index(drop=True)\n",
    "\n",
    "    if DATA_SMALL:\n",
    "        val_sz = int(0.01*len(loc_data.index.unique()))\n",
    "        val_series = loc_data.index.unique()[:val_sz]\n",
    "        loc_data = loc_data.loc[loc_data.index.isin(val_series)]\n",
    "    \n",
    "    series_counts = loc_data.index.value_counts()\n",
    "    \n",
    "    loc_data['orig_idx'] = np.arange(len(loc_data))\n",
    "    loc_data = loc_data.sort_values(['SeriesInstanceUID','pos_idx'])\n",
    "    loc_data['my_order'] = np.arange(len(loc_data))\n",
    "    loc_data = loc_data.sort_values(['orig_idx'])\n",
    "    \n",
    "    for i in range(cur_epoch+1, cur_epoch+epochs+1):\n",
    "        st = time.time()\n",
    "        \n",
    "        trn_ds.setFeats((i-1) % 4)\n",
    "        \n",
    "        if CLOUD and (not CLOUD_SINGLE):\n",
    "            results = model_parallel(train_loop_fn, loader)\n",
    "            tloss, tloss_count = np.stack(results).sum(0)\n",
    "            state_dict = model_parallel._models[0].state_dict()\n",
    "        else:\n",
    "            tloss, tloss_count = train_loop_fn(model, loader, device)\n",
    "            state_dict = model.state_dict()\n",
    "        \n",
    "        state_dict = {k:v.to('cpu') for k,v in state_dict.items()}\n",
    "        tr_ll = tloss / tloss_count\n",
    "        \n",
    "        train_time = time.time()-st\n",
    "        \n",
    "        model_file_name = modelFileName(return_next=True, fold=fold)\n",
    "        if not DATA_SMALL:\n",
    "            torch.save(state_dict, PATH_WORK/'models'/model_file_name)\n",
    "        \n",
    "        st = time.time()\n",
    "        if CLOUD and (not CLOUD_SINGLE):\n",
    "            results = model_parallel(val_loop_fn, loader_val)\n",
    "            predictions = np.concatenate([results[i][0] for i in range(MAX_DEVICES)])\n",
    "            indices = np.concatenate([results[i][1] for i in range(MAX_DEVICES)])\n",
    "            offsets = np.concatenate([results[i][2] for i in range(MAX_DEVICES)])\n",
    "        else:\n",
    "            predictions, indices, offsets = val_loop_fn(model, loader_val, device)\n",
    "        \n",
    "        predictions = predictions[np.argsort(indices)]\n",
    "        offsets = offsets[np.argsort(indices)]\n",
    "        assert len(predictions) == len(loc_data.index.unique())\n",
    "        assert len(predictions) == len(offsets)\n",
    "        assert np.all(indices[np.argsort(indices)] == np.array(range(len(predictions))))\n",
    "        \n",
    "        #val_results = np.zeros((len(loc_data),6))\n",
    "        val_results = []\n",
    "        for k, series in enumerate(np.sort(loc_data.index.unique())):\n",
    "            cnt = series_counts[series]\n",
    "            #mask = loc_data.SeriesInstanceUID == series\n",
    "            assert (offsets[k] + cnt) <= 60\n",
    "            #val_results[mask] = predictions[k,offsets[k]:(offsets[k] + cnt)]\n",
    "            val_results.append(predictions[k,offsets[k]:(offsets[k] + cnt)])\n",
    "        \n",
    "        val_results = np.concatenate(val_results)\n",
    "        assert np.isnan(val_results).sum() == 0\n",
    "        val_results = val_results[loc_data.my_order]\n",
    "        assert np.isnan(val_results).sum() == 0\n",
    "        assert len(val_results) == len(loc_data)\n",
    "        \n",
    "        lls = [log_loss(loc_data[all_ich[k]].values, val_results[:,k], eps=1e-8, labels=[0,1]) for k in range(6)]\n",
    "        ll = (class_weights * np.array(lls)).mean()\n",
    "        cor = np.corrcoef(loc_data.loc[:,all_ich].values.reshape(-1), val_results.reshape(-1))[0,1]\n",
    "\n",
    "        print('ver {}, epoch {}, fold {}, train ll: {:.4f}, val ll: {:.4f}, cor: {:.4f}, lr: {}'\n",
    "              .format(VERSION, i, fold, tr_ll, ll, cor, learning_rate))\n",
    "        valid_time = time.time()-st\n",
    "\n",
    "        epoch_stats = pd.DataFrame([[i, 0, tr_ll, ll, cor, lls[0], lls[1], lls[2], lls[3], lls[4], lls[5],\n",
    "                                     len(trn_ds), len(val_ds), bs, train_time, valid_time,\n",
    "                                     learning_rate, weight_decay]], \n",
    "                                   columns = \n",
    "                                    ['epoch','fold','train_loss','val_loss','cor',\n",
    "                                     'any','epidural','intraparenchymal','intraventricular','subarachnoid','subdural',\n",
    "                                     'train_sz','val_sz','bs','train_time','valid_time','lr','wd'\n",
    "                                     ])\n",
    "\n",
    "        stats_filename = PATH_WORK/'stats.f{}.v{}'.format(fold,VERSION)\n",
    "        if stats_filename.is_file():\n",
    "            epoch_stats = pd.concat([pd.read_csv(stats_filename), epoch_stats], sort=False)\n",
    "        #if not DATA_SMALL:\n",
    "        epoch_stats.to_csv(stats_filename, index=False)\n",
    "    \n",
    "    print('total running time', time.time() - st0)\n",
    "    \n",
    "    return model, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch 22 device: xla:1 time passed: 277.972 time per batch: 12.635 - 16 cores / 8 workers\n",
    "#Batch 22 device: xla:1 time passed: 209.280 time per batch: 9.513  - 16 cores / 16 workers\n",
    "#Batch 22 device: xla:1 time passed: 213.209 time per batch: 9.691  - 16 cores / 32 workers\n",
    "#Batch 22 device: xla:1 time passed: 275.780 time per batch: 12.535 - 16 cores / 8 workers\n",
    "#Batch 22 device: xla:1 time passed: 208.826 time per batch: 9.492  - 16 cores / 16 workers\n",
    "#Batch 22 device: xla:1 time passed: 245.750 time per batch: 11.170 - 16 cores / 12 workers\n",
    "#Batch 22 device: xla:1 time passed: 374.876 time per batch: 17.040 - 16 cores / 8 workers\n",
    "#Batch 22 device: xla:1 time passed: 400.221 time per batch: 18.192 - 8 cores / 8 workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs=16\n",
    "# ver 38, epoch 14, fold 0, train ll: 0.0365, val ll: 0.0694, cor: 0.8330, lr: 0.0002\n",
    "\n",
    "\n",
    "# back to 64, dropped dilated\n",
    "# ver 37, epoch 14, fold 2, train ll: 0.0367, val ll: 0.0641, cor: 0.8360, lr: 0.0002\n",
    "# ver 37, epoch 14, fold 1, train ll: 0.0374, val ll: 0.0665, cor: 0.8324, lr: 0.0002\n",
    "# ver 37, epoch 14, fold 0, train ll: 0.0360, val ll: 0.0676, cor: 0.8346, lr: 0.0002\n",
    "\n",
    "# 3x0.0001\n",
    "# ver 36, epoch 17, fold 2, train ll: 0.0364, val ll: 0.0646, cor: 0.8367\n",
    "# ver 36, epoch 16, fold 1, train ll: 0.0375, val ll: 0.0663, cor: 0.8326\n",
    "# ver 36, epoch 17, fold 0, train ll: 0.0358, val ll: 0.0677, cor: 0.8357\n",
    "\n",
    "# 7x0.02, 4x0.002, 3x0.0002\n",
    "# bs = 32\n",
    "# ver 36, epoch 14, fold 2, train ll: 0.0366, val ll: 0.0647, cor: 0.8362\n",
    "# ver 36, epoch 13, fold 1, train ll: 0.0379, val ll: 0.0663, cor: 0.8327\n",
    "# ver 36, epoch 14, fold 0, train ll: 0.0359, val ll: 0.0674, cor: 0.8354\n",
    "\n",
    "# 10x0.02, 6x0.002, 3x0.0002\n",
    "# bs = 64\n",
    "# ver 34, epoch 19, train ll: 0.0363, val ll: 0.0675, cor: 0.8347\n",
    "\n",
    "#--- dataset 2\n",
    "\n",
    "# 13x0.02, 6x0.002, 3x0.0002\n",
    "# bs = 64\n",
    "# ver 32, epoch 22, train ll: 0.0375, val ll: 0.0582, cor: 0.8480\n",
    "\n",
    "#--- dataset 1\n",
    "\n",
    "# 10x0.02, 8x0.002, 3x0.0002\n",
    "# with augmentations\n",
    "# ver 31, epoch 21, train ll: 0.0866, val ll: 0.1533, cor: 0.5319\n",
    "\n",
    "# 10x0.02, 6x0.002, 3x0.0002\n",
    "# 32 to 64 for conv2D reduce before, w dilated\n",
    "# ver 26, epoch 19, train ll: 0.0867, val ll: 0.1531, cor: 0.5305\n",
    "\n",
    "#--- dataset 2\n",
    "\n",
    "# 10x0.02, 3x0.002, 6x0.0002 (+10x0.0002)\n",
    "# back to orig best\n",
    "# epoch 19, train ll: 0.0389, val ll: 0.0602, cor: 0.8424\n",
    "# epoch 29, train ll: 0.0378, val ll: 0.0593, cor: 0.8448\n",
    "\n",
    "# 3x0.05, 5x0.02, 3x0.002, 6x0.0002\n",
    "# reduced all sizes\n",
    "# epoch 17, train ll: 0.0407, val ll: 0.0620, cor: 0.8371\n",
    "\n",
    "# 3x0.05, 5x0.02, 3x0.002, 6x0.0002\n",
    "# 32 to 64 for conv2D reduce before, w dilated\n",
    "# epoch 17, train ll: 0.0379, val ll: 0.0589, cor: 0.8460\n",
    "\n",
    "# 3x0.1, 5x0.02, 3x0.002, 6x0.0002\n",
    "# dilated, 3xTTA\n",
    "# epoch 17, train ll: 0.0364, val ll: 0.0582, cor: 0.8454, LB 0.066\n",
    "\n",
    "# train ll: 0.0354, val ll: 0.0577, cor: 0.8462, LB 0.065"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-cycle\n",
    "# copy latest model to GS code\n",
    "# improve black image meta data\n",
    "# freeze bias approach?\n",
    "# pseudo-labelling?\n",
    "# normalize metadata outliers\n",
    "# try GCP fast guide connecting\n",
    "# 32 TTA\n",
    "# bs 16\n",
    "# add AUC metric, check mixup on it\n",
    "\n",
    "# Yuval: zoom in, squish, perspective wraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epochs: 0 starting now: 7\n",
      "DataSet 2 train size 13042 fold 0\n",
      "adding dummy serieses 40\n",
      "DataSet 2 valid size 6528 fold 0\n",
      "setFeats, augmentation 0\n",
      "dataset train: 13042 valid: 6528 loader train: 815 valid: 408\n",
      "starting from scratch\n",
      "setFeats, augmentation 0\n",
      "Batch 4 device: xla:1 time passed: 11.256 time per batch: 2.814\n",
      "Batch 8 device: xla:1 time passed: 11.892 time per batch: 1.487\n",
      "Batch 12 device: xla:1 time passed: 12.428 time per batch: 1.036\n",
      "Batch 16 device: xla:1 time passed: 12.887 time per batch: 0.805\n",
      "Batch 20 device: xla:1 time passed: 13.531 time per batch: 0.677\n",
      "Batch 24 device: xla:1 time passed: 15.742 time per batch: 0.656\n",
      "Batch 28 device: xla:1 time passed: 18.121 time per batch: 0.647\n",
      "Batch 32 device: xla:1 time passed: 20.512 time per batch: 0.641\n",
      "Batch 36 device: xla:1 time passed: 22.959 time per batch: 0.638\n",
      "Batch 40 device: xla:1 time passed: 25.365 time per batch: 0.634\n",
      "Batch 44 device: xla:1 time passed: 27.715 time per batch: 0.630\n",
      "Batch 48 device: xla:1 time passed: 30.192 time per batch: 0.629\n",
      "Batch 52 device: xla:1 time passed: 32.628 time per batch: 0.627\n",
      "Batch 56 device: xla:1 time passed: 35.038 time per batch: 0.626\n",
      "Batch 60 device: xla:1 time passed: 37.354 time per batch: 0.623\n",
      "Batch 64 device: xla:1 time passed: 39.742 time per batch: 0.621\n",
      "Batch 68 device: xla:1 time passed: 42.144 time per batch: 0.620\n",
      "Batch 72 device: xla:1 time passed: 44.538 time per batch: 0.619\n",
      "Batch 76 device: xla:1 time passed: 46.911 time per batch: 0.617\n",
      "Batch 80 device: xla:1 time passed: 49.224 time per batch: 0.615\n",
      "Batch 84 device: xla:1 time passed: 51.650 time per batch: 0.615\n",
      "Batch 88 device: xla:1 time passed: 53.979 time per batch: 0.613\n",
      "Batch 92 device: xla:1 time passed: 56.349 time per batch: 0.612\n",
      "Batch 96 device: xla:1 time passed: 58.691 time per batch: 0.611\n",
      "Batch 100 device: xla:1 time passed: 61.195 time per batch: 0.612\n",
      "Batch 4 device: xla:1 time passed: 4.834 time per batch: 1.208\n",
      "Batch 8 device: xla:1 time passed: 5.311 time per batch: 0.664\n",
      "Batch 12 device: xla:1 time passed: 5.538 time per batch: 0.461\n",
      "Batch 16 device: xla:1 time passed: 7.111 time per batch: 0.444\n",
      "Batch 20 device: xla:1 time passed: 8.546 time per batch: 0.427\n",
      "Batch 24 device: xla:1 time passed: 10.089 time per batch: 0.420\n",
      "Batch 28 device: xla:1 time passed: 11.523 time per batch: 0.412\n",
      "Batch 32 device: xla:1 time passed: 13.019 time per batch: 0.407\n",
      "Batch 36 device: xla:1 time passed: 14.648 time per batch: 0.407\n",
      "Batch 40 device: xla:1 time passed: 15.967 time per batch: 0.399\n",
      "Batch 44 device: xla:1 time passed: 17.535 time per batch: 0.399\n",
      "Batch 48 device: xla:1 time passed: 19.023 time per batch: 0.396\n",
      "ver 38, epoch 1, fold 0, train ll: 0.1172, val ll: 0.1889, cor: 0.6793, lr: 0.02\n",
      "setFeats, augmentation 1\n",
      "Batch 4 device: xla:1 time passed: 5.860 time per batch: 1.465\n",
      "Batch 8 device: xla:1 time passed: 6.060 time per batch: 0.758\n",
      "Batch 12 device: xla:1 time passed: 8.323 time per batch: 0.694\n",
      "Batch 16 device: xla:1 time passed: 10.644 time per batch: 0.665\n",
      "Batch 20 device: xla:1 time passed: 13.018 time per batch: 0.651\n",
      "Batch 24 device: xla:1 time passed: 15.460 time per batch: 0.644\n",
      "Batch 28 device: xla:1 time passed: 17.835 time per batch: 0.637\n",
      "Batch 32 device: xla:1 time passed: 20.283 time per batch: 0.634\n",
      "Batch 36 device: xla:1 time passed: 22.632 time per batch: 0.629\n",
      "Batch 40 device: xla:1 time passed: 25.136 time per batch: 0.628\n",
      "Batch 44 device: xla:1 time passed: 27.435 time per batch: 0.624\n",
      "Batch 48 device: xla:1 time passed: 29.833 time per batch: 0.622\n",
      "Batch 52 device: xla:1 time passed: 32.283 time per batch: 0.621\n",
      "Batch 56 device: xla:1 time passed: 34.636 time per batch: 0.618\n",
      "Batch 60 device: xla:1 time passed: 37.055 time per batch: 0.618\n",
      "Batch 64 device: xla:1 time passed: 39.441 time per batch: 0.616\n",
      "Batch 68 device: xla:1 time passed: 41.786 time per batch: 0.615\n",
      "Batch 72 device: xla:1 time passed: 44.195 time per batch: 0.614\n",
      "Batch 76 device: xla:1 time passed: 46.605 time per batch: 0.613\n",
      "Batch 80 device: xla:1 time passed: 48.949 time per batch: 0.612\n",
      "Batch 84 device: xla:1 time passed: 51.370 time per batch: 0.612\n",
      "Batch 88 device: xla:1 time passed: 53.719 time per batch: 0.610\n",
      "Batch 92 device: xla:1 time passed: 56.074 time per batch: 0.610\n",
      "Batch 96 device: xla:1 time passed: 58.504 time per batch: 0.609\n",
      "Batch 100 device: xla:1 time passed: 60.892 time per batch: 0.609\n",
      "Batch 4 device: xla:1 time passed: 2.708 time per batch: 0.677\n",
      "Batch 8 device: xla:1 time passed: 3.959 time per batch: 0.495\n",
      "Batch 12 device: xla:1 time passed: 5.675 time per batch: 0.473\n",
      "Batch 16 device: xla:1 time passed: 7.124 time per batch: 0.445\n",
      "Batch 20 device: xla:1 time passed: 8.456 time per batch: 0.423\n",
      "Batch 24 device: xla:1 time passed: 9.882 time per batch: 0.412\n",
      "Batch 28 device: xla:1 time passed: 11.401 time per batch: 0.407\n",
      "Batch 32 device: xla:1 time passed: 12.890 time per batch: 0.403\n",
      "Batch 36 device: xla:1 time passed: 14.441 time per batch: 0.401\n",
      "Batch 40 device: xla:1 time passed: 15.809 time per batch: 0.395\n",
      "Batch 44 device: xla:1 time passed: 17.374 time per batch: 0.395\n",
      "Batch 48 device: xla:1 time passed: 18.812 time per batch: 0.392\n",
      "ver 38, epoch 2, fold 0, train ll: 0.0467, val ll: 0.0770, cor: 0.8038, lr: 0.02\n",
      "setFeats, augmentation 2\n",
      "Batch 4 device: xla:1 time passed: 3.646 time per batch: 0.911\n",
      "Batch 8 device: xla:1 time passed: 5.868 time per batch: 0.734\n",
      "Batch 12 device: xla:1 time passed: 8.242 time per batch: 0.687\n",
      "Batch 16 device: xla:1 time passed: 10.576 time per batch: 0.661\n",
      "Batch 20 device: xla:1 time passed: 12.960 time per batch: 0.648\n",
      "Batch 24 device: xla:1 time passed: 15.400 time per batch: 0.642\n",
      "Batch 28 device: xla:1 time passed: 17.842 time per batch: 0.637\n",
      "Batch 32 device: xla:1 time passed: 20.305 time per batch: 0.635\n",
      "Batch 36 device: xla:1 time passed: 22.663 time per batch: 0.630\n",
      "Batch 40 device: xla:1 time passed: 24.940 time per batch: 0.624\n",
      "Batch 44 device: xla:1 time passed: 27.353 time per batch: 0.622\n",
      "Batch 48 device: xla:1 time passed: 29.862 time per batch: 0.622\n",
      "Batch 52 device: xla:1 time passed: 32.205 time per batch: 0.619\n",
      "Batch 56 device: xla:1 time passed: 34.567 time per batch: 0.617\n",
      "Batch 60 device: xla:1 time passed: 36.969 time per batch: 0.616\n",
      "Batch 64 device: xla:1 time passed: 39.423 time per batch: 0.616\n",
      "Batch 68 device: xla:1 time passed: 41.763 time per batch: 0.614\n",
      "Batch 72 device: xla:1 time passed: 44.171 time per batch: 0.613\n",
      "Batch 76 device: xla:1 time passed: 46.511 time per batch: 0.612\n",
      "Batch 80 device: xla:1 time passed: 49.095 time per batch: 0.614\n",
      "Batch 84 device: xla:1 time passed: 51.317 time per batch: 0.611\n",
      "Batch 88 device: xla:1 time passed: 53.661 time per batch: 0.610\n",
      "Batch 92 device: xla:1 time passed: 56.067 time per batch: 0.609\n",
      "Batch 96 device: xla:1 time passed: 58.425 time per batch: 0.609\n",
      "Batch 100 device: xla:1 time passed: 60.824 time per batch: 0.608\n",
      "Batch 4 device: xla:1 time passed: 2.673 time per batch: 0.668\n",
      "Batch 8 device: xla:1 time passed: 3.933 time per batch: 0.492\n",
      "Batch 12 device: xla:1 time passed: 5.372 time per batch: 0.448\n",
      "Batch 16 device: xla:1 time passed: 6.895 time per batch: 0.431\n",
      "Batch 20 device: xla:1 time passed: 8.336 time per batch: 0.417\n",
      "Batch 24 device: xla:1 time passed: 10.029 time per batch: 0.418\n",
      "Batch 28 device: xla:1 time passed: 11.258 time per batch: 0.402\n",
      "Batch 32 device: xla:1 time passed: 12.782 time per batch: 0.399\n",
      "Batch 36 device: xla:1 time passed: 14.227 time per batch: 0.395\n",
      "Batch 40 device: xla:1 time passed: 15.688 time per batch: 0.392\n",
      "Batch 44 device: xla:1 time passed: 17.219 time per batch: 0.391\n",
      "Batch 48 device: xla:1 time passed: 18.635 time per batch: 0.388\n",
      "ver 38, epoch 3, fold 0, train ll: 0.0452, val ll: 0.0745, cor: 0.8147, lr: 0.02\n",
      "setFeats, augmentation 3\n",
      "Batch 4 device: xla:1 time passed: 3.662 time per batch: 0.916\n",
      "Batch 8 device: xla:1 time passed: 5.808 time per batch: 0.726\n",
      "Batch 12 device: xla:1 time passed: 8.184 time per batch: 0.682\n",
      "Batch 16 device: xla:1 time passed: 10.673 time per batch: 0.667\n",
      "Batch 20 device: xla:1 time passed: 13.115 time per batch: 0.656\n",
      "Batch 24 device: xla:1 time passed: 15.452 time per batch: 0.644\n",
      "Batch 28 device: xla:1 time passed: 17.853 time per batch: 0.638\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 32 device: xla:1 time passed: 20.457 time per batch: 0.639\n",
      "Batch 36 device: xla:1 time passed: 22.716 time per batch: 0.631\n",
      "Batch 40 device: xla:1 time passed: 25.002 time per batch: 0.625\n",
      "Batch 44 device: xla:1 time passed: 27.354 time per batch: 0.622\n",
      "Batch 48 device: xla:1 time passed: 29.774 time per batch: 0.620\n",
      "Batch 52 device: xla:1 time passed: 32.193 time per batch: 0.619\n",
      "Batch 56 device: xla:1 time passed: 34.572 time per batch: 0.617\n",
      "Batch 60 device: xla:1 time passed: 37.027 time per batch: 0.617\n",
      "Batch 64 device: xla:1 time passed: 39.444 time per batch: 0.616\n",
      "Batch 68 device: xla:1 time passed: 41.776 time per batch: 0.614\n",
      "Batch 72 device: xla:1 time passed: 44.180 time per batch: 0.614\n",
      "Batch 76 device: xla:1 time passed: 46.597 time per batch: 0.613\n",
      "Batch 80 device: xla:1 time passed: 49.019 time per batch: 0.613\n",
      "Batch 84 device: xla:1 time passed: 51.442 time per batch: 0.612\n",
      "Batch 88 device: xla:1 time passed: 53.825 time per batch: 0.612\n",
      "Batch 92 device: xla:1 time passed: 56.210 time per batch: 0.611\n",
      "Batch 96 device: xla:1 time passed: 58.651 time per batch: 0.611\n",
      "Batch 100 device: xla:1 time passed: 61.041 time per batch: 0.610\n",
      "Batch 4 device: xla:1 time passed: 2.587 time per batch: 0.647\n",
      "Batch 8 device: xla:1 time passed: 3.904 time per batch: 0.488\n",
      "Batch 12 device: xla:1 time passed: 5.387 time per batch: 0.449\n",
      "Batch 16 device: xla:1 time passed: 6.887 time per batch: 0.430\n",
      "Batch 20 device: xla:1 time passed: 8.441 time per batch: 0.422\n",
      "Batch 24 device: xla:1 time passed: 9.926 time per batch: 0.414\n",
      "Batch 28 device: xla:1 time passed: 11.398 time per batch: 0.407\n",
      "Batch 32 device: xla:1 time passed: 12.886 time per batch: 0.403\n",
      "Batch 36 device: xla:1 time passed: 14.337 time per batch: 0.398\n",
      "Batch 40 device: xla:1 time passed: 15.839 time per batch: 0.396\n",
      "Batch 44 device: xla:1 time passed: 17.353 time per batch: 0.394\n",
      "Batch 48 device: xla:1 time passed: 18.832 time per batch: 0.392\n",
      "ver 38, epoch 4, fold 0, train ll: 0.0446, val ll: 0.0743, cor: 0.8213, lr: 0.02\n",
      "setFeats, augmentation 0\n",
      "Batch 4 device: xla:1 time passed: 3.727 time per batch: 0.932\n",
      "Batch 8 device: xla:1 time passed: 5.922 time per batch: 0.740\n",
      "Batch 12 device: xla:1 time passed: 8.245 time per batch: 0.687\n",
      "Batch 16 device: xla:1 time passed: 10.704 time per batch: 0.669\n",
      "Batch 20 device: xla:1 time passed: 13.103 time per batch: 0.655\n",
      "Batch 24 device: xla:1 time passed: 15.551 time per batch: 0.648\n",
      "Batch 28 device: xla:1 time passed: 17.923 time per batch: 0.640\n",
      "Batch 32 device: xla:1 time passed: 20.226 time per batch: 0.632\n",
      "Batch 36 device: xla:1 time passed: 22.693 time per batch: 0.630\n",
      "Batch 40 device: xla:1 time passed: 25.042 time per batch: 0.626\n",
      "Batch 44 device: xla:1 time passed: 27.392 time per batch: 0.623\n",
      "Batch 48 device: xla:1 time passed: 29.830 time per batch: 0.621\n",
      "Batch 52 device: xla:1 time passed: 32.192 time per batch: 0.619\n",
      "Batch 56 device: xla:1 time passed: 34.552 time per batch: 0.617\n",
      "Batch 60 device: xla:1 time passed: 36.951 time per batch: 0.616\n",
      "Batch 64 device: xla:1 time passed: 39.346 time per batch: 0.615\n",
      "Batch 68 device: xla:1 time passed: 41.775 time per batch: 0.614\n",
      "Batch 72 device: xla:1 time passed: 44.276 time per batch: 0.615\n",
      "Batch 76 device: xla:1 time passed: 46.556 time per batch: 0.613\n",
      "Batch 80 device: xla:1 time passed: 48.909 time per batch: 0.611\n",
      "Batch 84 device: xla:1 time passed: 51.261 time per batch: 0.610\n",
      "Batch 88 device: xla:1 time passed: 53.693 time per batch: 0.610\n",
      "Batch 92 device: xla:1 time passed: 56.124 time per batch: 0.610\n",
      "Batch 96 device: xla:1 time passed: 58.514 time per batch: 0.610\n",
      "Batch 100 device: xla:1 time passed: 60.890 time per batch: 0.609\n",
      "Batch 4 device: xla:1 time passed: 2.741 time per batch: 0.685\n",
      "Batch 8 device: xla:1 time passed: 3.942 time per batch: 0.493\n",
      "Batch 12 device: xla:1 time passed: 5.432 time per batch: 0.453\n",
      "Batch 16 device: xla:1 time passed: 6.911 time per batch: 0.432\n",
      "Batch 20 device: xla:1 time passed: 8.415 time per batch: 0.421\n",
      "Batch 24 device: xla:1 time passed: 9.997 time per batch: 0.417\n",
      "Batch 28 device: xla:1 time passed: 11.379 time per batch: 0.406\n",
      "Batch 32 device: xla:1 time passed: 12.871 time per batch: 0.402\n",
      "Batch 36 device: xla:1 time passed: 14.368 time per batch: 0.399\n",
      "Batch 40 device: xla:1 time passed: 15.867 time per batch: 0.397\n",
      "Batch 44 device: xla:1 time passed: 17.405 time per batch: 0.396\n",
      "Batch 48 device: xla:1 time passed: 18.975 time per batch: 0.395\n",
      "ver 38, epoch 5, fold 0, train ll: 0.0448, val ll: 0.0733, cor: 0.8158, lr: 0.02\n",
      "setFeats, augmentation 1\n",
      "Batch 4 device: xla:1 time passed: 3.669 time per batch: 0.917\n",
      "Batch 8 device: xla:1 time passed: 5.894 time per batch: 0.737\n",
      "Batch 12 device: xla:1 time passed: 8.263 time per batch: 0.689\n",
      "Batch 16 device: xla:1 time passed: 10.684 time per batch: 0.668\n",
      "Batch 20 device: xla:1 time passed: 13.078 time per batch: 0.654\n",
      "Batch 24 device: xla:1 time passed: 15.500 time per batch: 0.646\n",
      "Batch 28 device: xla:1 time passed: 17.909 time per batch: 0.640\n",
      "Batch 32 device: xla:1 time passed: 20.300 time per batch: 0.634\n",
      "Batch 36 device: xla:1 time passed: 22.704 time per batch: 0.631\n",
      "Batch 40 device: xla:1 time passed: 25.097 time per batch: 0.627\n",
      "Batch 44 device: xla:1 time passed: 27.438 time per batch: 0.624\n",
      "Batch 48 device: xla:1 time passed: 29.796 time per batch: 0.621\n",
      "Batch 52 device: xla:1 time passed: 32.176 time per batch: 0.619\n",
      "Batch 56 device: xla:1 time passed: 34.581 time per batch: 0.618\n",
      "Batch 60 device: xla:1 time passed: 37.044 time per batch: 0.617\n",
      "Batch 64 device: xla:1 time passed: 39.441 time per batch: 0.616\n",
      "Batch 68 device: xla:1 time passed: 41.793 time per batch: 0.615\n",
      "Batch 72 device: xla:1 time passed: 44.124 time per batch: 0.613\n",
      "Batch 76 device: xla:1 time passed: 46.553 time per batch: 0.613\n",
      "Batch 80 device: xla:1 time passed: 48.990 time per batch: 0.612\n",
      "Batch 84 device: xla:1 time passed: 51.357 time per batch: 0.611\n",
      "Batch 88 device: xla:1 time passed: 53.746 time per batch: 0.611\n",
      "Batch 92 device: xla:1 time passed: 56.201 time per batch: 0.611\n",
      "Batch 96 device: xla:1 time passed: 58.535 time per batch: 0.610\n",
      "Batch 100 device: xla:1 time passed: 60.857 time per batch: 0.609\n",
      "Batch 4 device: xla:1 time passed: 2.747 time per batch: 0.687\n",
      "Batch 8 device: xla:1 time passed: 3.976 time per batch: 0.497\n",
      "Batch 12 device: xla:1 time passed: 5.402 time per batch: 0.450\n",
      "Batch 16 device: xla:1 time passed: 6.890 time per batch: 0.431\n",
      "Batch 20 device: xla:1 time passed: 8.416 time per batch: 0.421\n",
      "Batch 24 device: xla:1 time passed: 9.918 time per batch: 0.413\n",
      "Batch 28 device: xla:1 time passed: 11.532 time per batch: 0.412\n",
      "Batch 32 device: xla:1 time passed: 12.932 time per batch: 0.404\n",
      "Batch 36 device: xla:1 time passed: 14.569 time per batch: 0.405\n",
      "Batch 40 device: xla:1 time passed: 15.888 time per batch: 0.397\n",
      "Batch 44 device: xla:1 time passed: 17.449 time per batch: 0.397\n",
      "Batch 48 device: xla:1 time passed: 18.903 time per batch: 0.394\n",
      "ver 38, epoch 6, fold 0, train ll: 0.0455, val ll: 0.0725, cor: 0.8184, lr: 0.02\n",
      "setFeats, augmentation 2\n",
      "Batch 4 device: xla:1 time passed: 3.682 time per batch: 0.920\n",
      "Batch 8 device: xla:1 time passed: 5.869 time per batch: 0.734\n",
      "Batch 12 device: xla:1 time passed: 8.231 time per batch: 0.686\n",
      "Batch 16 device: xla:1 time passed: 10.667 time per batch: 0.667\n",
      "Batch 20 device: xla:1 time passed: 13.052 time per batch: 0.653\n",
      "Batch 24 device: xla:1 time passed: 15.423 time per batch: 0.643\n",
      "Batch 28 device: xla:1 time passed: 17.789 time per batch: 0.635\n",
      "Batch 32 device: xla:1 time passed: 20.199 time per batch: 0.631\n",
      "Batch 36 device: xla:1 time passed: 22.590 time per batch: 0.628\n",
      "Batch 40 device: xla:1 time passed: 24.961 time per batch: 0.624\n",
      "Batch 44 device: xla:1 time passed: 27.328 time per batch: 0.621\n",
      "Batch 48 device: xla:1 time passed: 29.667 time per batch: 0.618\n",
      "Batch 52 device: xla:1 time passed: 32.053 time per batch: 0.616\n",
      "Batch 56 device: xla:1 time passed: 34.516 time per batch: 0.616\n",
      "Batch 60 device: xla:1 time passed: 36.790 time per batch: 0.613\n",
      "Batch 64 device: xla:1 time passed: 39.265 time per batch: 0.614\n",
      "Batch 68 device: xla:1 time passed: 41.616 time per batch: 0.612\n",
      "Batch 72 device: xla:1 time passed: 44.016 time per batch: 0.611\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 76 device: xla:1 time passed: 46.444 time per batch: 0.611\n",
      "Batch 80 device: xla:1 time passed: 48.826 time per batch: 0.610\n",
      "Batch 84 device: xla:1 time passed: 51.202 time per batch: 0.610\n",
      "Batch 88 device: xla:1 time passed: 53.561 time per batch: 0.609\n",
      "Batch 92 device: xla:1 time passed: 55.932 time per batch: 0.608\n",
      "Batch 96 device: xla:1 time passed: 58.363 time per batch: 0.608\n",
      "Batch 100 device: xla:1 time passed: 60.707 time per batch: 0.607\n",
      "Batch 4 device: xla:1 time passed: 2.690 time per batch: 0.672\n",
      "Batch 8 device: xla:1 time passed: 3.883 time per batch: 0.485\n",
      "Batch 12 device: xla:1 time passed: 5.403 time per batch: 0.450\n",
      "Batch 16 device: xla:1 time passed: 6.873 time per batch: 0.430\n",
      "Batch 20 device: xla:1 time passed: 8.379 time per batch: 0.419\n",
      "Batch 24 device: xla:1 time passed: 9.870 time per batch: 0.411\n",
      "Batch 28 device: xla:1 time passed: 11.417 time per batch: 0.408\n",
      "Batch 32 device: xla:1 time passed: 12.851 time per batch: 0.402\n",
      "Batch 36 device: xla:1 time passed: 14.457 time per batch: 0.402\n",
      "Batch 40 device: xla:1 time passed: 15.866 time per batch: 0.397\n",
      "Batch 44 device: xla:1 time passed: 17.586 time per batch: 0.400\n",
      "Batch 48 device: xla:1 time passed: 18.915 time per batch: 0.394\n",
      "ver 38, epoch 7, fold 0, train ll: 0.0453, val ll: 0.0760, cor: 0.8048, lr: 0.02\n",
      "total running time 583.3061954975128\n",
      "completed epochs: 7 starting now: 4\n",
      "DataSet 2 train size 13042 fold 0\n",
      "adding dummy serieses 40\n",
      "DataSet 2 valid size 6528 fold 0\n",
      "setFeats, augmentation 0\n",
      "dataset train: 13042 valid: 6528 loader train: 815 valid: 408\n",
      "loading model model.b7.f0.v38\n",
      "setFeats, augmentation 3\n",
      "Batch 4 device: xla:1 time passed: 3.880 time per batch: 0.970\n",
      "Batch 8 device: xla:1 time passed: 6.101 time per batch: 0.763\n",
      "Batch 12 device: xla:1 time passed: 8.460 time per batch: 0.705\n",
      "Batch 16 device: xla:1 time passed: 10.834 time per batch: 0.677\n",
      "Batch 20 device: xla:1 time passed: 13.220 time per batch: 0.661\n",
      "Batch 24 device: xla:1 time passed: 15.605 time per batch: 0.650\n",
      "Batch 28 device: xla:1 time passed: 18.011 time per batch: 0.643\n",
      "Batch 32 device: xla:1 time passed: 20.422 time per batch: 0.638\n",
      "Batch 36 device: xla:1 time passed: 22.722 time per batch: 0.631\n",
      "Batch 40 device: xla:1 time passed: 25.218 time per batch: 0.630\n",
      "Batch 44 device: xla:1 time passed: 27.532 time per batch: 0.626\n",
      "Batch 48 device: xla:1 time passed: 29.931 time per batch: 0.624\n",
      "Batch 52 device: xla:1 time passed: 32.339 time per batch: 0.622\n",
      "Batch 56 device: xla:1 time passed: 34.719 time per batch: 0.620\n",
      "Batch 60 device: xla:1 time passed: 37.185 time per batch: 0.620\n",
      "Batch 64 device: xla:1 time passed: 39.595 time per batch: 0.619\n",
      "Batch 68 device: xla:1 time passed: 41.974 time per batch: 0.617\n",
      "Batch 72 device: xla:1 time passed: 44.372 time per batch: 0.616\n",
      "Batch 76 device: xla:1 time passed: 46.771 time per batch: 0.615\n",
      "Batch 80 device: xla:1 time passed: 49.188 time per batch: 0.615\n",
      "Batch 84 device: xla:1 time passed: 51.592 time per batch: 0.614\n",
      "Batch 88 device: xla:1 time passed: 53.920 time per batch: 0.613\n",
      "Batch 92 device: xla:1 time passed: 56.327 time per batch: 0.612\n",
      "Batch 96 device: xla:1 time passed: 58.689 time per batch: 0.611\n",
      "Batch 100 device: xla:1 time passed: 61.002 time per batch: 0.610\n",
      "Batch 4 device: xla:1 time passed: 2.702 time per batch: 0.675\n",
      "Batch 8 device: xla:1 time passed: 3.919 time per batch: 0.490\n",
      "Batch 12 device: xla:1 time passed: 5.397 time per batch: 0.450\n",
      "Batch 16 device: xla:1 time passed: 6.876 time per batch: 0.430\n",
      "Batch 20 device: xla:1 time passed: 8.422 time per batch: 0.421\n",
      "Batch 24 device: xla:1 time passed: 9.897 time per batch: 0.412\n",
      "Batch 28 device: xla:1 time passed: 11.387 time per batch: 0.407\n",
      "Batch 32 device: xla:1 time passed: 12.908 time per batch: 0.403\n",
      "Batch 36 device: xla:1 time passed: 14.370 time per batch: 0.399\n",
      "Batch 40 device: xla:1 time passed: 15.827 time per batch: 0.396\n",
      "Batch 44 device: xla:1 time passed: 17.307 time per batch: 0.393\n",
      "Batch 48 device: xla:1 time passed: 18.804 time per batch: 0.392\n",
      "ver 38, epoch 8, fold 0, train ll: 0.0395, val ll: 0.0688, cor: 0.8314, lr: 0.002\n",
      "setFeats, augmentation 0\n",
      "Batch 4 device: xla:1 time passed: 3.669 time per batch: 0.917\n",
      "Batch 8 device: xla:1 time passed: 5.842 time per batch: 0.730\n",
      "Batch 12 device: xla:1 time passed: 8.240 time per batch: 0.687\n",
      "Batch 16 device: xla:1 time passed: 10.663 time per batch: 0.666\n",
      "Batch 20 device: xla:1 time passed: 13.015 time per batch: 0.651\n",
      "Batch 24 device: xla:1 time passed: 15.500 time per batch: 0.646\n",
      "Batch 28 device: xla:1 time passed: 17.829 time per batch: 0.637\n",
      "Batch 32 device: xla:1 time passed: 20.139 time per batch: 0.629\n",
      "Batch 36 device: xla:1 time passed: 22.532 time per batch: 0.626\n",
      "Batch 40 device: xla:1 time passed: 24.936 time per batch: 0.623\n",
      "Batch 44 device: xla:1 time passed: 27.275 time per batch: 0.620\n",
      "Batch 48 device: xla:1 time passed: 29.664 time per batch: 0.618\n",
      "Batch 52 device: xla:1 time passed: 32.080 time per batch: 0.617\n",
      "Batch 56 device: xla:1 time passed: 34.475 time per batch: 0.616\n",
      "Batch 60 device: xla:1 time passed: 36.849 time per batch: 0.614\n",
      "Batch 64 device: xla:1 time passed: 39.233 time per batch: 0.613\n",
      "Batch 68 device: xla:1 time passed: 41.646 time per batch: 0.612\n",
      "Batch 72 device: xla:1 time passed: 44.062 time per batch: 0.612\n",
      "Batch 76 device: xla:1 time passed: 46.469 time per batch: 0.611\n",
      "Batch 80 device: xla:1 time passed: 48.834 time per batch: 0.610\n",
      "Batch 84 device: xla:1 time passed: 51.278 time per batch: 0.610\n",
      "Batch 88 device: xla:1 time passed: 53.676 time per batch: 0.610\n",
      "Batch 92 device: xla:1 time passed: 56.075 time per batch: 0.610\n",
      "Batch 96 device: xla:1 time passed: 58.497 time per batch: 0.609\n",
      "Batch 100 device: xla:1 time passed: 60.921 time per batch: 0.609\n",
      "Batch 4 device: xla:1 time passed: 2.726 time per batch: 0.682\n",
      "Batch 8 device: xla:1 time passed: 3.954 time per batch: 0.494\n",
      "Batch 12 device: xla:1 time passed: 5.437 time per batch: 0.453\n",
      "Batch 16 device: xla:1 time passed: 6.886 time per batch: 0.430\n",
      "Batch 20 device: xla:1 time passed: 8.400 time per batch: 0.420\n",
      "Batch 24 device: xla:1 time passed: 9.919 time per batch: 0.413\n",
      "Batch 28 device: xla:1 time passed: 11.402 time per batch: 0.407\n",
      "Batch 32 device: xla:1 time passed: 12.915 time per batch: 0.404\n",
      "Batch 36 device: xla:1 time passed: 14.349 time per batch: 0.399\n",
      "Batch 40 device: xla:1 time passed: 15.806 time per batch: 0.395\n",
      "Batch 44 device: xla:1 time passed: 17.344 time per batch: 0.394\n",
      "Batch 48 device: xla:1 time passed: 18.752 time per batch: 0.391\n",
      "ver 38, epoch 9, fold 0, train ll: 0.0386, val ll: 0.0691, cor: 0.8324, lr: 0.002\n",
      "setFeats, augmentation 1\n",
      "Batch 4 device: xla:1 time passed: 3.740 time per batch: 0.935\n",
      "Batch 8 device: xla:1 time passed: 5.860 time per batch: 0.732\n",
      "Batch 12 device: xla:1 time passed: 8.272 time per batch: 0.689\n",
      "Batch 16 device: xla:1 time passed: 10.664 time per batch: 0.667\n",
      "Batch 20 device: xla:1 time passed: 13.089 time per batch: 0.654\n",
      "Batch 24 device: xla:1 time passed: 15.479 time per batch: 0.645\n",
      "Batch 28 device: xla:1 time passed: 17.878 time per batch: 0.638\n",
      "Batch 32 device: xla:1 time passed: 20.298 time per batch: 0.634\n",
      "Batch 36 device: xla:1 time passed: 22.654 time per batch: 0.629\n",
      "Batch 40 device: xla:1 time passed: 25.080 time per batch: 0.627\n",
      "Batch 44 device: xla:1 time passed: 27.459 time per batch: 0.624\n",
      "Batch 48 device: xla:1 time passed: 29.862 time per batch: 0.622\n",
      "Batch 52 device: xla:1 time passed: 32.218 time per batch: 0.620\n",
      "Batch 56 device: xla:1 time passed: 34.665 time per batch: 0.619\n",
      "Batch 60 device: xla:1 time passed: 37.040 time per batch: 0.617\n",
      "Batch 64 device: xla:1 time passed: 39.385 time per batch: 0.615\n",
      "Batch 68 device: xla:1 time passed: 41.840 time per batch: 0.615\n",
      "Batch 72 device: xla:1 time passed: 44.258 time per batch: 0.615\n",
      "Batch 76 device: xla:1 time passed: 46.675 time per batch: 0.614\n",
      "Batch 80 device: xla:1 time passed: 49.040 time per batch: 0.613\n",
      "Batch 84 device: xla:1 time passed: 51.504 time per batch: 0.613\n",
      "Batch 88 device: xla:1 time passed: 53.930 time per batch: 0.613\n",
      "Batch 92 device: xla:1 time passed: 56.274 time per batch: 0.612\n",
      "Batch 96 device: xla:1 time passed: 58.752 time per batch: 0.612\n",
      "Batch 100 device: xla:1 time passed: 61.045 time per batch: 0.610\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 4 device: xla:1 time passed: 2.726 time per batch: 0.681\n",
      "Batch 8 device: xla:1 time passed: 3.982 time per batch: 0.498\n",
      "Batch 12 device: xla:1 time passed: 5.439 time per batch: 0.453\n",
      "Batch 16 device: xla:1 time passed: 7.109 time per batch: 0.444\n",
      "Batch 20 device: xla:1 time passed: 8.427 time per batch: 0.421\n",
      "Batch 24 device: xla:1 time passed: 9.969 time per batch: 0.415\n",
      "Batch 28 device: xla:1 time passed: 11.399 time per batch: 0.407\n",
      "Batch 32 device: xla:1 time passed: 12.986 time per batch: 0.406\n",
      "Batch 36 device: xla:1 time passed: 14.371 time per batch: 0.399\n",
      "Batch 40 device: xla:1 time passed: 15.907 time per batch: 0.398\n",
      "Batch 44 device: xla:1 time passed: 17.360 time per batch: 0.395\n",
      "Batch 48 device: xla:1 time passed: 18.837 time per batch: 0.392\n",
      "ver 38, epoch 10, fold 0, train ll: 0.0381, val ll: 0.0701, cor: 0.8305, lr: 0.002\n",
      "setFeats, augmentation 2\n",
      "Batch 4 device: xla:1 time passed: 3.734 time per batch: 0.933\n",
      "Batch 8 device: xla:1 time passed: 5.950 time per batch: 0.744\n",
      "Batch 12 device: xla:1 time passed: 8.290 time per batch: 0.691\n",
      "Batch 16 device: xla:1 time passed: 10.685 time per batch: 0.668\n",
      "Batch 20 device: xla:1 time passed: 13.146 time per batch: 0.657\n",
      "Batch 24 device: xla:1 time passed: 15.486 time per batch: 0.645\n",
      "Batch 28 device: xla:1 time passed: 17.840 time per batch: 0.637\n",
      "Batch 32 device: xla:1 time passed: 20.213 time per batch: 0.632\n",
      "Batch 36 device: xla:1 time passed: 22.670 time per batch: 0.630\n",
      "Batch 40 device: xla:1 time passed: 25.102 time per batch: 0.628\n",
      "Batch 44 device: xla:1 time passed: 27.448 time per batch: 0.624\n",
      "Batch 48 device: xla:1 time passed: 29.864 time per batch: 0.622\n",
      "Batch 52 device: xla:1 time passed: 32.235 time per batch: 0.620\n",
      "Batch 56 device: xla:1 time passed: 34.628 time per batch: 0.618\n",
      "Batch 60 device: xla:1 time passed: 37.062 time per batch: 0.618\n",
      "Batch 64 device: xla:1 time passed: 39.429 time per batch: 0.616\n",
      "Batch 68 device: xla:1 time passed: 41.796 time per batch: 0.615\n",
      "Batch 72 device: xla:1 time passed: 44.216 time per batch: 0.614\n",
      "Batch 76 device: xla:1 time passed: 46.623 time per batch: 0.613\n",
      "Batch 80 device: xla:1 time passed: 49.071 time per batch: 0.613\n",
      "Batch 84 device: xla:1 time passed: 51.517 time per batch: 0.613\n",
      "Batch 88 device: xla:1 time passed: 53.899 time per batch: 0.612\n",
      "Batch 92 device: xla:1 time passed: 56.345 time per batch: 0.612\n",
      "Batch 96 device: xla:1 time passed: 58.710 time per batch: 0.612\n",
      "Batch 100 device: xla:1 time passed: 61.091 time per batch: 0.611\n",
      "Batch 4 device: xla:1 time passed: 2.733 time per batch: 0.683\n",
      "Batch 8 device: xla:1 time passed: 3.924 time per batch: 0.490\n",
      "Batch 12 device: xla:1 time passed: 5.413 time per batch: 0.451\n",
      "Batch 16 device: xla:1 time passed: 6.880 time per batch: 0.430\n",
      "Batch 20 device: xla:1 time passed: 8.367 time per batch: 0.418\n",
      "Batch 24 device: xla:1 time passed: 9.844 time per batch: 0.410\n",
      "Batch 28 device: xla:1 time passed: 11.331 time per batch: 0.405\n",
      "Batch 32 device: xla:1 time passed: 12.798 time per batch: 0.400\n",
      "Batch 36 device: xla:1 time passed: 14.296 time per batch: 0.397\n",
      "Batch 40 device: xla:1 time passed: 15.819 time per batch: 0.395\n",
      "Batch 44 device: xla:1 time passed: 17.261 time per batch: 0.392\n",
      "Batch 48 device: xla:1 time passed: 18.748 time per batch: 0.391\n",
      "ver 38, epoch 11, fold 0, train ll: 0.0381, val ll: 0.0700, cor: 0.8312, lr: 0.002\n",
      "total running time 333.5600531101227\n",
      "completed epochs: 11 starting now: 3\n",
      "DataSet 2 train size 13042 fold 0\n",
      "adding dummy serieses 40\n",
      "DataSet 2 valid size 6528 fold 0\n",
      "setFeats, augmentation 0\n",
      "dataset train: 13042 valid: 6528 loader train: 815 valid: 408\n",
      "loading model model.b11.f0.v38\n",
      "setFeats, augmentation 3\n",
      "Batch 4 device: xla:1 time passed: 3.792 time per batch: 0.948\n",
      "Batch 8 device: xla:1 time passed: 5.985 time per batch: 0.748\n",
      "Batch 12 device: xla:1 time passed: 8.372 time per batch: 0.698\n",
      "Batch 16 device: xla:1 time passed: 10.770 time per batch: 0.673\n",
      "Batch 20 device: xla:1 time passed: 13.176 time per batch: 0.659\n",
      "Batch 24 device: xla:1 time passed: 15.542 time per batch: 0.648\n",
      "Batch 28 device: xla:1 time passed: 17.901 time per batch: 0.639\n",
      "Batch 32 device: xla:1 time passed: 20.278 time per batch: 0.634\n",
      "Batch 36 device: xla:1 time passed: 22.614 time per batch: 0.628\n",
      "Batch 40 device: xla:1 time passed: 25.034 time per batch: 0.626\n",
      "Batch 44 device: xla:1 time passed: 27.385 time per batch: 0.622\n",
      "Batch 48 device: xla:1 time passed: 29.853 time per batch: 0.622\n",
      "Batch 52 device: xla:1 time passed: 32.190 time per batch: 0.619\n",
      "Batch 56 device: xla:1 time passed: 34.539 time per batch: 0.617\n",
      "Batch 60 device: xla:1 time passed: 36.962 time per batch: 0.616\n",
      "Batch 64 device: xla:1 time passed: 39.391 time per batch: 0.615\n",
      "Batch 68 device: xla:1 time passed: 41.749 time per batch: 0.614\n",
      "Batch 72 device: xla:1 time passed: 44.147 time per batch: 0.613\n",
      "Batch 76 device: xla:1 time passed: 46.545 time per batch: 0.612\n",
      "Batch 80 device: xla:1 time passed: 48.980 time per batch: 0.612\n",
      "Batch 84 device: xla:1 time passed: 51.368 time per batch: 0.612\n",
      "Batch 88 device: xla:1 time passed: 53.720 time per batch: 0.610\n",
      "Batch 92 device: xla:1 time passed: 56.052 time per batch: 0.609\n",
      "Batch 96 device: xla:1 time passed: 58.472 time per batch: 0.609\n",
      "Batch 100 device: xla:1 time passed: 60.928 time per batch: 0.609\n",
      "Batch 4 device: xla:1 time passed: 2.774 time per batch: 0.693\n",
      "Batch 8 device: xla:1 time passed: 3.984 time per batch: 0.498\n",
      "Batch 12 device: xla:1 time passed: 5.450 time per batch: 0.454\n",
      "Batch 16 device: xla:1 time passed: 6.971 time per batch: 0.436\n",
      "Batch 20 device: xla:1 time passed: 8.483 time per batch: 0.424\n",
      "Batch 24 device: xla:1 time passed: 9.950 time per batch: 0.415\n",
      "Batch 28 device: xla:1 time passed: 11.451 time per batch: 0.409\n",
      "Batch 32 device: xla:1 time passed: 12.902 time per batch: 0.403\n",
      "Batch 36 device: xla:1 time passed: 14.453 time per batch: 0.401\n",
      "Batch 40 device: xla:1 time passed: 15.864 time per batch: 0.397\n",
      "Batch 44 device: xla:1 time passed: 17.330 time per batch: 0.394\n",
      "Batch 48 device: xla:1 time passed: 18.805 time per batch: 0.392\n",
      "ver 38, epoch 12, fold 0, train ll: 0.0369, val ll: 0.0691, cor: 0.8340, lr: 0.0002\n",
      "setFeats, augmentation 0\n",
      "Batch 4 device: xla:1 time passed: 3.654 time per batch: 0.914\n",
      "Batch 8 device: xla:1 time passed: 5.890 time per batch: 0.736\n",
      "Batch 12 device: xla:1 time passed: 8.208 time per batch: 0.684\n",
      "Batch 16 device: xla:1 time passed: 10.658 time per batch: 0.666\n",
      "Batch 20 device: xla:1 time passed: 13.051 time per batch: 0.653\n",
      "Batch 24 device: xla:1 time passed: 15.430 time per batch: 0.643\n",
      "Batch 28 device: xla:1 time passed: 17.874 time per batch: 0.638\n",
      "Batch 32 device: xla:1 time passed: 20.192 time per batch: 0.631\n",
      "Batch 36 device: xla:1 time passed: 22.650 time per batch: 0.629\n",
      "Batch 40 device: xla:1 time passed: 25.062 time per batch: 0.627\n",
      "Batch 44 device: xla:1 time passed: 27.411 time per batch: 0.623\n",
      "Batch 48 device: xla:1 time passed: 29.807 time per batch: 0.621\n",
      "Batch 52 device: xla:1 time passed: 32.156 time per batch: 0.618\n",
      "Batch 56 device: xla:1 time passed: 34.621 time per batch: 0.618\n",
      "Batch 60 device: xla:1 time passed: 36.983 time per batch: 0.616\n",
      "Batch 64 device: xla:1 time passed: 39.414 time per batch: 0.616\n",
      "Batch 68 device: xla:1 time passed: 41.758 time per batch: 0.614\n",
      "Batch 72 device: xla:1 time passed: 44.143 time per batch: 0.613\n",
      "Batch 76 device: xla:1 time passed: 46.481 time per batch: 0.612\n",
      "Batch 80 device: xla:1 time passed: 48.847 time per batch: 0.611\n",
      "Batch 84 device: xla:1 time passed: 51.233 time per batch: 0.610\n",
      "Batch 88 device: xla:1 time passed: 53.579 time per batch: 0.609\n",
      "Batch 92 device: xla:1 time passed: 55.984 time per batch: 0.609\n",
      "Batch 96 device: xla:1 time passed: 58.438 time per batch: 0.609\n",
      "Batch 100 device: xla:1 time passed: 60.777 time per batch: 0.608\n",
      "Batch 4 device: xla:1 time passed: 2.781 time per batch: 0.695\n",
      "Batch 8 device: xla:1 time passed: 3.933 time per batch: 0.492\n",
      "Batch 12 device: xla:1 time passed: 5.417 time per batch: 0.451\n",
      "Batch 16 device: xla:1 time passed: 6.941 time per batch: 0.434\n",
      "Batch 20 device: xla:1 time passed: 8.413 time per batch: 0.421\n",
      "Batch 24 device: xla:1 time passed: 9.852 time per batch: 0.410\n",
      "Batch 28 device: xla:1 time passed: 11.336 time per batch: 0.405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 32 device: xla:1 time passed: 12.828 time per batch: 0.401\n",
      "Batch 36 device: xla:1 time passed: 14.308 time per batch: 0.397\n",
      "Batch 40 device: xla:1 time passed: 15.797 time per batch: 0.395\n",
      "Batch 44 device: xla:1 time passed: 17.271 time per batch: 0.393\n",
      "Batch 48 device: xla:1 time passed: 18.750 time per batch: 0.391\n",
      "ver 38, epoch 13, fold 0, train ll: 0.0366, val ll: 0.0687, cor: 0.8343, lr: 0.0002\n",
      "setFeats, augmentation 1\n",
      "Batch 4 device: xla:1 time passed: 3.704 time per batch: 0.926\n",
      "Batch 8 device: xla:1 time passed: 5.925 time per batch: 0.741\n",
      "Batch 12 device: xla:1 time passed: 8.480 time per batch: 0.707\n",
      "Batch 16 device: xla:1 time passed: 10.668 time per batch: 0.667\n",
      "Batch 20 device: xla:1 time passed: 13.090 time per batch: 0.655\n",
      "Batch 24 device: xla:1 time passed: 15.528 time per batch: 0.647\n",
      "Batch 28 device: xla:1 time passed: 17.877 time per batch: 0.638\n",
      "Batch 32 device: xla:1 time passed: 20.218 time per batch: 0.632\n",
      "Batch 36 device: xla:1 time passed: 22.678 time per batch: 0.630\n",
      "Batch 40 device: xla:1 time passed: 25.041 time per batch: 0.626\n",
      "Batch 44 device: xla:1 time passed: 27.445 time per batch: 0.624\n",
      "Batch 48 device: xla:1 time passed: 29.812 time per batch: 0.621\n",
      "Batch 52 device: xla:1 time passed: 32.206 time per batch: 0.619\n",
      "Batch 56 device: xla:1 time passed: 34.583 time per batch: 0.618\n",
      "Batch 60 device: xla:1 time passed: 36.987 time per batch: 0.616\n",
      "Batch 64 device: xla:1 time passed: 39.435 time per batch: 0.616\n",
      "Batch 68 device: xla:1 time passed: 41.803 time per batch: 0.615\n",
      "Batch 72 device: xla:1 time passed: 44.186 time per batch: 0.614\n",
      "Batch 76 device: xla:1 time passed: 46.548 time per batch: 0.612\n",
      "Batch 80 device: xla:1 time passed: 48.952 time per batch: 0.612\n",
      "Batch 84 device: xla:1 time passed: 51.245 time per batch: 0.610\n",
      "Batch 88 device: xla:1 time passed: 53.674 time per batch: 0.610\n",
      "Batch 92 device: xla:1 time passed: 56.075 time per batch: 0.610\n",
      "Batch 96 device: xla:1 time passed: 58.401 time per batch: 0.608\n",
      "Batch 100 device: xla:1 time passed: 60.838 time per batch: 0.608\n",
      "Batch 4 device: xla:1 time passed: 2.668 time per batch: 0.667\n",
      "Batch 8 device: xla:1 time passed: 3.918 time per batch: 0.490\n",
      "Batch 12 device: xla:1 time passed: 5.410 time per batch: 0.451\n",
      "Batch 16 device: xla:1 time passed: 6.885 time per batch: 0.430\n",
      "Batch 20 device: xla:1 time passed: 8.327 time per batch: 0.416\n",
      "Batch 24 device: xla:1 time passed: 9.820 time per batch: 0.409\n",
      "Batch 28 device: xla:1 time passed: 11.245 time per batch: 0.402\n",
      "Batch 32 device: xla:1 time passed: 12.769 time per batch: 0.399\n",
      "Batch 36 device: xla:1 time passed: 14.258 time per batch: 0.396\n",
      "Batch 40 device: xla:1 time passed: 15.693 time per batch: 0.392\n",
      "Batch 44 device: xla:1 time passed: 17.219 time per batch: 0.391\n",
      "Batch 48 device: xla:1 time passed: 18.673 time per batch: 0.389\n",
      "ver 38, epoch 14, fold 0, train ll: 0.0365, val ll: 0.0694, cor: 0.8330, lr: 0.0002\n",
      "total running time 250.4052300453186\n"
     ]
    }
   ],
   "source": [
    "DATA_SMALL = False\n",
    "MIXUP = False\n",
    "learning_rate = 0.02\n",
    "weight_decay = 1e-3\n",
    "model, predictions = train_one(epochs=7, bs=16, fold=0)\n",
    "learning_rate = 0.002\n",
    "model, predictions = train_one(epochs=4, bs=16, fold=0)\n",
    "learning_rate = 0.0002\n",
    "model, predictions = train_one(epochs=3, bs=16, fold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epochs: 14 starting now: 3\n",
      "DataSet 2 train size 13042 fold 0\n",
      "adding dummy serieses 40\n",
      "DataSet 2 valid size 6528 fold 0\n",
      "setFeats, augmentation 0\n",
      "dataset train: 13042 valid: 6528 loader train: 815 valid: 408\n",
      "loading model model.b14.f0.v38\n",
      "setFeats, augmentation 2\n",
      "Batch 4 device: xla:1 time passed: 3.953 time per batch: 0.988\n",
      "Batch 8 device: xla:1 time passed: 6.078 time per batch: 0.760\n",
      "Batch 12 device: xla:1 time passed: 8.513 time per batch: 0.709\n",
      "Batch 16 device: xla:1 time passed: 10.829 time per batch: 0.677\n",
      "Batch 20 device: xla:1 time passed: 13.296 time per batch: 0.665\n",
      "Batch 24 device: xla:1 time passed: 15.614 time per batch: 0.651\n",
      "Batch 28 device: xla:1 time passed: 18.142 time per batch: 0.648\n",
      "Batch 32 device: xla:1 time passed: 20.509 time per batch: 0.641\n",
      "Batch 36 device: xla:1 time passed: 22.897 time per batch: 0.636\n",
      "Batch 40 device: xla:1 time passed: 25.260 time per batch: 0.632\n",
      "Batch 44 device: xla:1 time passed: 27.661 time per batch: 0.629\n",
      "Batch 48 device: xla:1 time passed: 30.077 time per batch: 0.627\n",
      "Batch 52 device: xla:1 time passed: 32.476 time per batch: 0.625\n",
      "Batch 56 device: xla:1 time passed: 34.922 time per batch: 0.624\n",
      "Batch 60 device: xla:1 time passed: 37.279 time per batch: 0.621\n",
      "Batch 64 device: xla:1 time passed: 39.680 time per batch: 0.620\n",
      "Batch 68 device: xla:1 time passed: 42.118 time per batch: 0.619\n",
      "Batch 72 device: xla:1 time passed: 44.554 time per batch: 0.619\n",
      "Batch 76 device: xla:1 time passed: 46.991 time per batch: 0.618\n",
      "Batch 80 device: xla:1 time passed: 49.380 time per batch: 0.617\n",
      "Batch 84 device: xla:1 time passed: 51.798 time per batch: 0.617\n",
      "Batch 88 device: xla:1 time passed: 54.192 time per batch: 0.616\n",
      "Batch 92 device: xla:1 time passed: 56.521 time per batch: 0.614\n",
      "Batch 96 device: xla:1 time passed: 58.958 time per batch: 0.614\n",
      "Batch 100 device: xla:1 time passed: 61.378 time per batch: 0.614\n",
      "Batch 4 device: xla:1 time passed: 2.674 time per batch: 0.669\n",
      "Batch 8 device: xla:1 time passed: 4.019 time per batch: 0.502\n",
      "Batch 12 device: xla:1 time passed: 5.588 time per batch: 0.466\n",
      "Batch 16 device: xla:1 time passed: 7.042 time per batch: 0.440\n",
      "Batch 20 device: xla:1 time passed: 8.580 time per batch: 0.429\n",
      "Batch 24 device: xla:1 time passed: 10.013 time per batch: 0.417\n",
      "Batch 28 device: xla:1 time passed: 11.596 time per batch: 0.414\n",
      "Batch 32 device: xla:1 time passed: 12.991 time per batch: 0.406\n",
      "Batch 36 device: xla:1 time passed: 14.563 time per batch: 0.405\n",
      "Batch 40 device: xla:1 time passed: 16.027 time per batch: 0.401\n",
      "Batch 44 device: xla:1 time passed: 17.552 time per batch: 0.399\n",
      "Batch 48 device: xla:1 time passed: 19.061 time per batch: 0.397\n",
      "ver 38, epoch 15, fold 0, train ll: 0.0363, val ll: 0.0686, cor: 0.8338, lr: 0.0001\n",
      "setFeats, augmentation 3\n",
      "Batch 4 device: xla:1 time passed: 3.595 time per batch: 0.899\n",
      "Batch 8 device: xla:1 time passed: 5.851 time per batch: 0.731\n",
      "Batch 12 device: xla:1 time passed: 8.262 time per batch: 0.688\n",
      "Batch 16 device: xla:1 time passed: 10.620 time per batch: 0.664\n",
      "Batch 20 device: xla:1 time passed: 13.045 time per batch: 0.652\n",
      "Batch 24 device: xla:1 time passed: 15.418 time per batch: 0.642\n",
      "Batch 28 device: xla:1 time passed: 17.834 time per batch: 0.637\n",
      "Batch 32 device: xla:1 time passed: 20.194 time per batch: 0.631\n",
      "Batch 36 device: xla:1 time passed: 22.613 time per batch: 0.628\n",
      "Batch 40 device: xla:1 time passed: 25.079 time per batch: 0.627\n",
      "Batch 44 device: xla:1 time passed: 27.424 time per batch: 0.623\n",
      "Batch 48 device: xla:1 time passed: 29.809 time per batch: 0.621\n",
      "Batch 52 device: xla:1 time passed: 32.142 time per batch: 0.618\n",
      "Batch 56 device: xla:1 time passed: 34.502 time per batch: 0.616\n",
      "Batch 60 device: xla:1 time passed: 36.917 time per batch: 0.615\n",
      "Batch 64 device: xla:1 time passed: 39.311 time per batch: 0.614\n",
      "Batch 68 device: xla:1 time passed: 41.629 time per batch: 0.612\n",
      "Batch 72 device: xla:1 time passed: 44.010 time per batch: 0.611\n",
      "Batch 76 device: xla:1 time passed: 46.374 time per batch: 0.610\n",
      "Batch 80 device: xla:1 time passed: 48.799 time per batch: 0.610\n",
      "Batch 84 device: xla:1 time passed: 51.260 time per batch: 0.610\n",
      "Batch 88 device: xla:1 time passed: 53.548 time per batch: 0.609\n",
      "Batch 92 device: xla:1 time passed: 55.965 time per batch: 0.608\n",
      "Batch 96 device: xla:1 time passed: 58.352 time per batch: 0.608\n",
      "Batch 100 device: xla:1 time passed: 60.749 time per batch: 0.607\n",
      "Batch 4 device: xla:1 time passed: 2.795 time per batch: 0.699\n",
      "Batch 8 device: xla:1 time passed: 4.041 time per batch: 0.505\n",
      "Batch 12 device: xla:1 time passed: 5.516 time per batch: 0.460\n",
      "Batch 16 device: xla:1 time passed: 7.015 time per batch: 0.438\n",
      "Batch 20 device: xla:1 time passed: 8.561 time per batch: 0.428\n",
      "Batch 24 device: xla:1 time passed: 9.976 time per batch: 0.416\n",
      "Batch 28 device: xla:1 time passed: 11.468 time per batch: 0.410\n",
      "Batch 32 device: xla:1 time passed: 12.962 time per batch: 0.405\n",
      "Batch 36 device: xla:1 time passed: 14.419 time per batch: 0.401\n",
      "Batch 40 device: xla:1 time passed: 15.919 time per batch: 0.398\n",
      "Batch 44 device: xla:1 time passed: 17.366 time per batch: 0.395\n",
      "Batch 48 device: xla:1 time passed: 18.841 time per batch: 0.393\n",
      "ver 38, epoch 16, fold 0, train ll: 0.0364, val ll: 0.0678, cor: 0.8343, lr: 0.0001\n",
      "setFeats, augmentation 0\n",
      "Batch 4 device: xla:1 time passed: 3.751 time per batch: 0.938\n",
      "Batch 8 device: xla:1 time passed: 5.894 time per batch: 0.737\n",
      "Batch 12 device: xla:1 time passed: 8.307 time per batch: 0.692\n",
      "Batch 16 device: xla:1 time passed: 10.686 time per batch: 0.668\n",
      "Batch 20 device: xla:1 time passed: 13.346 time per batch: 0.667\n",
      "Batch 24 device: xla:1 time passed: 15.427 time per batch: 0.643\n",
      "Batch 28 device: xla:1 time passed: 17.847 time per batch: 0.637\n",
      "Batch 32 device: xla:1 time passed: 20.196 time per batch: 0.631\n",
      "Batch 36 device: xla:1 time passed: 22.677 time per batch: 0.630\n",
      "Batch 40 device: xla:1 time passed: 25.087 time per batch: 0.627\n",
      "Batch 44 device: xla:1 time passed: 27.444 time per batch: 0.624\n",
      "Batch 48 device: xla:1 time passed: 29.836 time per batch: 0.622\n",
      "Batch 52 device: xla:1 time passed: 32.239 time per batch: 0.620\n",
      "Batch 56 device: xla:1 time passed: 34.645 time per batch: 0.619\n",
      "Batch 60 device: xla:1 time passed: 37.066 time per batch: 0.618\n",
      "Batch 64 device: xla:1 time passed: 39.497 time per batch: 0.617\n",
      "Batch 68 device: xla:1 time passed: 41.882 time per batch: 0.616\n",
      "Batch 72 device: xla:1 time passed: 44.262 time per batch: 0.615\n",
      "Batch 76 device: xla:1 time passed: 46.695 time per batch: 0.614\n",
      "Batch 80 device: xla:1 time passed: 49.270 time per batch: 0.616\n",
      "Batch 84 device: xla:1 time passed: 51.507 time per batch: 0.613\n",
      "Batch 88 device: xla:1 time passed: 53.894 time per batch: 0.612\n",
      "Batch 92 device: xla:1 time passed: 56.292 time per batch: 0.612\n",
      "Batch 96 device: xla:1 time passed: 58.772 time per batch: 0.612\n",
      "Batch 100 device: xla:1 time passed: 61.051 time per batch: 0.611\n",
      "Batch 4 device: xla:1 time passed: 2.774 time per batch: 0.693\n",
      "Batch 8 device: xla:1 time passed: 4.004 time per batch: 0.501\n",
      "Batch 12 device: xla:1 time passed: 5.681 time per batch: 0.473\n",
      "Batch 16 device: xla:1 time passed: 7.104 time per batch: 0.444\n",
      "Batch 20 device: xla:1 time passed: 8.616 time per batch: 0.431\n",
      "Batch 24 device: xla:1 time passed: 10.077 time per batch: 0.420\n",
      "Batch 28 device: xla:1 time passed: 11.565 time per batch: 0.413\n",
      "Batch 32 device: xla:1 time passed: 13.197 time per batch: 0.412\n",
      "Batch 36 device: xla:1 time passed: 14.601 time per batch: 0.406\n",
      "Batch 40 device: xla:1 time passed: 16.173 time per batch: 0.404\n",
      "Batch 44 device: xla:1 time passed: 17.643 time per batch: 0.401\n",
      "Batch 48 device: xla:1 time passed: 19.117 time per batch: 0.398\n",
      "ver 38, epoch 17, fold 0, train ll: 0.0362, val ll: 0.0684, cor: 0.8346, lr: 0.0001\n",
      "total running time 251.39635705947876\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "model, predictions = train_one(epochs=3, bs=16, fold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>fold</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>cor</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "      <th>train_sz</th>\n",
       "      <th>val_sz</th>\n",
       "      <th>bs</th>\n",
       "      <th>train_time</th>\n",
       "      <th>valid_time</th>\n",
       "      <th>lr</th>\n",
       "      <th>wd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.178314</td>\n",
       "      <td>0.454581</td>\n",
       "      <td>0.478824</td>\n",
       "      <td>0.930924</td>\n",
       "      <td>0.042852</td>\n",
       "      <td>0.299050</td>\n",
       "      <td>0.236014</td>\n",
       "      <td>0.314261</td>\n",
       "      <td>0.428042</td>\n",
       "      <td>13042</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>65.093604</td>\n",
       "      <td>20.088444</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.056295</td>\n",
       "      <td>0.213536</td>\n",
       "      <td>0.686728</td>\n",
       "      <td>0.397209</td>\n",
       "      <td>0.027492</td>\n",
       "      <td>0.139909</td>\n",
       "      <td>0.126820</td>\n",
       "      <td>0.197202</td>\n",
       "      <td>0.208909</td>\n",
       "      <td>13042</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>65.058618</td>\n",
       "      <td>20.088497</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.045671</td>\n",
       "      <td>0.105922</td>\n",
       "      <td>0.765908</td>\n",
       "      <td>0.165728</td>\n",
       "      <td>0.025776</td>\n",
       "      <td>0.085854</td>\n",
       "      <td>0.058419</td>\n",
       "      <td>0.109287</td>\n",
       "      <td>0.130666</td>\n",
       "      <td>13042</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>65.370224</td>\n",
       "      <td>20.209457</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.044726</td>\n",
       "      <td>0.095684</td>\n",
       "      <td>0.777345</td>\n",
       "      <td>0.147313</td>\n",
       "      <td>0.023883</td>\n",
       "      <td>0.069447</td>\n",
       "      <td>0.054453</td>\n",
       "      <td>0.115409</td>\n",
       "      <td>0.111968</td>\n",
       "      <td>13042</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>65.662750</td>\n",
       "      <td>20.068205</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.043015</td>\n",
       "      <td>0.079246</td>\n",
       "      <td>0.816614</td>\n",
       "      <td>0.115108</td>\n",
       "      <td>0.026872</td>\n",
       "      <td>0.063458</td>\n",
       "      <td>0.038354</td>\n",
       "      <td>0.083832</td>\n",
       "      <td>0.111989</td>\n",
       "      <td>13042</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>61.507829</td>\n",
       "      <td>20.008875</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.044515</td>\n",
       "      <td>0.102922</td>\n",
       "      <td>0.771920</td>\n",
       "      <td>0.173404</td>\n",
       "      <td>0.022863</td>\n",
       "      <td>0.074319</td>\n",
       "      <td>0.058562</td>\n",
       "      <td>0.102248</td>\n",
       "      <td>0.115653</td>\n",
       "      <td>13042</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>61.461192</td>\n",
       "      <td>20.050842</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.044378</td>\n",
       "      <td>0.093102</td>\n",
       "      <td>0.786027</td>\n",
       "      <td>0.147556</td>\n",
       "      <td>0.023341</td>\n",
       "      <td>0.062479</td>\n",
       "      <td>0.053917</td>\n",
       "      <td>0.097110</td>\n",
       "      <td>0.119757</td>\n",
       "      <td>13042</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>61.537823</td>\n",
       "      <td>20.025274</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.039212</td>\n",
       "      <td>0.069597</td>\n",
       "      <td>0.829053</td>\n",
       "      <td>0.106871</td>\n",
       "      <td>0.022700</td>\n",
       "      <td>0.051330</td>\n",
       "      <td>0.032882</td>\n",
       "      <td>0.073547</td>\n",
       "      <td>0.092977</td>\n",
       "      <td>13042</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>61.746062</td>\n",
       "      <td>20.191090</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038235</td>\n",
       "      <td>0.069006</td>\n",
       "      <td>0.831004</td>\n",
       "      <td>0.106983</td>\n",
       "      <td>0.022627</td>\n",
       "      <td>0.051246</td>\n",
       "      <td>0.032268</td>\n",
       "      <td>0.073159</td>\n",
       "      <td>0.089772</td>\n",
       "      <td>13042</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>61.427153</td>\n",
       "      <td>20.092560</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037827</td>\n",
       "      <td>0.068303</td>\n",
       "      <td>0.831848</td>\n",
       "      <td>0.105220</td>\n",
       "      <td>0.022541</td>\n",
       "      <td>0.052443</td>\n",
       "      <td>0.031924</td>\n",
       "      <td>0.072051</td>\n",
       "      <td>0.088726</td>\n",
       "      <td>13042</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>61.640848</td>\n",
       "      <td>20.182223</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037269</td>\n",
       "      <td>0.068285</td>\n",
       "      <td>0.833144</td>\n",
       "      <td>0.106037</td>\n",
       "      <td>0.022160</td>\n",
       "      <td>0.050273</td>\n",
       "      <td>0.031376</td>\n",
       "      <td>0.072139</td>\n",
       "      <td>0.089977</td>\n",
       "      <td>13042</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>61.335586</td>\n",
       "      <td>20.179189</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036424</td>\n",
       "      <td>0.067489</td>\n",
       "      <td>0.835308</td>\n",
       "      <td>0.104623</td>\n",
       "      <td>0.022229</td>\n",
       "      <td>0.049621</td>\n",
       "      <td>0.030687</td>\n",
       "      <td>0.070387</td>\n",
       "      <td>0.090252</td>\n",
       "      <td>13042</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>61.914129</td>\n",
       "      <td>20.069763</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036562</td>\n",
       "      <td>0.067528</td>\n",
       "      <td>0.834743</td>\n",
       "      <td>0.104821</td>\n",
       "      <td>0.022170</td>\n",
       "      <td>0.049720</td>\n",
       "      <td>0.030530</td>\n",
       "      <td>0.070784</td>\n",
       "      <td>0.089848</td>\n",
       "      <td>13042</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>61.625276</td>\n",
       "      <td>20.132065</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035929</td>\n",
       "      <td>0.067441</td>\n",
       "      <td>0.835443</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>0.022096</td>\n",
       "      <td>0.049549</td>\n",
       "      <td>0.030439</td>\n",
       "      <td>0.070327</td>\n",
       "      <td>0.089277</td>\n",
       "      <td>13042</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>61.487574</td>\n",
       "      <td>20.181284</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.175563</td>\n",
       "      <td>0.393331</td>\n",
       "      <td>0.516732</td>\n",
       "      <td>0.732745</td>\n",
       "      <td>0.030713</td>\n",
       "      <td>0.317051</td>\n",
       "      <td>0.238271</td>\n",
       "      <td>0.281393</td>\n",
       "      <td>0.420402</td>\n",
       "      <td>12982</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>65.665394</td>\n",
       "      <td>20.279479</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.050444</td>\n",
       "      <td>0.209809</td>\n",
       "      <td>0.725626</td>\n",
       "      <td>0.398472</td>\n",
       "      <td>0.020520</td>\n",
       "      <td>0.151887</td>\n",
       "      <td>0.110953</td>\n",
       "      <td>0.165880</td>\n",
       "      <td>0.222476</td>\n",
       "      <td>12982</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>65.193563</td>\n",
       "      <td>20.330725</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046775</td>\n",
       "      <td>0.087768</td>\n",
       "      <td>0.783476</td>\n",
       "      <td>0.135451</td>\n",
       "      <td>0.020173</td>\n",
       "      <td>0.070021</td>\n",
       "      <td>0.050855</td>\n",
       "      <td>0.090310</td>\n",
       "      <td>0.112117</td>\n",
       "      <td>12982</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>66.008949</td>\n",
       "      <td>20.319844</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046639</td>\n",
       "      <td>0.087673</td>\n",
       "      <td>0.792104</td>\n",
       "      <td>0.130818</td>\n",
       "      <td>0.020947</td>\n",
       "      <td>0.075369</td>\n",
       "      <td>0.054317</td>\n",
       "      <td>0.085261</td>\n",
       "      <td>0.116184</td>\n",
       "      <td>12982</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>65.774135</td>\n",
       "      <td>20.286688</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.044879</td>\n",
       "      <td>0.077779</td>\n",
       "      <td>0.795196</td>\n",
       "      <td>0.112969</td>\n",
       "      <td>0.020058</td>\n",
       "      <td>0.076391</td>\n",
       "      <td>0.033764</td>\n",
       "      <td>0.092406</td>\n",
       "      <td>0.095897</td>\n",
       "      <td>12982</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>61.596651</td>\n",
       "      <td>20.307943</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.044989</td>\n",
       "      <td>0.088305</td>\n",
       "      <td>0.786895</td>\n",
       "      <td>0.140079</td>\n",
       "      <td>0.019222</td>\n",
       "      <td>0.067456</td>\n",
       "      <td>0.049240</td>\n",
       "      <td>0.097661</td>\n",
       "      <td>0.104397</td>\n",
       "      <td>12982</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>61.695053</td>\n",
       "      <td>20.312417</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.044509</td>\n",
       "      <td>0.079284</td>\n",
       "      <td>0.809093</td>\n",
       "      <td>0.119223</td>\n",
       "      <td>0.021933</td>\n",
       "      <td>0.063221</td>\n",
       "      <td>0.045264</td>\n",
       "      <td>0.081872</td>\n",
       "      <td>0.104248</td>\n",
       "      <td>12982</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>62.369274</td>\n",
       "      <td>20.385399</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.040554</td>\n",
       "      <td>0.069564</td>\n",
       "      <td>0.825610</td>\n",
       "      <td>0.108107</td>\n",
       "      <td>0.019702</td>\n",
       "      <td>0.054041</td>\n",
       "      <td>0.031765</td>\n",
       "      <td>0.072196</td>\n",
       "      <td>0.093032</td>\n",
       "      <td>12982</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>61.655366</td>\n",
       "      <td>20.357985</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.039750</td>\n",
       "      <td>0.068552</td>\n",
       "      <td>0.828097</td>\n",
       "      <td>0.108694</td>\n",
       "      <td>0.019447</td>\n",
       "      <td>0.053198</td>\n",
       "      <td>0.029934</td>\n",
       "      <td>0.071933</td>\n",
       "      <td>0.087964</td>\n",
       "      <td>12982</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>61.873797</td>\n",
       "      <td>20.375277</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.039139</td>\n",
       "      <td>0.067828</td>\n",
       "      <td>0.829944</td>\n",
       "      <td>0.108900</td>\n",
       "      <td>0.019045</td>\n",
       "      <td>0.051346</td>\n",
       "      <td>0.028970</td>\n",
       "      <td>0.069918</td>\n",
       "      <td>0.087713</td>\n",
       "      <td>12982</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>62.074346</td>\n",
       "      <td>20.214985</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037886</td>\n",
       "      <td>0.066679</td>\n",
       "      <td>0.832456</td>\n",
       "      <td>0.107048</td>\n",
       "      <td>0.018931</td>\n",
       "      <td>0.050955</td>\n",
       "      <td>0.028376</td>\n",
       "      <td>0.068656</td>\n",
       "      <td>0.085740</td>\n",
       "      <td>12982</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>62.304821</td>\n",
       "      <td>20.253818</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038176</td>\n",
       "      <td>0.066741</td>\n",
       "      <td>0.832258</td>\n",
       "      <td>0.107378</td>\n",
       "      <td>0.018812</td>\n",
       "      <td>0.050712</td>\n",
       "      <td>0.028429</td>\n",
       "      <td>0.068510</td>\n",
       "      <td>0.085964</td>\n",
       "      <td>12982</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>62.090540</td>\n",
       "      <td>20.345872</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037884</td>\n",
       "      <td>0.066304</td>\n",
       "      <td>0.832689</td>\n",
       "      <td>0.105889</td>\n",
       "      <td>0.019043</td>\n",
       "      <td>0.050758</td>\n",
       "      <td>0.028528</td>\n",
       "      <td>0.068289</td>\n",
       "      <td>0.085730</td>\n",
       "      <td>12982</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>61.838056</td>\n",
       "      <td>20.424386</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.164072</td>\n",
       "      <td>0.330215</td>\n",
       "      <td>0.593049</td>\n",
       "      <td>0.637117</td>\n",
       "      <td>0.036060</td>\n",
       "      <td>0.248951</td>\n",
       "      <td>0.184876</td>\n",
       "      <td>0.248037</td>\n",
       "      <td>0.319350</td>\n",
       "      <td>13036</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>66.168918</td>\n",
       "      <td>20.397983</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.049729</td>\n",
       "      <td>0.173964</td>\n",
       "      <td>0.726833</td>\n",
       "      <td>0.314922</td>\n",
       "      <td>0.023065</td>\n",
       "      <td>0.148310</td>\n",
       "      <td>0.100384</td>\n",
       "      <td>0.138872</td>\n",
       "      <td>0.177275</td>\n",
       "      <td>13036</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>65.748904</td>\n",
       "      <td>20.361132</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046195</td>\n",
       "      <td>0.095407</td>\n",
       "      <td>0.778942</td>\n",
       "      <td>0.156114</td>\n",
       "      <td>0.021454</td>\n",
       "      <td>0.082154</td>\n",
       "      <td>0.051790</td>\n",
       "      <td>0.092381</td>\n",
       "      <td>0.107840</td>\n",
       "      <td>13036</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>65.674282</td>\n",
       "      <td>20.352818</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.045358</td>\n",
       "      <td>0.081992</td>\n",
       "      <td>0.797524</td>\n",
       "      <td>0.128940</td>\n",
       "      <td>0.021887</td>\n",
       "      <td>0.069975</td>\n",
       "      <td>0.040184</td>\n",
       "      <td>0.085555</td>\n",
       "      <td>0.098467</td>\n",
       "      <td>13036</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>65.227834</td>\n",
       "      <td>20.373779</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.045764</td>\n",
       "      <td>0.074893</td>\n",
       "      <td>0.805417</td>\n",
       "      <td>0.109358</td>\n",
       "      <td>0.022328</td>\n",
       "      <td>0.071435</td>\n",
       "      <td>0.043942</td>\n",
       "      <td>0.074158</td>\n",
       "      <td>0.093672</td>\n",
       "      <td>13036</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>61.705719</td>\n",
       "      <td>20.505659</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.045049</td>\n",
       "      <td>0.072067</td>\n",
       "      <td>0.816079</td>\n",
       "      <td>0.109614</td>\n",
       "      <td>0.022133</td>\n",
       "      <td>0.063176</td>\n",
       "      <td>0.040239</td>\n",
       "      <td>0.073249</td>\n",
       "      <td>0.086443</td>\n",
       "      <td>13036</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>62.140555</td>\n",
       "      <td>20.394986</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.044529</td>\n",
       "      <td>0.110724</td>\n",
       "      <td>0.743942</td>\n",
       "      <td>0.200788</td>\n",
       "      <td>0.021281</td>\n",
       "      <td>0.082693</td>\n",
       "      <td>0.057588</td>\n",
       "      <td>0.100433</td>\n",
       "      <td>0.111497</td>\n",
       "      <td>13036</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>62.187569</td>\n",
       "      <td>20.439576</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.039903</td>\n",
       "      <td>0.066690</td>\n",
       "      <td>0.829891</td>\n",
       "      <td>0.104738</td>\n",
       "      <td>0.020675</td>\n",
       "      <td>0.051592</td>\n",
       "      <td>0.031970</td>\n",
       "      <td>0.069892</td>\n",
       "      <td>0.083225</td>\n",
       "      <td>13036</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>61.883817</td>\n",
       "      <td>20.410600</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.039371</td>\n",
       "      <td>0.066436</td>\n",
       "      <td>0.830265</td>\n",
       "      <td>0.105400</td>\n",
       "      <td>0.020590</td>\n",
       "      <td>0.050008</td>\n",
       "      <td>0.031607</td>\n",
       "      <td>0.069057</td>\n",
       "      <td>0.082989</td>\n",
       "      <td>13036</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>61.782478</td>\n",
       "      <td>20.475066</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038832</td>\n",
       "      <td>0.066323</td>\n",
       "      <td>0.831172</td>\n",
       "      <td>0.104555</td>\n",
       "      <td>0.020346</td>\n",
       "      <td>0.050642</td>\n",
       "      <td>0.031594</td>\n",
       "      <td>0.069855</td>\n",
       "      <td>0.082712</td>\n",
       "      <td>13036</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>61.982853</td>\n",
       "      <td>20.453695</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038037</td>\n",
       "      <td>0.064657</td>\n",
       "      <td>0.834958</td>\n",
       "      <td>0.103093</td>\n",
       "      <td>0.019856</td>\n",
       "      <td>0.046580</td>\n",
       "      <td>0.030928</td>\n",
       "      <td>0.068175</td>\n",
       "      <td>0.080874</td>\n",
       "      <td>13036</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>62.149936</td>\n",
       "      <td>20.565762</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037168</td>\n",
       "      <td>0.064857</td>\n",
       "      <td>0.834826</td>\n",
       "      <td>0.103901</td>\n",
       "      <td>0.019814</td>\n",
       "      <td>0.047022</td>\n",
       "      <td>0.029990</td>\n",
       "      <td>0.067367</td>\n",
       "      <td>0.082004</td>\n",
       "      <td>13036</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>62.044564</td>\n",
       "      <td>20.403126</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036973</td>\n",
       "      <td>0.064748</td>\n",
       "      <td>0.835290</td>\n",
       "      <td>0.104306</td>\n",
       "      <td>0.019595</td>\n",
       "      <td>0.046511</td>\n",
       "      <td>0.029499</td>\n",
       "      <td>0.067180</td>\n",
       "      <td>0.081838</td>\n",
       "      <td>13036</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>62.456031</td>\n",
       "      <td>20.568438</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036647</td>\n",
       "      <td>0.064660</td>\n",
       "      <td>0.836230</td>\n",
       "      <td>0.103826</td>\n",
       "      <td>0.019713</td>\n",
       "      <td>0.046288</td>\n",
       "      <td>0.029587</td>\n",
       "      <td>0.066905</td>\n",
       "      <td>0.082476</td>\n",
       "      <td>13036</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>62.017212</td>\n",
       "      <td>20.503824</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035913</td>\n",
       "      <td>0.067058</td>\n",
       "      <td>0.835938</td>\n",
       "      <td>0.104367</td>\n",
       "      <td>0.022106</td>\n",
       "      <td>0.049437</td>\n",
       "      <td>0.030474</td>\n",
       "      <td>0.070110</td>\n",
       "      <td>0.088543</td>\n",
       "      <td>13042</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>62.048992</td>\n",
       "      <td>20.827352</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035673</td>\n",
       "      <td>0.067600</td>\n",
       "      <td>0.835605</td>\n",
       "      <td>0.105215</td>\n",
       "      <td>0.022069</td>\n",
       "      <td>0.049199</td>\n",
       "      <td>0.030341</td>\n",
       "      <td>0.070479</td>\n",
       "      <td>0.090679</td>\n",
       "      <td>13042</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>61.994573</td>\n",
       "      <td>20.624591</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.035784</td>\n",
       "      <td>0.067664</td>\n",
       "      <td>0.835736</td>\n",
       "      <td>0.105468</td>\n",
       "      <td>0.022049</td>\n",
       "      <td>0.049525</td>\n",
       "      <td>0.030358</td>\n",
       "      <td>0.070289</td>\n",
       "      <td>0.090495</td>\n",
       "      <td>13042</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>62.116615</td>\n",
       "      <td>20.724090</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037345</td>\n",
       "      <td>0.066355</td>\n",
       "      <td>0.832846</td>\n",
       "      <td>0.106361</td>\n",
       "      <td>0.018796</td>\n",
       "      <td>0.050764</td>\n",
       "      <td>0.028504</td>\n",
       "      <td>0.068415</td>\n",
       "      <td>0.085285</td>\n",
       "      <td>12982</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>62.270850</td>\n",
       "      <td>20.643576</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037549</td>\n",
       "      <td>0.066164</td>\n",
       "      <td>0.833185</td>\n",
       "      <td>0.106243</td>\n",
       "      <td>0.018568</td>\n",
       "      <td>0.050498</td>\n",
       "      <td>0.028333</td>\n",
       "      <td>0.068215</td>\n",
       "      <td>0.085051</td>\n",
       "      <td>12982</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>62.541094</td>\n",
       "      <td>20.702332</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037528</td>\n",
       "      <td>0.066325</td>\n",
       "      <td>0.832592</td>\n",
       "      <td>0.106467</td>\n",
       "      <td>0.018703</td>\n",
       "      <td>0.050410</td>\n",
       "      <td>0.028357</td>\n",
       "      <td>0.068444</td>\n",
       "      <td>0.085424</td>\n",
       "      <td>12982</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>62.285178</td>\n",
       "      <td>21.236999</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036299</td>\n",
       "      <td>0.063966</td>\n",
       "      <td>0.836567</td>\n",
       "      <td>0.102585</td>\n",
       "      <td>0.019481</td>\n",
       "      <td>0.045509</td>\n",
       "      <td>0.029539</td>\n",
       "      <td>0.067060</td>\n",
       "      <td>0.081004</td>\n",
       "      <td>13036</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>62.110817</td>\n",
       "      <td>20.938057</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036398</td>\n",
       "      <td>0.063720</td>\n",
       "      <td>0.836732</td>\n",
       "      <td>0.102171</td>\n",
       "      <td>0.019610</td>\n",
       "      <td>0.045279</td>\n",
       "      <td>0.029597</td>\n",
       "      <td>0.066814</td>\n",
       "      <td>0.080400</td>\n",
       "      <td>13036</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>62.440073</td>\n",
       "      <td>20.801542</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036378</td>\n",
       "      <td>0.064570</td>\n",
       "      <td>0.836745</td>\n",
       "      <td>0.103918</td>\n",
       "      <td>0.019383</td>\n",
       "      <td>0.045866</td>\n",
       "      <td>0.029359</td>\n",
       "      <td>0.066857</td>\n",
       "      <td>0.082687</td>\n",
       "      <td>13036</td>\n",
       "      <td>6656</td>\n",
       "      <td>32</td>\n",
       "      <td>62.695830</td>\n",
       "      <td>20.884507</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  fold  train_loss  val_loss       cor       any  epidural  \\\n",
       "0       1     0    0.178314  0.454581  0.478824  0.930924  0.042852   \n",
       "1       2     0    0.056295  0.213536  0.686728  0.397209  0.027492   \n",
       "2       3     0    0.045671  0.105922  0.765908  0.165728  0.025776   \n",
       "3       4     0    0.044726  0.095684  0.777345  0.147313  0.023883   \n",
       "4       5     0    0.043015  0.079246  0.816614  0.115108  0.026872   \n",
       "5       6     0    0.044515  0.102922  0.771920  0.173404  0.022863   \n",
       "6       7     0    0.044378  0.093102  0.786027  0.147556  0.023341   \n",
       "7       8     0    0.039212  0.069597  0.829053  0.106871  0.022700   \n",
       "8       9     0    0.038235  0.069006  0.831004  0.106983  0.022627   \n",
       "9      10     0    0.037827  0.068303  0.831848  0.105220  0.022541   \n",
       "10     11     0    0.037269  0.068285  0.833144  0.106037  0.022160   \n",
       "11     12     0    0.036424  0.067489  0.835308  0.104623  0.022229   \n",
       "12     13     0    0.036562  0.067528  0.834743  0.104821  0.022170   \n",
       "13     14     0    0.035929  0.067441  0.835443  0.105200  0.022096   \n",
       "14      1     0    0.175563  0.393331  0.516732  0.732745  0.030713   \n",
       "15      2     0    0.050444  0.209809  0.725626  0.398472  0.020520   \n",
       "16      3     0    0.046775  0.087768  0.783476  0.135451  0.020173   \n",
       "17      4     0    0.046639  0.087673  0.792104  0.130818  0.020947   \n",
       "18      5     0    0.044879  0.077779  0.795196  0.112969  0.020058   \n",
       "19      6     0    0.044989  0.088305  0.786895  0.140079  0.019222   \n",
       "20      7     0    0.044509  0.079284  0.809093  0.119223  0.021933   \n",
       "21      8     0    0.040554  0.069564  0.825610  0.108107  0.019702   \n",
       "22      9     0    0.039750  0.068552  0.828097  0.108694  0.019447   \n",
       "23     10     0    0.039139  0.067828  0.829944  0.108900  0.019045   \n",
       "24     11     0    0.037886  0.066679  0.832456  0.107048  0.018931   \n",
       "25     12     0    0.038176  0.066741  0.832258  0.107378  0.018812   \n",
       "26     13     0    0.037884  0.066304  0.832689  0.105889  0.019043   \n",
       "27      1     0    0.164072  0.330215  0.593049  0.637117  0.036060   \n",
       "28      2     0    0.049729  0.173964  0.726833  0.314922  0.023065   \n",
       "29      3     0    0.046195  0.095407  0.778942  0.156114  0.021454   \n",
       "30      4     0    0.045358  0.081992  0.797524  0.128940  0.021887   \n",
       "31      5     0    0.045764  0.074893  0.805417  0.109358  0.022328   \n",
       "32      6     0    0.045049  0.072067  0.816079  0.109614  0.022133   \n",
       "33      7     0    0.044529  0.110724  0.743942  0.200788  0.021281   \n",
       "34      8     0    0.039903  0.066690  0.829891  0.104738  0.020675   \n",
       "35      9     0    0.039371  0.066436  0.830265  0.105400  0.020590   \n",
       "36     10     0    0.038832  0.066323  0.831172  0.104555  0.020346   \n",
       "37     11     0    0.038037  0.064657  0.834958  0.103093  0.019856   \n",
       "38     12     0    0.037168  0.064857  0.834826  0.103901  0.019814   \n",
       "39     13     0    0.036973  0.064748  0.835290  0.104306  0.019595   \n",
       "40     14     0    0.036647  0.064660  0.836230  0.103826  0.019713   \n",
       "41     15     0    0.035913  0.067058  0.835938  0.104367  0.022106   \n",
       "42     16     0    0.035673  0.067600  0.835605  0.105215  0.022069   \n",
       "43     17     0    0.035784  0.067664  0.835736  0.105468  0.022049   \n",
       "44     14     0    0.037345  0.066355  0.832846  0.106361  0.018796   \n",
       "45     15     0    0.037549  0.066164  0.833185  0.106243  0.018568   \n",
       "46     16     0    0.037528  0.066325  0.832592  0.106467  0.018703   \n",
       "47     15     0    0.036299  0.063966  0.836567  0.102585  0.019481   \n",
       "48     16     0    0.036398  0.063720  0.836732  0.102171  0.019610   \n",
       "49     17     0    0.036378  0.064570  0.836745  0.103918  0.019383   \n",
       "\n",
       "    intraparenchymal  intraventricular  subarachnoid  subdural  train_sz  \\\n",
       "0           0.299050          0.236014      0.314261  0.428042     13042   \n",
       "1           0.139909          0.126820      0.197202  0.208909     13042   \n",
       "2           0.085854          0.058419      0.109287  0.130666     13042   \n",
       "3           0.069447          0.054453      0.115409  0.111968     13042   \n",
       "4           0.063458          0.038354      0.083832  0.111989     13042   \n",
       "5           0.074319          0.058562      0.102248  0.115653     13042   \n",
       "6           0.062479          0.053917      0.097110  0.119757     13042   \n",
       "7           0.051330          0.032882      0.073547  0.092977     13042   \n",
       "8           0.051246          0.032268      0.073159  0.089772     13042   \n",
       "9           0.052443          0.031924      0.072051  0.088726     13042   \n",
       "10          0.050273          0.031376      0.072139  0.089977     13042   \n",
       "11          0.049621          0.030687      0.070387  0.090252     13042   \n",
       "12          0.049720          0.030530      0.070784  0.089848     13042   \n",
       "13          0.049549          0.030439      0.070327  0.089277     13042   \n",
       "14          0.317051          0.238271      0.281393  0.420402     12982   \n",
       "15          0.151887          0.110953      0.165880  0.222476     12982   \n",
       "16          0.070021          0.050855      0.090310  0.112117     12982   \n",
       "17          0.075369          0.054317      0.085261  0.116184     12982   \n",
       "18          0.076391          0.033764      0.092406  0.095897     12982   \n",
       "19          0.067456          0.049240      0.097661  0.104397     12982   \n",
       "20          0.063221          0.045264      0.081872  0.104248     12982   \n",
       "21          0.054041          0.031765      0.072196  0.093032     12982   \n",
       "22          0.053198          0.029934      0.071933  0.087964     12982   \n",
       "23          0.051346          0.028970      0.069918  0.087713     12982   \n",
       "24          0.050955          0.028376      0.068656  0.085740     12982   \n",
       "25          0.050712          0.028429      0.068510  0.085964     12982   \n",
       "26          0.050758          0.028528      0.068289  0.085730     12982   \n",
       "27          0.248951          0.184876      0.248037  0.319350     13036   \n",
       "28          0.148310          0.100384      0.138872  0.177275     13036   \n",
       "29          0.082154          0.051790      0.092381  0.107840     13036   \n",
       "30          0.069975          0.040184      0.085555  0.098467     13036   \n",
       "31          0.071435          0.043942      0.074158  0.093672     13036   \n",
       "32          0.063176          0.040239      0.073249  0.086443     13036   \n",
       "33          0.082693          0.057588      0.100433  0.111497     13036   \n",
       "34          0.051592          0.031970      0.069892  0.083225     13036   \n",
       "35          0.050008          0.031607      0.069057  0.082989     13036   \n",
       "36          0.050642          0.031594      0.069855  0.082712     13036   \n",
       "37          0.046580          0.030928      0.068175  0.080874     13036   \n",
       "38          0.047022          0.029990      0.067367  0.082004     13036   \n",
       "39          0.046511          0.029499      0.067180  0.081838     13036   \n",
       "40          0.046288          0.029587      0.066905  0.082476     13036   \n",
       "41          0.049437          0.030474      0.070110  0.088543     13042   \n",
       "42          0.049199          0.030341      0.070479  0.090679     13042   \n",
       "43          0.049525          0.030358      0.070289  0.090495     13042   \n",
       "44          0.050764          0.028504      0.068415  0.085285     12982   \n",
       "45          0.050498          0.028333      0.068215  0.085051     12982   \n",
       "46          0.050410          0.028357      0.068444  0.085424     12982   \n",
       "47          0.045509          0.029539      0.067060  0.081004     13036   \n",
       "48          0.045279          0.029597      0.066814  0.080400     13036   \n",
       "49          0.045866          0.029359      0.066857  0.082687     13036   \n",
       "\n",
       "    val_sz  bs  train_time  valid_time      lr     wd  \n",
       "0     6656  32   65.093604   20.088444  0.0200  0.001  \n",
       "1     6656  32   65.058618   20.088497  0.0200  0.001  \n",
       "2     6656  32   65.370224   20.209457  0.0200  0.001  \n",
       "3     6656  32   65.662750   20.068205  0.0200  0.001  \n",
       "4     6656  32   61.507829   20.008875  0.0200  0.001  \n",
       "5     6656  32   61.461192   20.050842  0.0200  0.001  \n",
       "6     6656  32   61.537823   20.025274  0.0200  0.001  \n",
       "7     6656  32   61.746062   20.191090  0.0020  0.001  \n",
       "8     6656  32   61.427153   20.092560  0.0020  0.001  \n",
       "9     6656  32   61.640848   20.182223  0.0020  0.001  \n",
       "10    6656  32   61.335586   20.179189  0.0020  0.001  \n",
       "11    6656  32   61.914129   20.069763  0.0002  0.001  \n",
       "12    6656  32   61.625276   20.132065  0.0002  0.001  \n",
       "13    6656  32   61.487574   20.181284  0.0002  0.001  \n",
       "14    6656  32   65.665394   20.279479  0.0200  0.001  \n",
       "15    6656  32   65.193563   20.330725  0.0200  0.001  \n",
       "16    6656  32   66.008949   20.319844  0.0200  0.001  \n",
       "17    6656  32   65.774135   20.286688  0.0200  0.001  \n",
       "18    6656  32   61.596651   20.307943  0.0200  0.001  \n",
       "19    6656  32   61.695053   20.312417  0.0200  0.001  \n",
       "20    6656  32   62.369274   20.385399  0.0200  0.001  \n",
       "21    6656  32   61.655366   20.357985  0.0020  0.001  \n",
       "22    6656  32   61.873797   20.375277  0.0020  0.001  \n",
       "23    6656  32   62.074346   20.214985  0.0020  0.001  \n",
       "24    6656  32   62.304821   20.253818  0.0002  0.001  \n",
       "25    6656  32   62.090540   20.345872  0.0002  0.001  \n",
       "26    6656  32   61.838056   20.424386  0.0002  0.001  \n",
       "27    6656  32   66.168918   20.397983  0.0200  0.001  \n",
       "28    6656  32   65.748904   20.361132  0.0200  0.001  \n",
       "29    6656  32   65.674282   20.352818  0.0200  0.001  \n",
       "30    6656  32   65.227834   20.373779  0.0200  0.001  \n",
       "31    6656  32   61.705719   20.505659  0.0200  0.001  \n",
       "32    6656  32   62.140555   20.394986  0.0200  0.001  \n",
       "33    6656  32   62.187569   20.439576  0.0200  0.001  \n",
       "34    6656  32   61.883817   20.410600  0.0020  0.001  \n",
       "35    6656  32   61.782478   20.475066  0.0020  0.001  \n",
       "36    6656  32   61.982853   20.453695  0.0020  0.001  \n",
       "37    6656  32   62.149936   20.565762  0.0020  0.001  \n",
       "38    6656  32   62.044564   20.403126  0.0002  0.001  \n",
       "39    6656  32   62.456031   20.568438  0.0002  0.001  \n",
       "40    6656  32   62.017212   20.503824  0.0002  0.001  \n",
       "41    6656  32   62.048992   20.827352  0.0001  0.001  \n",
       "42    6656  32   61.994573   20.624591  0.0001  0.001  \n",
       "43    6656  32   62.116615   20.724090  0.0001  0.001  \n",
       "44    6656  32   62.270850   20.643576  0.0001  0.001  \n",
       "45    6656  32   62.541094   20.702332  0.0001  0.001  \n",
       "46    6656  32   62.285178   21.236999  0.0001  0.001  \n",
       "47    6656  32   62.110817   20.938057  0.0001  0.001  \n",
       "48    6656  32   62.440073   20.801542  0.0001  0.001  \n",
       "49    6656  32   62.695830   20.884507  0.0001  0.001  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(PATH_WORK/'stats.f{}.v{}'.format(0,VERSION-2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epochs: 0 starting now: 7\n",
      "DataSet 2 train size 12982 fold 1\n",
      "adding dummy serieses 108\n",
      "DataSet 2 valid size 6656 fold 1\n",
      "setFeats, augmentation 0\n",
      "dataset train: 12982 valid: 6656 loader train: 405 valid: 208\n",
      "starting from scratch\n",
      "setFeats, augmentation 0\n",
      "Batch 4 device: xla:1 time passed: 6.416 time per batch: 1.604\n",
      "Batch 8 device: xla:1 time passed: 10.739 time per batch: 1.342\n",
      "Batch 12 device: xla:1 time passed: 15.432 time per batch: 1.286\n",
      "Batch 16 device: xla:1 time passed: 20.126 time per batch: 1.258\n",
      "Batch 20 device: xla:1 time passed: 24.823 time per batch: 1.241\n",
      "Batch 24 device: xla:1 time passed: 29.590 time per batch: 1.233\n",
      "Batch 28 device: xla:1 time passed: 34.262 time per batch: 1.224\n",
      "Batch 32 device: xla:1 time passed: 38.984 time per batch: 1.218\n",
      "Batch 36 device: xla:1 time passed: 43.688 time per batch: 1.214\n",
      "Batch 40 device: xla:1 time passed: 48.438 time per batch: 1.211\n",
      "Batch 44 device: xla:1 time passed: 53.109 time per batch: 1.207\n",
      "Batch 48 device: xla:1 time passed: 57.827 time per batch: 1.205\n",
      "Batch 4 device: xla:1 time passed: 4.444 time per batch: 1.111\n",
      "Batch 8 device: xla:1 time passed: 7.024 time per batch: 0.878\n",
      "Batch 12 device: xla:1 time passed: 9.934 time per batch: 0.828\n",
      "Batch 16 device: xla:1 time passed: 12.854 time per batch: 0.803\n",
      "Batch 20 device: xla:1 time passed: 15.771 time per batch: 0.789\n",
      "Batch 24 device: xla:1 time passed: 18.736 time per batch: 0.781\n",
      "ver 37, epoch 1, fold 1, train ll: 0.1928, val ll: 0.3491, cor: 0.5576, lr: 0.02\n",
      "setFeats, augmentation 1\n",
      "Batch 4 device: xla:1 time passed: 6.183 time per batch: 1.546\n",
      "Batch 8 device: xla:1 time passed: 10.599 time per batch: 1.325\n",
      "Batch 12 device: xla:1 time passed: 15.281 time per batch: 1.273\n",
      "Batch 16 device: xla:1 time passed: 20.026 time per batch: 1.252\n",
      "Batch 20 device: xla:1 time passed: 24.839 time per batch: 1.242\n",
      "Batch 24 device: xla:1 time passed: 29.506 time per batch: 1.229\n",
      "Batch 28 device: xla:1 time passed: 34.234 time per batch: 1.223\n",
      "Batch 32 device: xla:1 time passed: 38.868 time per batch: 1.215\n",
      "Batch 36 device: xla:1 time passed: 43.637 time per batch: 1.212\n",
      "Batch 40 device: xla:1 time passed: 48.359 time per batch: 1.209\n",
      "Batch 44 device: xla:1 time passed: 52.966 time per batch: 1.204\n",
      "Batch 48 device: xla:1 time passed: 57.658 time per batch: 1.201\n",
      "Batch 4 device: xla:1 time passed: 4.371 time per batch: 1.093\n",
      "Batch 8 device: xla:1 time passed: 6.925 time per batch: 0.866\n",
      "Batch 12 device: xla:1 time passed: 9.898 time per batch: 0.825\n",
      "Batch 16 device: xla:1 time passed: 12.800 time per batch: 0.800\n",
      "Batch 20 device: xla:1 time passed: 15.774 time per batch: 0.789\n",
      "Batch 24 device: xla:1 time passed: 18.699 time per batch: 0.779\n",
      "ver 37, epoch 2, fold 1, train ll: 0.0526, val ll: 0.1642, cor: 0.7197, lr: 0.02\n",
      "setFeats, augmentation 2\n",
      "Batch 4 device: xla:1 time passed: 6.153 time per batch: 1.538\n",
      "Batch 8 device: xla:1 time passed: 10.615 time per batch: 1.327\n",
      "Batch 12 device: xla:1 time passed: 15.341 time per batch: 1.278\n",
      "Batch 16 device: xla:1 time passed: 20.047 time per batch: 1.253\n",
      "Batch 20 device: xla:1 time passed: 24.816 time per batch: 1.241\n",
      "Batch 24 device: xla:1 time passed: 29.571 time per batch: 1.232\n",
      "Batch 28 device: xla:1 time passed: 34.325 time per batch: 1.226\n",
      "Batch 32 device: xla:1 time passed: 38.967 time per batch: 1.218\n",
      "Batch 36 device: xla:1 time passed: 43.687 time per batch: 1.214\n",
      "Batch 40 device: xla:1 time passed: 48.460 time per batch: 1.211\n",
      "Batch 44 device: xla:1 time passed: 53.167 time per batch: 1.208\n",
      "Batch 48 device: xla:1 time passed: 57.881 time per batch: 1.206\n",
      "Batch 4 device: xla:1 time passed: 4.371 time per batch: 1.093\n",
      "Batch 8 device: xla:1 time passed: 7.057 time per batch: 0.882\n",
      "Batch 12 device: xla:1 time passed: 9.992 time per batch: 0.833\n",
      "Batch 16 device: xla:1 time passed: 12.964 time per batch: 0.810\n",
      "Batch 20 device: xla:1 time passed: 15.746 time per batch: 0.787\n",
      "Batch 24 device: xla:1 time passed: 18.705 time per batch: 0.779\n",
      "ver 37, epoch 3, fold 1, train ll: 0.0487, val ll: 0.1202, cor: 0.7271, lr: 0.02\n",
      "setFeats, augmentation 3\n",
      "Batch 4 device: xla:1 time passed: 6.261 time per batch: 1.565\n",
      "Batch 8 device: xla:1 time passed: 10.608 time per batch: 1.326\n",
      "Batch 12 device: xla:1 time passed: 15.353 time per batch: 1.279\n",
      "Batch 16 device: xla:1 time passed: 20.086 time per batch: 1.255\n",
      "Batch 20 device: xla:1 time passed: 24.815 time per batch: 1.241\n",
      "Batch 24 device: xla:1 time passed: 29.496 time per batch: 1.229\n",
      "Batch 28 device: xla:1 time passed: 34.225 time per batch: 1.222\n",
      "Batch 32 device: xla:1 time passed: 39.013 time per batch: 1.219\n",
      "Batch 36 device: xla:1 time passed: 43.645 time per batch: 1.212\n",
      "Batch 40 device: xla:1 time passed: 48.386 time per batch: 1.210\n",
      "Batch 44 device: xla:1 time passed: 53.047 time per batch: 1.206\n",
      "Batch 48 device: xla:1 time passed: 57.822 time per batch: 1.205\n",
      "Batch 4 device: xla:1 time passed: 4.291 time per batch: 1.073\n",
      "Batch 8 device: xla:1 time passed: 7.013 time per batch: 0.877\n",
      "Batch 12 device: xla:1 time passed: 9.893 time per batch: 0.824\n",
      "Batch 16 device: xla:1 time passed: 12.800 time per batch: 0.800\n",
      "Batch 20 device: xla:1 time passed: 15.796 time per batch: 0.790\n",
      "Batch 24 device: xla:1 time passed: 18.647 time per batch: 0.777\n",
      "ver 37, epoch 4, fold 1, train ll: 0.0461, val ll: 0.0914, cor: 0.7769, lr: 0.02\n",
      "setFeats, augmentation 0\n",
      "Batch 4 device: xla:1 time passed: 6.272 time per batch: 1.568\n",
      "Batch 8 device: xla:1 time passed: 10.613 time per batch: 1.327\n",
      "Batch 12 device: xla:1 time passed: 15.392 time per batch: 1.283\n",
      "Batch 16 device: xla:1 time passed: 20.113 time per batch: 1.257\n",
      "Batch 20 device: xla:1 time passed: 24.840 time per batch: 1.242\n",
      "Batch 24 device: xla:1 time passed: 29.547 time per batch: 1.231\n",
      "Batch 28 device: xla:1 time passed: 34.254 time per batch: 1.223\n",
      "Batch 32 device: xla:1 time passed: 38.929 time per batch: 1.217\n",
      "Batch 36 device: xla:1 time passed: 43.612 time per batch: 1.211\n",
      "Batch 40 device: xla:1 time passed: 48.385 time per batch: 1.210\n",
      "Batch 44 device: xla:1 time passed: 53.082 time per batch: 1.206\n",
      "Batch 48 device: xla:1 time passed: 57.697 time per batch: 1.202\n",
      "Batch 4 device: xla:1 time passed: 4.523 time per batch: 1.131\n",
      "Batch 8 device: xla:1 time passed: 7.069 time per batch: 0.884\n",
      "Batch 12 device: xla:1 time passed: 10.015 time per batch: 0.835\n",
      "Batch 16 device: xla:1 time passed: 12.952 time per batch: 0.810\n",
      "Batch 20 device: xla:1 time passed: 15.888 time per batch: 0.794\n",
      "Batch 24 device: xla:1 time passed: 18.814 time per batch: 0.784\n",
      "ver 37, epoch 5, fold 1, train ll: 0.0464, val ll: 0.0851, cor: 0.7982, lr: 0.02\n",
      "setFeats, augmentation 1\n",
      "Batch 4 device: xla:1 time passed: 6.214 time per batch: 1.553\n",
      "Batch 8 device: xla:1 time passed: 10.694 time per batch: 1.337\n",
      "Batch 12 device: xla:1 time passed: 15.485 time per batch: 1.290\n",
      "Batch 16 device: xla:1 time passed: 20.236 time per batch: 1.265\n",
      "Batch 20 device: xla:1 time passed: 25.000 time per batch: 1.250\n",
      "Batch 24 device: xla:1 time passed: 29.796 time per batch: 1.241\n",
      "Batch 28 device: xla:1 time passed: 34.508 time per batch: 1.232\n",
      "Batch 32 device: xla:1 time passed: 39.298 time per batch: 1.228\n",
      "Batch 36 device: xla:1 time passed: 43.983 time per batch: 1.222\n",
      "Batch 40 device: xla:1 time passed: 48.706 time per batch: 1.218\n",
      "Batch 44 device: xla:1 time passed: 53.394 time per batch: 1.213\n",
      "Batch 48 device: xla:1 time passed: 58.121 time per batch: 1.211\n",
      "Batch 4 device: xla:1 time passed: 4.404 time per batch: 1.101\n",
      "Batch 8 device: xla:1 time passed: 7.014 time per batch: 0.877\n",
      "Batch 12 device: xla:1 time passed: 9.877 time per batch: 0.823\n",
      "Batch 16 device: xla:1 time passed: 12.817 time per batch: 0.801\n",
      "Batch 20 device: xla:1 time passed: 15.786 time per batch: 0.789\n",
      "Batch 24 device: xla:1 time passed: 18.700 time per batch: 0.779\n",
      "ver 37, epoch 6, fold 1, train ll: 0.0475, val ll: 0.0841, cor: 0.7944, lr: 0.02\n",
      "setFeats, augmentation 2\n",
      "Batch 4 device: xla:1 time passed: 6.310 time per batch: 1.577\n",
      "Batch 8 device: xla:1 time passed: 10.697 time per batch: 1.337\n",
      "Batch 12 device: xla:1 time passed: 15.444 time per batch: 1.287\n",
      "Batch 16 device: xla:1 time passed: 20.132 time per batch: 1.258\n",
      "Batch 20 device: xla:1 time passed: 24.833 time per batch: 1.242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 24 device: xla:1 time passed: 29.539 time per batch: 1.231\n",
      "Batch 28 device: xla:1 time passed: 34.297 time per batch: 1.225\n",
      "Batch 32 device: xla:1 time passed: 39.038 time per batch: 1.220\n",
      "Batch 36 device: xla:1 time passed: 43.761 time per batch: 1.216\n",
      "Batch 40 device: xla:1 time passed: 48.545 time per batch: 1.214\n",
      "Batch 44 device: xla:1 time passed: 53.283 time per batch: 1.211\n",
      "Batch 48 device: xla:1 time passed: 58.008 time per batch: 1.209\n",
      "Batch 4 device: xla:1 time passed: 4.521 time per batch: 1.130\n",
      "Batch 8 device: xla:1 time passed: 7.083 time per batch: 0.885\n",
      "Batch 12 device: xla:1 time passed: 9.981 time per batch: 0.832\n",
      "Batch 16 device: xla:1 time passed: 12.910 time per batch: 0.807\n",
      "Batch 20 device: xla:1 time passed: 15.836 time per batch: 0.792\n",
      "Batch 24 device: xla:1 time passed: 18.727 time per batch: 0.780\n",
      "ver 37, epoch 7, fold 1, train ll: 0.0456, val ll: 0.0803, cor: 0.7900, lr: 0.02\n",
      "total running time 580.2911779880524\n",
      "completed epochs: 7 starting now: 4\n",
      "DataSet 2 train size 12982 fold 1\n",
      "adding dummy serieses 108\n",
      "DataSet 2 valid size 6656 fold 1\n",
      "setFeats, augmentation 0\n",
      "dataset train: 12982 valid: 6656 loader train: 405 valid: 208\n",
      "loading model model.b7.f1.v37\n",
      "setFeats, augmentation 3\n",
      "Batch 4 device: xla:1 time passed: 7.390 time per batch: 1.847\n",
      "Batch 8 device: xla:1 time passed: 11.732 time per batch: 1.467\n",
      "Batch 12 device: xla:1 time passed: 16.462 time per batch: 1.372\n",
      "Batch 16 device: xla:1 time passed: 21.172 time per batch: 1.323\n",
      "Batch 20 device: xla:1 time passed: 25.849 time per batch: 1.292\n",
      "Batch 24 device: xla:1 time passed: 30.521 time per batch: 1.272\n",
      "Batch 28 device: xla:1 time passed: 35.239 time per batch: 1.259\n",
      "Batch 32 device: xla:1 time passed: 40.029 time per batch: 1.251\n",
      "Batch 36 device: xla:1 time passed: 44.717 time per batch: 1.242\n",
      "Batch 40 device: xla:1 time passed: 49.466 time per batch: 1.237\n",
      "Batch 44 device: xla:1 time passed: 54.060 time per batch: 1.229\n",
      "Batch 48 device: xla:1 time passed: 58.756 time per batch: 1.224\n",
      "Batch 4 device: xla:1 time passed: 4.335 time per batch: 1.084\n",
      "Batch 8 device: xla:1 time passed: 6.988 time per batch: 0.873\n",
      "Batch 12 device: xla:1 time passed: 9.896 time per batch: 0.825\n",
      "Batch 16 device: xla:1 time passed: 12.850 time per batch: 0.803\n",
      "Batch 20 device: xla:1 time passed: 15.781 time per batch: 0.789\n",
      "Batch 24 device: xla:1 time passed: 18.692 time per batch: 0.779\n",
      "ver 37, epoch 8, fold 1, train ll: 0.0411, val ll: 0.0696, cor: 0.8252, lr: 0.002\n",
      "setFeats, augmentation 0\n",
      "Batch 4 device: xla:1 time passed: 6.278 time per batch: 1.569\n",
      "Batch 8 device: xla:1 time passed: 10.698 time per batch: 1.337\n",
      "Batch 12 device: xla:1 time passed: 15.419 time per batch: 1.285\n",
      "Batch 16 device: xla:1 time passed: 20.284 time per batch: 1.268\n",
      "Batch 20 device: xla:1 time passed: 24.963 time per batch: 1.248\n",
      "Batch 24 device: xla:1 time passed: 29.788 time per batch: 1.241\n",
      "Batch 28 device: xla:1 time passed: 34.375 time per batch: 1.228\n",
      "Batch 32 device: xla:1 time passed: 39.081 time per batch: 1.221\n",
      "Batch 36 device: xla:1 time passed: 43.845 time per batch: 1.218\n",
      "Batch 40 device: xla:1 time passed: 48.619 time per batch: 1.215\n",
      "Batch 44 device: xla:1 time passed: 53.334 time per batch: 1.212\n",
      "Batch 48 device: xla:1 time passed: 58.089 time per batch: 1.210\n",
      "Batch 4 device: xla:1 time passed: 4.384 time per batch: 1.096\n",
      "Batch 8 device: xla:1 time passed: 6.970 time per batch: 0.871\n",
      "Batch 12 device: xla:1 time passed: 9.937 time per batch: 0.828\n",
      "Batch 16 device: xla:1 time passed: 12.808 time per batch: 0.801\n",
      "Batch 20 device: xla:1 time passed: 15.775 time per batch: 0.789\n",
      "Batch 24 device: xla:1 time passed: 18.718 time per batch: 0.780\n",
      "ver 37, epoch 9, fold 1, train ll: 0.0402, val ll: 0.0687, cor: 0.8271, lr: 0.002\n",
      "setFeats, augmentation 1\n",
      "Batch 4 device: xla:1 time passed: 6.199 time per batch: 1.550\n",
      "Batch 8 device: xla:1 time passed: 10.635 time per batch: 1.329\n",
      "Batch 12 device: xla:1 time passed: 15.366 time per batch: 1.281\n",
      "Batch 16 device: xla:1 time passed: 20.077 time per batch: 1.255\n",
      "Batch 20 device: xla:1 time passed: 24.858 time per batch: 1.243\n",
      "Batch 24 device: xla:1 time passed: 29.636 time per batch: 1.235\n",
      "Batch 28 device: xla:1 time passed: 34.326 time per batch: 1.226\n",
      "Batch 32 device: xla:1 time passed: 39.111 time per batch: 1.222\n",
      "Batch 36 device: xla:1 time passed: 43.801 time per batch: 1.217\n",
      "Batch 40 device: xla:1 time passed: 48.586 time per batch: 1.215\n",
      "Batch 44 device: xla:1 time passed: 53.244 time per batch: 1.210\n",
      "Batch 48 device: xla:1 time passed: 58.020 time per batch: 1.209\n",
      "Batch 4 device: xla:1 time passed: 4.433 time per batch: 1.108\n",
      "Batch 8 device: xla:1 time passed: 7.049 time per batch: 0.881\n",
      "Batch 12 device: xla:1 time passed: 10.050 time per batch: 0.838\n",
      "Batch 16 device: xla:1 time passed: 12.977 time per batch: 0.811\n",
      "Batch 20 device: xla:1 time passed: 15.879 time per batch: 0.794\n",
      "Batch 24 device: xla:1 time passed: 18.894 time per batch: 0.787\n",
      "ver 37, epoch 10, fold 1, train ll: 0.0393, val ll: 0.0680, cor: 0.8296, lr: 0.002\n",
      "setFeats, augmentation 2\n",
      "Batch 4 device: xla:1 time passed: 6.206 time per batch: 1.552\n",
      "Batch 8 device: xla:1 time passed: 10.629 time per batch: 1.329\n",
      "Batch 12 device: xla:1 time passed: 15.439 time per batch: 1.287\n",
      "Batch 16 device: xla:1 time passed: 20.156 time per batch: 1.260\n",
      "Batch 20 device: xla:1 time passed: 24.895 time per batch: 1.245\n",
      "Batch 24 device: xla:1 time passed: 29.640 time per batch: 1.235\n",
      "Batch 28 device: xla:1 time passed: 34.335 time per batch: 1.226\n",
      "Batch 32 device: xla:1 time passed: 39.076 time per batch: 1.221\n",
      "Batch 36 device: xla:1 time passed: 43.873 time per batch: 1.219\n",
      "Batch 40 device: xla:1 time passed: 48.684 time per batch: 1.217\n",
      "Batch 44 device: xla:1 time passed: 53.398 time per batch: 1.214\n",
      "Batch 48 device: xla:1 time passed: 58.150 time per batch: 1.211\n",
      "Batch 4 device: xla:1 time passed: 4.384 time per batch: 1.096\n",
      "Batch 8 device: xla:1 time passed: 7.017 time per batch: 0.877\n",
      "Batch 12 device: xla:1 time passed: 9.973 time per batch: 0.831\n",
      "Batch 16 device: xla:1 time passed: 12.887 time per batch: 0.805\n",
      "Batch 20 device: xla:1 time passed: 15.893 time per batch: 0.795\n",
      "Batch 24 device: xla:1 time passed: 18.759 time per batch: 0.782\n",
      "ver 37, epoch 11, fold 1, train ll: 0.0388, val ll: 0.0679, cor: 0.8277, lr: 0.002\n",
      "total running time 334.8574233055115\n",
      "completed epochs: 11 starting now: 3\n",
      "DataSet 2 train size 12982 fold 1\n",
      "adding dummy serieses 108\n",
      "DataSet 2 valid size 6656 fold 1\n",
      "setFeats, augmentation 0\n",
      "dataset train: 12982 valid: 6656 loader train: 405 valid: 208\n",
      "loading model model.b11.f1.v37\n",
      "setFeats, augmentation 3\n",
      "Batch 4 device: xla:1 time passed: 6.402 time per batch: 1.600\n",
      "Batch 8 device: xla:1 time passed: 10.853 time per batch: 1.357\n",
      "Batch 12 device: xla:1 time passed: 15.599 time per batch: 1.300\n",
      "Batch 16 device: xla:1 time passed: 20.292 time per batch: 1.268\n",
      "Batch 20 device: xla:1 time passed: 25.018 time per batch: 1.251\n",
      "Batch 24 device: xla:1 time passed: 29.782 time per batch: 1.241\n",
      "Batch 28 device: xla:1 time passed: 34.494 time per batch: 1.232\n",
      "Batch 32 device: xla:1 time passed: 39.223 time per batch: 1.226\n",
      "Batch 36 device: xla:1 time passed: 43.884 time per batch: 1.219\n",
      "Batch 40 device: xla:1 time passed: 48.684 time per batch: 1.217\n",
      "Batch 44 device: xla:1 time passed: 53.441 time per batch: 1.215\n",
      "Batch 48 device: xla:1 time passed: 58.101 time per batch: 1.210\n",
      "Batch 4 device: xla:1 time passed: 4.358 time per batch: 1.089\n",
      "Batch 8 device: xla:1 time passed: 6.984 time per batch: 0.873\n",
      "Batch 12 device: xla:1 time passed: 9.929 time per batch: 0.827\n",
      "Batch 16 device: xla:1 time passed: 12.864 time per batch: 0.804\n",
      "Batch 20 device: xla:1 time passed: 15.818 time per batch: 0.791\n",
      "Batch 24 device: xla:1 time passed: 18.715 time per batch: 0.780\n",
      "ver 37, epoch 12, fold 1, train ll: 0.0379, val ll: 0.0668, cor: 0.8318, lr: 0.0002\n",
      "setFeats, augmentation 0\n",
      "Batch 4 device: xla:1 time passed: 6.248 time per batch: 1.562\n",
      "Batch 8 device: xla:1 time passed: 10.728 time per batch: 1.341\n",
      "Batch 12 device: xla:1 time passed: 15.414 time per batch: 1.284\n",
      "Batch 16 device: xla:1 time passed: 20.171 time per batch: 1.261\n",
      "Batch 20 device: xla:1 time passed: 24.893 time per batch: 1.245\n",
      "Batch 24 device: xla:1 time passed: 29.683 time per batch: 1.237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 28 device: xla:1 time passed: 34.352 time per batch: 1.227\n",
      "Batch 32 device: xla:1 time passed: 39.243 time per batch: 1.226\n",
      "Batch 36 device: xla:1 time passed: 43.888 time per batch: 1.219\n",
      "Batch 40 device: xla:1 time passed: 48.616 time per batch: 1.215\n",
      "Batch 44 device: xla:1 time passed: 53.329 time per batch: 1.212\n",
      "Batch 48 device: xla:1 time passed: 58.031 time per batch: 1.209\n",
      "Batch 4 device: xla:1 time passed: 4.391 time per batch: 1.098\n",
      "Batch 8 device: xla:1 time passed: 7.002 time per batch: 0.875\n",
      "Batch 12 device: xla:1 time passed: 9.942 time per batch: 0.829\n",
      "Batch 16 device: xla:1 time passed: 12.908 time per batch: 0.807\n",
      "Batch 20 device: xla:1 time passed: 15.787 time per batch: 0.789\n",
      "Batch 24 device: xla:1 time passed: 18.688 time per batch: 0.779\n",
      "ver 37, epoch 13, fold 1, train ll: 0.0377, val ll: 0.0665, cor: 0.8322, lr: 0.0002\n",
      "setFeats, augmentation 1\n",
      "Batch 4 device: xla:1 time passed: 6.268 time per batch: 1.567\n",
      "Batch 8 device: xla:1 time passed: 10.662 time per batch: 1.333\n",
      "Batch 12 device: xla:1 time passed: 15.438 time per batch: 1.287\n",
      "Batch 16 device: xla:1 time passed: 20.153 time per batch: 1.260\n",
      "Batch 20 device: xla:1 time passed: 24.856 time per batch: 1.243\n",
      "Batch 24 device: xla:1 time passed: 29.633 time per batch: 1.235\n",
      "Batch 28 device: xla:1 time passed: 34.367 time per batch: 1.227\n",
      "Batch 32 device: xla:1 time passed: 39.047 time per batch: 1.220\n",
      "Batch 36 device: xla:1 time passed: 43.830 time per batch: 1.217\n",
      "Batch 40 device: xla:1 time passed: 48.501 time per batch: 1.213\n",
      "Batch 44 device: xla:1 time passed: 53.245 time per batch: 1.210\n",
      "Batch 48 device: xla:1 time passed: 57.973 time per batch: 1.208\n",
      "Batch 4 device: xla:1 time passed: 4.488 time per batch: 1.122\n",
      "Batch 8 device: xla:1 time passed: 7.039 time per batch: 0.880\n",
      "Batch 12 device: xla:1 time passed: 9.971 time per batch: 0.831\n",
      "Batch 16 device: xla:1 time passed: 12.949 time per batch: 0.809\n",
      "Batch 20 device: xla:1 time passed: 15.880 time per batch: 0.794\n",
      "Batch 24 device: xla:1 time passed: 18.851 time per batch: 0.785\n",
      "ver 37, epoch 14, fold 1, train ll: 0.0374, val ll: 0.0665, cor: 0.8324, lr: 0.0002\n",
      "total running time 251.18300938606262\n"
     ]
    }
   ],
   "source": [
    "DATA_SMALL = False\n",
    "MIXUP = False\n",
    "learning_rate = 0.02\n",
    "weight_decay = 1e-3\n",
    "model, predictions = train_one(epochs=7, bs=bs, fold=1)\n",
    "learning_rate = 0.002\n",
    "model, predictions = train_one(epochs=4, bs=bs, fold=1)\n",
    "learning_rate = 0.0002\n",
    "model, predictions = train_one(epochs=3, bs=bs, fold=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epochs: 0 starting now: 7\n",
      "DataSet 2 train size 13036 fold 2\n",
      "adding dummy serieses 162\n",
      "DataSet 2 valid size 6656 fold 2\n",
      "setFeats, augmentation 0\n",
      "dataset train: 13036 valid: 6656 loader train: 407 valid: 208\n",
      "starting from scratch\n",
      "setFeats, augmentation 0\n",
      "Batch 4 device: xla:1 time passed: 6.341 time per batch: 1.585\n",
      "Batch 8 device: xla:1 time passed: 10.809 time per batch: 1.351\n",
      "Batch 12 device: xla:1 time passed: 15.432 time per batch: 1.286\n",
      "Batch 16 device: xla:1 time passed: 20.169 time per batch: 1.261\n",
      "Batch 20 device: xla:1 time passed: 24.892 time per batch: 1.245\n",
      "Batch 24 device: xla:1 time passed: 29.550 time per batch: 1.231\n",
      "Batch 28 device: xla:1 time passed: 34.377 time per batch: 1.228\n",
      "Batch 32 device: xla:1 time passed: 38.978 time per batch: 1.218\n",
      "Batch 36 device: xla:1 time passed: 43.673 time per batch: 1.213\n",
      "Batch 40 device: xla:1 time passed: 48.451 time per batch: 1.211\n",
      "Batch 44 device: xla:1 time passed: 53.066 time per batch: 1.206\n",
      "Batch 48 device: xla:1 time passed: 57.837 time per batch: 1.205\n",
      "Batch 4 device: xla:1 time passed: 4.379 time per batch: 1.095\n",
      "Batch 8 device: xla:1 time passed: 6.989 time per batch: 0.874\n",
      "Batch 12 device: xla:1 time passed: 9.922 time per batch: 0.827\n",
      "Batch 16 device: xla:1 time passed: 12.879 time per batch: 0.805\n",
      "Batch 20 device: xla:1 time passed: 15.791 time per batch: 0.790\n",
      "Batch 24 device: xla:1 time passed: 18.747 time per batch: 0.781\n",
      "ver 37, epoch 1, fold 2, train ll: 0.1569, val ll: 0.3241, cor: 0.6621, lr: 0.02\n",
      "setFeats, augmentation 1\n",
      "Batch 4 device: xla:1 time passed: 6.283 time per batch: 1.571\n",
      "Batch 8 device: xla:1 time passed: 10.685 time per batch: 1.336\n",
      "Batch 12 device: xla:1 time passed: 15.466 time per batch: 1.289\n",
      "Batch 16 device: xla:1 time passed: 20.114 time per batch: 1.257\n",
      "Batch 20 device: xla:1 time passed: 24.847 time per batch: 1.242\n",
      "Batch 24 device: xla:1 time passed: 29.606 time per batch: 1.234\n",
      "Batch 28 device: xla:1 time passed: 34.303 time per batch: 1.225\n",
      "Batch 32 device: xla:1 time passed: 39.130 time per batch: 1.223\n",
      "Batch 36 device: xla:1 time passed: 43.783 time per batch: 1.216\n",
      "Batch 40 device: xla:1 time passed: 48.430 time per batch: 1.211\n",
      "Batch 44 device: xla:1 time passed: 53.166 time per batch: 1.208\n",
      "Batch 48 device: xla:1 time passed: 57.885 time per batch: 1.206\n",
      "Batch 4 device: xla:1 time passed: 4.694 time per batch: 1.174\n",
      "Batch 8 device: xla:1 time passed: 7.084 time per batch: 0.886\n",
      "Batch 12 device: xla:1 time passed: 10.021 time per batch: 0.835\n",
      "Batch 16 device: xla:1 time passed: 12.952 time per batch: 0.809\n",
      "Batch 20 device: xla:1 time passed: 15.910 time per batch: 0.796\n",
      "Batch 24 device: xla:1 time passed: 18.845 time per batch: 0.785\n",
      "ver 37, epoch 2, fold 2, train ll: 0.0480, val ll: 0.1338, cor: 0.7306, lr: 0.02\n",
      "setFeats, augmentation 2\n",
      "Batch 4 device: xla:1 time passed: 6.236 time per batch: 1.559\n",
      "Batch 8 device: xla:1 time passed: 10.605 time per batch: 1.326\n",
      "Batch 12 device: xla:1 time passed: 15.358 time per batch: 1.280\n",
      "Batch 16 device: xla:1 time passed: 20.067 time per batch: 1.254\n",
      "Batch 20 device: xla:1 time passed: 24.832 time per batch: 1.242\n",
      "Batch 24 device: xla:1 time passed: 29.548 time per batch: 1.231\n",
      "Batch 28 device: xla:1 time passed: 34.300 time per batch: 1.225\n",
      "Batch 32 device: xla:1 time passed: 38.973 time per batch: 1.218\n",
      "Batch 36 device: xla:1 time passed: 43.708 time per batch: 1.214\n",
      "Batch 40 device: xla:1 time passed: 48.434 time per batch: 1.211\n",
      "Batch 44 device: xla:1 time passed: 53.252 time per batch: 1.210\n",
      "Batch 48 device: xla:1 time passed: 57.970 time per batch: 1.208\n",
      "Batch 4 device: xla:1 time passed: 4.512 time per batch: 1.128\n",
      "Batch 8 device: xla:1 time passed: 6.971 time per batch: 0.871\n",
      "Batch 12 device: xla:1 time passed: 9.949 time per batch: 0.829\n",
      "Batch 16 device: xla:1 time passed: 12.901 time per batch: 0.806\n",
      "Batch 20 device: xla:1 time passed: 15.850 time per batch: 0.793\n",
      "Batch 24 device: xla:1 time passed: 18.759 time per batch: 0.782\n",
      "ver 37, epoch 3, fold 2, train ll: 0.0476, val ll: 0.1073, cor: 0.7475, lr: 0.02\n",
      "setFeats, augmentation 3\n",
      "Batch 4 device: xla:1 time passed: 6.226 time per batch: 1.557\n",
      "Batch 8 device: xla:1 time passed: 10.693 time per batch: 1.337\n",
      "Batch 12 device: xla:1 time passed: 15.518 time per batch: 1.293\n",
      "Batch 16 device: xla:1 time passed: 20.134 time per batch: 1.258\n",
      "Batch 20 device: xla:1 time passed: 24.827 time per batch: 1.241\n",
      "Batch 24 device: xla:1 time passed: 29.462 time per batch: 1.228\n",
      "Batch 28 device: xla:1 time passed: 34.190 time per batch: 1.221\n",
      "Batch 32 device: xla:1 time passed: 38.912 time per batch: 1.216\n",
      "Batch 36 device: xla:1 time passed: 43.602 time per batch: 1.211\n",
      "Batch 40 device: xla:1 time passed: 48.292 time per batch: 1.207\n",
      "Batch 44 device: xla:1 time passed: 52.989 time per batch: 1.204\n",
      "Batch 48 device: xla:1 time passed: 57.689 time per batch: 1.202\n",
      "Batch 4 device: xla:1 time passed: 4.536 time per batch: 1.134\n",
      "Batch 8 device: xla:1 time passed: 7.029 time per batch: 0.879\n",
      "Batch 12 device: xla:1 time passed: 10.011 time per batch: 0.834\n",
      "Batch 16 device: xla:1 time passed: 12.963 time per batch: 0.810\n",
      "Batch 20 device: xla:1 time passed: 15.859 time per batch: 0.793\n",
      "Batch 24 device: xla:1 time passed: 18.798 time per batch: 0.783\n",
      "ver 37, epoch 4, fold 2, train ll: 0.0460, val ll: 0.0880, cor: 0.7792, lr: 0.02\n",
      "setFeats, augmentation 0\n",
      "Batch 4 device: xla:1 time passed: 6.249 time per batch: 1.562\n",
      "Batch 8 device: xla:1 time passed: 10.638 time per batch: 1.330\n",
      "Batch 12 device: xla:1 time passed: 15.331 time per batch: 1.278\n",
      "Batch 16 device: xla:1 time passed: 20.023 time per batch: 1.251\n",
      "Batch 20 device: xla:1 time passed: 24.772 time per batch: 1.239\n",
      "Batch 24 device: xla:1 time passed: 29.506 time per batch: 1.229\n",
      "Batch 28 device: xla:1 time passed: 34.292 time per batch: 1.225\n",
      "Batch 32 device: xla:1 time passed: 38.904 time per batch: 1.216\n",
      "Batch 36 device: xla:1 time passed: 43.654 time per batch: 1.213\n",
      "Batch 40 device: xla:1 time passed: 48.366 time per batch: 1.209\n",
      "Batch 44 device: xla:1 time passed: 53.134 time per batch: 1.208\n",
      "Batch 48 device: xla:1 time passed: 57.858 time per batch: 1.205\n",
      "Batch 4 device: xla:1 time passed: 4.469 time per batch: 1.117\n",
      "Batch 8 device: xla:1 time passed: 7.092 time per batch: 0.887\n",
      "Batch 12 device: xla:1 time passed: 10.054 time per batch: 0.838\n",
      "Batch 16 device: xla:1 time passed: 12.971 time per batch: 0.811\n",
      "Batch 20 device: xla:1 time passed: 15.915 time per batch: 0.796\n",
      "Batch 24 device: xla:1 time passed: 18.871 time per batch: 0.786\n",
      "ver 37, epoch 5, fold 2, train ll: 0.0458, val ll: 0.0856, cor: 0.7942, lr: 0.02\n",
      "setFeats, augmentation 1\n",
      "Batch 4 device: xla:1 time passed: 6.189 time per batch: 1.547\n",
      "Batch 8 device: xla:1 time passed: 10.613 time per batch: 1.327\n",
      "Batch 12 device: xla:1 time passed: 15.396 time per batch: 1.283\n",
      "Batch 16 device: xla:1 time passed: 20.090 time per batch: 1.256\n",
      "Batch 20 device: xla:1 time passed: 24.802 time per batch: 1.240\n",
      "Batch 24 device: xla:1 time passed: 29.552 time per batch: 1.231\n",
      "Batch 28 device: xla:1 time passed: 34.211 time per batch: 1.222\n",
      "Batch 32 device: xla:1 time passed: 39.006 time per batch: 1.219\n",
      "Batch 36 device: xla:1 time passed: 43.701 time per batch: 1.214\n",
      "Batch 40 device: xla:1 time passed: 48.412 time per batch: 1.210\n",
      "Batch 44 device: xla:1 time passed: 53.119 time per batch: 1.207\n",
      "Batch 48 device: xla:1 time passed: 57.835 time per batch: 1.205\n",
      "Batch 4 device: xla:1 time passed: 4.301 time per batch: 1.075\n",
      "Batch 8 device: xla:1 time passed: 6.967 time per batch: 0.871\n",
      "Batch 12 device: xla:1 time passed: 9.868 time per batch: 0.822\n",
      "Batch 16 device: xla:1 time passed: 12.944 time per batch: 0.809\n",
      "Batch 20 device: xla:1 time passed: 15.764 time per batch: 0.788\n",
      "Batch 24 device: xla:1 time passed: 18.675 time per batch: 0.778\n",
      "ver 37, epoch 6, fold 2, train ll: 0.0472, val ll: 0.0806, cor: 0.7988, lr: 0.02\n",
      "setFeats, augmentation 2\n",
      "Batch 4 device: xla:1 time passed: 6.315 time per batch: 1.579\n",
      "Batch 8 device: xla:1 time passed: 10.683 time per batch: 1.335\n",
      "Batch 12 device: xla:1 time passed: 15.402 time per batch: 1.283\n",
      "Batch 16 device: xla:1 time passed: 20.071 time per batch: 1.254\n",
      "Batch 20 device: xla:1 time passed: 24.827 time per batch: 1.241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 24 device: xla:1 time passed: 29.549 time per batch: 1.231\n",
      "Batch 28 device: xla:1 time passed: 34.281 time per batch: 1.224\n",
      "Batch 32 device: xla:1 time passed: 39.004 time per batch: 1.219\n",
      "Batch 36 device: xla:1 time passed: 43.720 time per batch: 1.214\n",
      "Batch 40 device: xla:1 time passed: 48.470 time per batch: 1.212\n",
      "Batch 44 device: xla:1 time passed: 53.141 time per batch: 1.208\n",
      "Batch 48 device: xla:1 time passed: 57.956 time per batch: 1.207\n",
      "Batch 4 device: xla:1 time passed: 4.564 time per batch: 1.141\n",
      "Batch 8 device: xla:1 time passed: 7.135 time per batch: 0.892\n",
      "Batch 12 device: xla:1 time passed: 9.997 time per batch: 0.833\n",
      "Batch 16 device: xla:1 time passed: 12.974 time per batch: 0.811\n",
      "Batch 20 device: xla:1 time passed: 15.926 time per batch: 0.796\n",
      "Batch 24 device: xla:1 time passed: 18.863 time per batch: 0.786\n",
      "ver 37, epoch 7, fold 2, train ll: 0.0447, val ll: 0.0738, cor: 0.8136, lr: 0.02\n",
      "total running time 582.0936133861542\n",
      "completed epochs: 7 starting now: 4\n",
      "DataSet 2 train size 13036 fold 2\n",
      "adding dummy serieses 162\n",
      "DataSet 2 valid size 6656 fold 2\n",
      "setFeats, augmentation 0\n",
      "dataset train: 13036 valid: 6656 loader train: 407 valid: 208\n",
      "loading model model.b7.f2.v37\n",
      "setFeats, augmentation 3\n",
      "Batch 4 device: xla:1 time passed: 6.380 time per batch: 1.595\n",
      "Batch 8 device: xla:1 time passed: 10.788 time per batch: 1.348\n",
      "Batch 12 device: xla:1 time passed: 15.490 time per batch: 1.291\n",
      "Batch 16 device: xla:1 time passed: 20.277 time per batch: 1.267\n",
      "Batch 20 device: xla:1 time passed: 25.000 time per batch: 1.250\n",
      "Batch 24 device: xla:1 time passed: 29.717 time per batch: 1.238\n",
      "Batch 28 device: xla:1 time passed: 34.320 time per batch: 1.226\n",
      "Batch 32 device: xla:1 time passed: 39.024 time per batch: 1.220\n",
      "Batch 36 device: xla:1 time passed: 43.734 time per batch: 1.215\n",
      "Batch 40 device: xla:1 time passed: 48.391 time per batch: 1.210\n",
      "Batch 44 device: xla:1 time passed: 53.156 time per batch: 1.208\n",
      "Batch 48 device: xla:1 time passed: 57.831 time per batch: 1.205\n",
      "Batch 4 device: xla:1 time passed: 4.475 time per batch: 1.119\n",
      "Batch 8 device: xla:1 time passed: 7.127 time per batch: 0.891\n",
      "Batch 12 device: xla:1 time passed: 10.062 time per batch: 0.838\n",
      "Batch 16 device: xla:1 time passed: 13.058 time per batch: 0.816\n",
      "Batch 20 device: xla:1 time passed: 15.954 time per batch: 0.798\n",
      "Batch 24 device: xla:1 time passed: 18.906 time per batch: 0.788\n",
      "ver 37, epoch 8, fold 2, train ll: 0.0397, val ll: 0.0666, cor: 0.8308, lr: 0.002\n",
      "setFeats, augmentation 0\n",
      "Batch 4 device: xla:1 time passed: 6.302 time per batch: 1.576\n",
      "Batch 8 device: xla:1 time passed: 10.685 time per batch: 1.336\n",
      "Batch 12 device: xla:1 time passed: 15.441 time per batch: 1.287\n",
      "Batch 16 device: xla:1 time passed: 20.173 time per batch: 1.261\n",
      "Batch 20 device: xla:1 time passed: 24.871 time per batch: 1.244\n",
      "Batch 24 device: xla:1 time passed: 29.563 time per batch: 1.232\n",
      "Batch 28 device: xla:1 time passed: 34.347 time per batch: 1.227\n",
      "Batch 32 device: xla:1 time passed: 39.146 time per batch: 1.223\n",
      "Batch 36 device: xla:1 time passed: 43.850 time per batch: 1.218\n",
      "Batch 40 device: xla:1 time passed: 48.507 time per batch: 1.213\n",
      "Batch 44 device: xla:1 time passed: 53.196 time per batch: 1.209\n",
      "Batch 48 device: xla:1 time passed: 57.868 time per batch: 1.206\n",
      "Batch 4 device: xla:1 time passed: 4.759 time per batch: 1.190\n",
      "Batch 8 device: xla:1 time passed: 7.149 time per batch: 0.894\n",
      "Batch 12 device: xla:1 time passed: 10.003 time per batch: 0.834\n",
      "Batch 16 device: xla:1 time passed: 13.001 time per batch: 0.813\n",
      "Batch 20 device: xla:1 time passed: 15.941 time per batch: 0.797\n",
      "Batch 24 device: xla:1 time passed: 18.864 time per batch: 0.786\n",
      "ver 37, epoch 9, fold 2, train ll: 0.0392, val ll: 0.0662, cor: 0.8287, lr: 0.002\n",
      "setFeats, augmentation 1\n",
      "Batch 4 device: xla:1 time passed: 6.296 time per batch: 1.574\n",
      "Batch 8 device: xla:1 time passed: 10.699 time per batch: 1.337\n",
      "Batch 12 device: xla:1 time passed: 15.408 time per batch: 1.284\n",
      "Batch 16 device: xla:1 time passed: 20.163 time per batch: 1.260\n",
      "Batch 20 device: xla:1 time passed: 24.903 time per batch: 1.245\n",
      "Batch 24 device: xla:1 time passed: 29.584 time per batch: 1.233\n",
      "Batch 28 device: xla:1 time passed: 34.347 time per batch: 1.227\n",
      "Batch 32 device: xla:1 time passed: 39.067 time per batch: 1.221\n",
      "Batch 36 device: xla:1 time passed: 43.972 time per batch: 1.221\n",
      "Batch 40 device: xla:1 time passed: 48.596 time per batch: 1.215\n",
      "Batch 44 device: xla:1 time passed: 53.263 time per batch: 1.211\n",
      "Batch 48 device: xla:1 time passed: 58.037 time per batch: 1.209\n",
      "Batch 4 device: xla:1 time passed: 4.455 time per batch: 1.114\n",
      "Batch 8 device: xla:1 time passed: 7.040 time per batch: 0.880\n",
      "Batch 12 device: xla:1 time passed: 9.990 time per batch: 0.832\n",
      "Batch 16 device: xla:1 time passed: 12.974 time per batch: 0.811\n",
      "Batch 20 device: xla:1 time passed: 15.917 time per batch: 0.796\n",
      "Batch 24 device: xla:1 time passed: 18.843 time per batch: 0.785\n",
      "ver 37, epoch 10, fold 2, train ll: 0.0384, val ll: 0.0668, cor: 0.8283, lr: 0.002\n",
      "setFeats, augmentation 2\n",
      "Batch 4 device: xla:1 time passed: 6.216 time per batch: 1.554\n",
      "Batch 8 device: xla:1 time passed: 10.685 time per batch: 1.336\n",
      "Batch 12 device: xla:1 time passed: 15.372 time per batch: 1.281\n",
      "Batch 16 device: xla:1 time passed: 20.075 time per batch: 1.255\n",
      "Batch 20 device: xla:1 time passed: 24.876 time per batch: 1.244\n",
      "Batch 24 device: xla:1 time passed: 29.508 time per batch: 1.230\n",
      "Batch 28 device: xla:1 time passed: 34.278 time per batch: 1.224\n",
      "Batch 32 device: xla:1 time passed: 38.969 time per batch: 1.218\n",
      "Batch 36 device: xla:1 time passed: 43.764 time per batch: 1.216\n",
      "Batch 40 device: xla:1 time passed: 48.411 time per batch: 1.210\n",
      "Batch 44 device: xla:1 time passed: 53.188 time per batch: 1.209\n",
      "Batch 48 device: xla:1 time passed: 57.841 time per batch: 1.205\n",
      "Batch 4 device: xla:1 time passed: 4.429 time per batch: 1.107\n",
      "Batch 8 device: xla:1 time passed: 7.105 time per batch: 0.888\n",
      "Batch 12 device: xla:1 time passed: 10.058 time per batch: 0.838\n",
      "Batch 16 device: xla:1 time passed: 12.987 time per batch: 0.812\n",
      "Batch 20 device: xla:1 time passed: 15.916 time per batch: 0.796\n",
      "Batch 24 device: xla:1 time passed: 18.855 time per batch: 0.786\n",
      "ver 37, epoch 11, fold 2, train ll: 0.0383, val ll: 0.0654, cor: 0.8314, lr: 0.002\n",
      "total running time 334.4663259983063\n",
      "completed epochs: 11 starting now: 3\n",
      "DataSet 2 train size 13036 fold 2\n",
      "adding dummy serieses 162\n",
      "DataSet 2 valid size 6656 fold 2\n",
      "setFeats, augmentation 0\n",
      "dataset train: 13036 valid: 6656 loader train: 407 valid: 208\n",
      "loading model model.b11.f2.v37\n",
      "setFeats, augmentation 3\n",
      "Batch 4 device: xla:1 time passed: 6.387 time per batch: 1.597\n",
      "Batch 8 device: xla:1 time passed: 10.763 time per batch: 1.345\n",
      "Batch 12 device: xla:1 time passed: 15.537 time per batch: 1.295\n",
      "Batch 16 device: xla:1 time passed: 20.241 time per batch: 1.265\n",
      "Batch 20 device: xla:1 time passed: 25.027 time per batch: 1.251\n",
      "Batch 24 device: xla:1 time passed: 29.654 time per batch: 1.236\n",
      "Batch 28 device: xla:1 time passed: 34.375 time per batch: 1.228\n",
      "Batch 32 device: xla:1 time passed: 39.078 time per batch: 1.221\n",
      "Batch 36 device: xla:1 time passed: 43.798 time per batch: 1.217\n",
      "Batch 40 device: xla:1 time passed: 48.472 time per batch: 1.212\n",
      "Batch 44 device: xla:1 time passed: 53.225 time per batch: 1.210\n",
      "Batch 48 device: xla:1 time passed: 57.897 time per batch: 1.206\n",
      "Batch 4 device: xla:1 time passed: 4.446 time per batch: 1.112\n",
      "Batch 8 device: xla:1 time passed: 7.106 time per batch: 0.888\n",
      "Batch 12 device: xla:1 time passed: 10.087 time per batch: 0.841\n",
      "Batch 16 device: xla:1 time passed: 12.995 time per batch: 0.812\n",
      "Batch 20 device: xla:1 time passed: 15.940 time per batch: 0.797\n",
      "Batch 24 device: xla:1 time passed: 18.898 time per batch: 0.787\n",
      "ver 37, epoch 12, fold 2, train ll: 0.0371, val ll: 0.0643, cor: 0.8357, lr: 0.0002\n",
      "setFeats, augmentation 0\n",
      "Batch 4 device: xla:1 time passed: 6.282 time per batch: 1.570\n",
      "Batch 8 device: xla:1 time passed: 10.683 time per batch: 1.335\n",
      "Batch 12 device: xla:1 time passed: 15.373 time per batch: 1.281\n",
      "Batch 16 device: xla:1 time passed: 20.134 time per batch: 1.258\n",
      "Batch 20 device: xla:1 time passed: 24.890 time per batch: 1.244\n",
      "Batch 24 device: xla:1 time passed: 29.504 time per batch: 1.229\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 28 device: xla:1 time passed: 34.258 time per batch: 1.224\n",
      "Batch 32 device: xla:1 time passed: 38.947 time per batch: 1.217\n",
      "Batch 36 device: xla:1 time passed: 43.724 time per batch: 1.215\n",
      "Batch 40 device: xla:1 time passed: 48.464 time per batch: 1.212\n",
      "Batch 44 device: xla:1 time passed: 53.166 time per batch: 1.208\n",
      "Batch 48 device: xla:1 time passed: 57.917 time per batch: 1.207\n",
      "Batch 4 device: xla:1 time passed: 4.482 time per batch: 1.120\n",
      "Batch 8 device: xla:1 time passed: 7.097 time per batch: 0.887\n",
      "Batch 12 device: xla:1 time passed: 9.968 time per batch: 0.831\n",
      "Batch 16 device: xla:1 time passed: 12.921 time per batch: 0.808\n",
      "Batch 20 device: xla:1 time passed: 15.865 time per batch: 0.793\n",
      "Batch 24 device: xla:1 time passed: 18.731 time per batch: 0.780\n",
      "ver 37, epoch 13, fold 2, train ll: 0.0369, val ll: 0.0641, cor: 0.8358, lr: 0.0002\n",
      "setFeats, augmentation 1\n",
      "Batch 4 device: xla:1 time passed: 6.344 time per batch: 1.586\n",
      "Batch 8 device: xla:1 time passed: 10.657 time per batch: 1.332\n",
      "Batch 12 device: xla:1 time passed: 15.364 time per batch: 1.280\n",
      "Batch 16 device: xla:1 time passed: 20.102 time per batch: 1.256\n",
      "Batch 20 device: xla:1 time passed: 24.843 time per batch: 1.242\n",
      "Batch 24 device: xla:1 time passed: 29.475 time per batch: 1.228\n",
      "Batch 28 device: xla:1 time passed: 34.243 time per batch: 1.223\n",
      "Batch 32 device: xla:1 time passed: 38.916 time per batch: 1.216\n",
      "Batch 36 device: xla:1 time passed: 43.721 time per batch: 1.214\n",
      "Batch 40 device: xla:1 time passed: 48.444 time per batch: 1.211\n",
      "Batch 44 device: xla:1 time passed: 53.069 time per batch: 1.206\n",
      "Batch 48 device: xla:1 time passed: 57.808 time per batch: 1.204\n",
      "Batch 4 device: xla:1 time passed: 4.481 time per batch: 1.120\n",
      "Batch 8 device: xla:1 time passed: 7.075 time per batch: 0.884\n",
      "Batch 12 device: xla:1 time passed: 10.007 time per batch: 0.834\n",
      "Batch 16 device: xla:1 time passed: 12.931 time per batch: 0.808\n",
      "Batch 20 device: xla:1 time passed: 15.835 time per batch: 0.792\n",
      "Batch 24 device: xla:1 time passed: 18.784 time per batch: 0.783\n",
      "ver 37, epoch 14, fold 2, train ll: 0.0367, val ll: 0.0641, cor: 0.8360, lr: 0.0002\n",
      "total running time 251.14941668510437\n"
     ]
    }
   ],
   "source": [
    "DATA_SMALL = False\n",
    "MIXUP = False\n",
    "learning_rate = 0.02\n",
    "weight_decay = 1e-3\n",
    "model, predictions = train_one(epochs=7, bs=bs, fold=2)\n",
    "learning_rate = 0.002\n",
    "model, predictions = train_one(epochs=4, bs=bs, fold=2)\n",
    "learning_rate = 0.0002\n",
    "model, predictions = train_one(epochs=3, bs=bs, fold=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>fold</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>cor</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "      <th>train_sz</th>\n",
       "      <th>val_sz</th>\n",
       "      <th>bs</th>\n",
       "      <th>train_time</th>\n",
       "      <th>valid_time</th>\n",
       "      <th>lr</th>\n",
       "      <th>wd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.495845</td>\n",
       "      <td>0.222767</td>\n",
       "      <td>0.262498</td>\n",
       "      <td>0.372876</td>\n",
       "      <td>0.077706</td>\n",
       "      <td>0.184072</td>\n",
       "      <td>0.147871</td>\n",
       "      <td>0.184957</td>\n",
       "      <td>0.219007</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>80.560221</td>\n",
       "      <td>140.067585</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.139651</td>\n",
       "      <td>0.212226</td>\n",
       "      <td>0.341014</td>\n",
       "      <td>0.371492</td>\n",
       "      <td>0.032217</td>\n",
       "      <td>0.182368</td>\n",
       "      <td>0.141474</td>\n",
       "      <td>0.176040</td>\n",
       "      <td>0.210502</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>80.586969</td>\n",
       "      <td>143.552010</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.116839</td>\n",
       "      <td>0.206248</td>\n",
       "      <td>0.416058</td>\n",
       "      <td>0.357151</td>\n",
       "      <td>0.030161</td>\n",
       "      <td>0.177022</td>\n",
       "      <td>0.138181</td>\n",
       "      <td>0.175438</td>\n",
       "      <td>0.208632</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>81.362263</td>\n",
       "      <td>142.334380</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.100432</td>\n",
       "      <td>0.248836</td>\n",
       "      <td>0.337192</td>\n",
       "      <td>0.446849</td>\n",
       "      <td>0.031468</td>\n",
       "      <td>0.198398</td>\n",
       "      <td>0.151840</td>\n",
       "      <td>0.208802</td>\n",
       "      <td>0.257649</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>80.487220</td>\n",
       "      <td>142.352772</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.101299</td>\n",
       "      <td>0.198162</td>\n",
       "      <td>0.448064</td>\n",
       "      <td>0.339129</td>\n",
       "      <td>0.027744</td>\n",
       "      <td>0.173676</td>\n",
       "      <td>0.136346</td>\n",
       "      <td>0.179236</td>\n",
       "      <td>0.191874</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>84.477289</td>\n",
       "      <td>146.857369</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.097309</td>\n",
       "      <td>0.190345</td>\n",
       "      <td>0.473876</td>\n",
       "      <td>0.327777</td>\n",
       "      <td>0.027687</td>\n",
       "      <td>0.166361</td>\n",
       "      <td>0.123394</td>\n",
       "      <td>0.165293</td>\n",
       "      <td>0.194129</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>81.727369</td>\n",
       "      <td>143.910612</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095991</td>\n",
       "      <td>0.179739</td>\n",
       "      <td>0.478224</td>\n",
       "      <td>0.308505</td>\n",
       "      <td>0.027745</td>\n",
       "      <td>0.164267</td>\n",
       "      <td>0.121925</td>\n",
       "      <td>0.155056</td>\n",
       "      <td>0.172170</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>81.609755</td>\n",
       "      <td>140.314667</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.105157</td>\n",
       "      <td>0.162818</td>\n",
       "      <td>0.491558</td>\n",
       "      <td>0.280636</td>\n",
       "      <td>0.027615</td>\n",
       "      <td>0.150090</td>\n",
       "      <td>0.105528</td>\n",
       "      <td>0.135659</td>\n",
       "      <td>0.159565</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>81.324294</td>\n",
       "      <td>141.004881</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.101255</td>\n",
       "      <td>0.182301</td>\n",
       "      <td>0.492721</td>\n",
       "      <td>0.322835</td>\n",
       "      <td>0.027490</td>\n",
       "      <td>0.159502</td>\n",
       "      <td>0.115876</td>\n",
       "      <td>0.152459</td>\n",
       "      <td>0.175106</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>81.279920</td>\n",
       "      <td>140.633701</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.103850</td>\n",
       "      <td>0.161570</td>\n",
       "      <td>0.499187</td>\n",
       "      <td>0.276134</td>\n",
       "      <td>0.029684</td>\n",
       "      <td>0.149958</td>\n",
       "      <td>0.109171</td>\n",
       "      <td>0.133094</td>\n",
       "      <td>0.156818</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>81.465451</td>\n",
       "      <td>142.004258</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.089629</td>\n",
       "      <td>0.163397</td>\n",
       "      <td>0.510048</td>\n",
       "      <td>0.281652</td>\n",
       "      <td>0.028058</td>\n",
       "      <td>0.149294</td>\n",
       "      <td>0.107586</td>\n",
       "      <td>0.132725</td>\n",
       "      <td>0.162813</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>79.851575</td>\n",
       "      <td>141.318649</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.089327</td>\n",
       "      <td>0.160301</td>\n",
       "      <td>0.517028</td>\n",
       "      <td>0.276732</td>\n",
       "      <td>0.027610</td>\n",
       "      <td>0.147871</td>\n",
       "      <td>0.105576</td>\n",
       "      <td>0.131298</td>\n",
       "      <td>0.156291</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>80.063437</td>\n",
       "      <td>140.657684</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.088809</td>\n",
       "      <td>0.158927</td>\n",
       "      <td>0.515110</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>0.027109</td>\n",
       "      <td>0.145623</td>\n",
       "      <td>0.105583</td>\n",
       "      <td>0.130697</td>\n",
       "      <td>0.155481</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>80.327655</td>\n",
       "      <td>142.896167</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.088837</td>\n",
       "      <td>0.155999</td>\n",
       "      <td>0.521167</td>\n",
       "      <td>0.270748</td>\n",
       "      <td>0.026655</td>\n",
       "      <td>0.143846</td>\n",
       "      <td>0.101793</td>\n",
       "      <td>0.126280</td>\n",
       "      <td>0.151921</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>80.514647</td>\n",
       "      <td>144.235245</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.088136</td>\n",
       "      <td>0.157832</td>\n",
       "      <td>0.523962</td>\n",
       "      <td>0.274537</td>\n",
       "      <td>0.026952</td>\n",
       "      <td>0.144709</td>\n",
       "      <td>0.101704</td>\n",
       "      <td>0.127705</td>\n",
       "      <td>0.154680</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>80.382700</td>\n",
       "      <td>143.656591</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.088043</td>\n",
       "      <td>0.155825</td>\n",
       "      <td>0.526298</td>\n",
       "      <td>0.272011</td>\n",
       "      <td>0.026415</td>\n",
       "      <td>0.142138</td>\n",
       "      <td>0.099895</td>\n",
       "      <td>0.126003</td>\n",
       "      <td>0.152305</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>80.809602</td>\n",
       "      <td>141.776692</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0.086902</td>\n",
       "      <td>0.154525</td>\n",
       "      <td>0.529515</td>\n",
       "      <td>0.269470</td>\n",
       "      <td>0.026794</td>\n",
       "      <td>0.141182</td>\n",
       "      <td>0.098794</td>\n",
       "      <td>0.123728</td>\n",
       "      <td>0.152238</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>80.541241</td>\n",
       "      <td>141.074992</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.086735</td>\n",
       "      <td>0.154822</td>\n",
       "      <td>0.530893</td>\n",
       "      <td>0.270364</td>\n",
       "      <td>0.026630</td>\n",
       "      <td>0.141084</td>\n",
       "      <td>0.098386</td>\n",
       "      <td>0.123932</td>\n",
       "      <td>0.152994</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>80.734518</td>\n",
       "      <td>142.519661</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.086563</td>\n",
       "      <td>0.153274</td>\n",
       "      <td>0.531932</td>\n",
       "      <td>0.267862</td>\n",
       "      <td>0.026561</td>\n",
       "      <td>0.140366</td>\n",
       "      <td>0.097915</td>\n",
       "      <td>0.123196</td>\n",
       "      <td>0.149155</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>80.243078</td>\n",
       "      <td>143.743049</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.086106</td>\n",
       "      <td>0.364768</td>\n",
       "      <td>0.062663</td>\n",
       "      <td>0.698073</td>\n",
       "      <td>0.033167</td>\n",
       "      <td>0.267723</td>\n",
       "      <td>0.216273</td>\n",
       "      <td>0.286145</td>\n",
       "      <td>0.353920</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>64.752618</td>\n",
       "      <td>22.980388</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0.138757</td>\n",
       "      <td>0.242859</td>\n",
       "      <td>0.235416</td>\n",
       "      <td>0.429052</td>\n",
       "      <td>0.034606</td>\n",
       "      <td>0.201127</td>\n",
       "      <td>0.150016</td>\n",
       "      <td>0.216166</td>\n",
       "      <td>0.239994</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>47.874474</td>\n",
       "      <td>16.058632</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0.121541</td>\n",
       "      <td>0.212181</td>\n",
       "      <td>0.299105</td>\n",
       "      <td>0.364818</td>\n",
       "      <td>0.031306</td>\n",
       "      <td>0.177752</td>\n",
       "      <td>0.139025</td>\n",
       "      <td>0.188272</td>\n",
       "      <td>0.219280</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>48.058998</td>\n",
       "      <td>16.376153</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  fold  train_loss  val_loss       cor       any  epidural  \\\n",
       "0       1     0    0.495845  0.222767  0.262498  0.372876  0.077706   \n",
       "1       2     0    0.139651  0.212226  0.341014  0.371492  0.032217   \n",
       "2       3     0    0.116839  0.206248  0.416058  0.357151  0.030161   \n",
       "3       4     0    0.100432  0.248836  0.337192  0.446849  0.031468   \n",
       "4       5     0    0.101299  0.198162  0.448064  0.339129  0.027744   \n",
       "5       6     0    0.097309  0.190345  0.473876  0.327777  0.027687   \n",
       "6       7     0    0.095991  0.179739  0.478224  0.308505  0.027745   \n",
       "7       8     0    0.105157  0.162818  0.491558  0.280636  0.027615   \n",
       "8       9     0    0.101255  0.182301  0.492721  0.322835  0.027490   \n",
       "9      10     0    0.103850  0.161570  0.499187  0.276134  0.029684   \n",
       "10     13     0    0.089629  0.163397  0.510048  0.281652  0.028058   \n",
       "11     14     0    0.089327  0.160301  0.517028  0.276732  0.027610   \n",
       "12     15     0    0.088809  0.158927  0.515110  0.274000  0.027109   \n",
       "13     16     0    0.088837  0.155999  0.521167  0.270748  0.026655   \n",
       "14     17     0    0.088136  0.157832  0.523962  0.274537  0.026952   \n",
       "15     18     0    0.088043  0.155825  0.526298  0.272011  0.026415   \n",
       "16     19     0    0.086902  0.154525  0.529515  0.269470  0.026794   \n",
       "17     20     0    0.086735  0.154822  0.530893  0.270364  0.026630   \n",
       "18     21     0    0.086563  0.153274  0.531932  0.267862  0.026561   \n",
       "19     25     0    0.086106  0.364768  0.062663  0.698073  0.033167   \n",
       "20     26     0    0.138757  0.242859  0.235416  0.429052  0.034606   \n",
       "21     27     0    0.121541  0.212181  0.299105  0.364818  0.031306   \n",
       "\n",
       "    intraparenchymal  intraventricular  subarachnoid  subdural  train_sz  \\\n",
       "0           0.184072          0.147871      0.184957  0.219007     13042   \n",
       "1           0.182368          0.141474      0.176040  0.210502     13042   \n",
       "2           0.177022          0.138181      0.175438  0.208632     13042   \n",
       "3           0.198398          0.151840      0.208802  0.257649     13042   \n",
       "4           0.173676          0.136346      0.179236  0.191874     13042   \n",
       "5           0.166361          0.123394      0.165293  0.194129     13042   \n",
       "6           0.164267          0.121925      0.155056  0.172170     13042   \n",
       "7           0.150090          0.105528      0.135659  0.159565     13042   \n",
       "8           0.159502          0.115876      0.152459  0.175106     13042   \n",
       "9           0.149958          0.109171      0.133094  0.156818     13042   \n",
       "10          0.149294          0.107586      0.132725  0.162813     13042   \n",
       "11          0.147871          0.105576      0.131298  0.156291     13042   \n",
       "12          0.145623          0.105583      0.130697  0.155481     13042   \n",
       "13          0.143846          0.101793      0.126280  0.151921     13042   \n",
       "14          0.144709          0.101704      0.127705  0.154680     13042   \n",
       "15          0.142138          0.099895      0.126003  0.152305     13042   \n",
       "16          0.141182          0.098794      0.123728  0.152238     13042   \n",
       "17          0.141084          0.098386      0.123932  0.152994     13042   \n",
       "18          0.140366          0.097915      0.123196  0.149155     13042   \n",
       "19          0.267723          0.216273      0.286145  0.353920     13042   \n",
       "20          0.201127          0.150016      0.216166  0.239994     13042   \n",
       "21          0.177752          0.139025      0.188272  0.219280     13042   \n",
       "\n",
       "    val_sz   bs  train_time  valid_time      lr     wd  \n",
       "0     7200  100   80.560221  140.067585  0.0200  0.001  \n",
       "1     7200  100   80.586969  143.552010  0.0200  0.001  \n",
       "2     7200  100   81.362263  142.334380  0.0200  0.001  \n",
       "3     7200  100   80.487220  142.352772  0.0200  0.001  \n",
       "4     7200  100   84.477289  146.857369  0.0200  0.001  \n",
       "5     7200  100   81.727369  143.910612  0.0200  0.001  \n",
       "6     7200  100   81.609755  140.314667  0.0200  0.001  \n",
       "7     7200  100   81.324294  141.004881  0.0200  0.001  \n",
       "8     7200  100   81.279920  140.633701  0.0200  0.001  \n",
       "9     7200  100   81.465451  142.004258  0.0200  0.001  \n",
       "10    7200  100   79.851575  141.318649  0.0020  0.001  \n",
       "11    7200  100   80.063437  140.657684  0.0020  0.001  \n",
       "12    7200  100   80.327655  142.896167  0.0020  0.001  \n",
       "13    7200  100   80.514647  144.235245  0.0020  0.001  \n",
       "14    7200  100   80.382700  143.656591  0.0020  0.001  \n",
       "15    7200  100   80.809602  141.776692  0.0020  0.001  \n",
       "16    7200  100   80.541241  141.074992  0.0002  0.001  \n",
       "17    7200  100   80.734518  142.519661  0.0002  0.001  \n",
       "18    7200  100   80.243078  143.743049  0.0002  0.001  \n",
       "19    7200  100   64.752618   22.980388  0.0002  0.001  \n",
       "20    7200  100   47.874474   16.058632  0.0002  0.001  \n",
       "21    7200  100   48.058998   16.376153  0.0002  0.001  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(PATH_WORK/'stats.f{}.v{}'.format(0,VERSION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>fold</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>cor</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "      <th>train_sz</th>\n",
       "      <th>val_sz</th>\n",
       "      <th>bs</th>\n",
       "      <th>train_time</th>\n",
       "      <th>valid_time</th>\n",
       "      <th>lr</th>\n",
       "      <th>wd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.268860</td>\n",
       "      <td>0.143166</td>\n",
       "      <td>0.613321</td>\n",
       "      <td>0.163072</td>\n",
       "      <td>0.043452</td>\n",
       "      <td>0.119038</td>\n",
       "      <td>0.088011</td>\n",
       "      <td>0.204062</td>\n",
       "      <td>0.221455</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>366.598575</td>\n",
       "      <td>55.069581</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.163124</td>\n",
       "      <td>0.621400</td>\n",
       "      <td>0.279981</td>\n",
       "      <td>1.371310</td>\n",
       "      <td>0.041597</td>\n",
       "      <td>0.409983</td>\n",
       "      <td>0.280379</td>\n",
       "      <td>0.337232</td>\n",
       "      <td>0.537986</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>157.264272</td>\n",
       "      <td>29.822798</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.121807</td>\n",
       "      <td>0.216327</td>\n",
       "      <td>0.506429</td>\n",
       "      <td>0.331250</td>\n",
       "      <td>0.025227</td>\n",
       "      <td>0.253565</td>\n",
       "      <td>0.086670</td>\n",
       "      <td>0.267595</td>\n",
       "      <td>0.218730</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>156.105009</td>\n",
       "      <td>26.639628</td>\n",
       "      <td>0.1000</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.055151</td>\n",
       "      <td>0.159198</td>\n",
       "      <td>0.640255</td>\n",
       "      <td>0.259493</td>\n",
       "      <td>0.016848</td>\n",
       "      <td>0.186059</td>\n",
       "      <td>0.071142</td>\n",
       "      <td>0.168241</td>\n",
       "      <td>0.153109</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>158.559280</td>\n",
       "      <td>26.512727</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046670</td>\n",
       "      <td>0.132980</td>\n",
       "      <td>0.701180</td>\n",
       "      <td>0.218808</td>\n",
       "      <td>0.015350</td>\n",
       "      <td>0.134875</td>\n",
       "      <td>0.061654</td>\n",
       "      <td>0.129225</td>\n",
       "      <td>0.152139</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>159.042082</td>\n",
       "      <td>28.155369</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.046544</td>\n",
       "      <td>0.081856</td>\n",
       "      <td>0.793531</td>\n",
       "      <td>0.133274</td>\n",
       "      <td>0.015343</td>\n",
       "      <td>0.065370</td>\n",
       "      <td>0.044277</td>\n",
       "      <td>0.083408</td>\n",
       "      <td>0.098045</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>160.890759</td>\n",
       "      <td>28.551072</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.042350</td>\n",
       "      <td>0.076822</td>\n",
       "      <td>0.802252</td>\n",
       "      <td>0.121926</td>\n",
       "      <td>0.014430</td>\n",
       "      <td>0.076024</td>\n",
       "      <td>0.038554</td>\n",
       "      <td>0.068125</td>\n",
       "      <td>0.096769</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>157.371435</td>\n",
       "      <td>28.277143</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.041801</td>\n",
       "      <td>0.068393</td>\n",
       "      <td>0.822944</td>\n",
       "      <td>0.109251</td>\n",
       "      <td>0.014549</td>\n",
       "      <td>0.058177</td>\n",
       "      <td>0.035710</td>\n",
       "      <td>0.066958</td>\n",
       "      <td>0.084854</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>158.508394</td>\n",
       "      <td>26.840242</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.038437</td>\n",
       "      <td>0.060963</td>\n",
       "      <td>0.838129</td>\n",
       "      <td>0.098239</td>\n",
       "      <td>0.014021</td>\n",
       "      <td>0.046781</td>\n",
       "      <td>0.031096</td>\n",
       "      <td>0.059095</td>\n",
       "      <td>0.079272</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>159.288100</td>\n",
       "      <td>26.728020</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037768</td>\n",
       "      <td>0.059423</td>\n",
       "      <td>0.841837</td>\n",
       "      <td>0.096557</td>\n",
       "      <td>0.013188</td>\n",
       "      <td>0.045602</td>\n",
       "      <td>0.030030</td>\n",
       "      <td>0.058118</td>\n",
       "      <td>0.075906</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>159.072762</td>\n",
       "      <td>28.434797</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0.037344</td>\n",
       "      <td>0.059671</td>\n",
       "      <td>0.842345</td>\n",
       "      <td>0.098072</td>\n",
       "      <td>0.012431</td>\n",
       "      <td>0.045787</td>\n",
       "      <td>0.029444</td>\n",
       "      <td>0.057719</td>\n",
       "      <td>0.076169</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>158.250142</td>\n",
       "      <td>27.065008</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036959</td>\n",
       "      <td>0.058352</td>\n",
       "      <td>0.844836</td>\n",
       "      <td>0.095667</td>\n",
       "      <td>0.012542</td>\n",
       "      <td>0.045106</td>\n",
       "      <td>0.028727</td>\n",
       "      <td>0.056887</td>\n",
       "      <td>0.073870</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>159.215915</td>\n",
       "      <td>26.805205</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036706</td>\n",
       "      <td>0.058233</td>\n",
       "      <td>0.845160</td>\n",
       "      <td>0.095445</td>\n",
       "      <td>0.012548</td>\n",
       "      <td>0.045051</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.056732</td>\n",
       "      <td>0.073839</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>157.341968</td>\n",
       "      <td>26.664258</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036681</td>\n",
       "      <td>0.058124</td>\n",
       "      <td>0.845519</td>\n",
       "      <td>0.095233</td>\n",
       "      <td>0.012649</td>\n",
       "      <td>0.044803</td>\n",
       "      <td>0.028464</td>\n",
       "      <td>0.056771</td>\n",
       "      <td>0.073717</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>159.055664</td>\n",
       "      <td>27.114434</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036489</td>\n",
       "      <td>0.058256</td>\n",
       "      <td>0.845202</td>\n",
       "      <td>0.095650</td>\n",
       "      <td>0.012617</td>\n",
       "      <td>0.044774</td>\n",
       "      <td>0.028461</td>\n",
       "      <td>0.056715</td>\n",
       "      <td>0.073924</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>158.027945</td>\n",
       "      <td>27.053142</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036589</td>\n",
       "      <td>0.058325</td>\n",
       "      <td>0.845258</td>\n",
       "      <td>0.095791</td>\n",
       "      <td>0.012605</td>\n",
       "      <td>0.044762</td>\n",
       "      <td>0.028428</td>\n",
       "      <td>0.056738</td>\n",
       "      <td>0.074158</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>156.406418</td>\n",
       "      <td>28.478744</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.036371</td>\n",
       "      <td>0.058204</td>\n",
       "      <td>0.845376</td>\n",
       "      <td>0.095510</td>\n",
       "      <td>0.012744</td>\n",
       "      <td>0.044829</td>\n",
       "      <td>0.028270</td>\n",
       "      <td>0.056709</td>\n",
       "      <td>0.073857</td>\n",
       "      <td>17577</td>\n",
       "      <td>2400</td>\n",
       "      <td>100</td>\n",
       "      <td>157.756094</td>\n",
       "      <td>28.209587</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.0001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  fold  train_loss  val_loss       cor       any  epidural  \\\n",
       "0       1     0    0.268860  0.143166  0.613321  0.163072  0.043452   \n",
       "1       2     0    0.163124  0.621400  0.279981  1.371310  0.041597   \n",
       "2       3     0    0.121807  0.216327  0.506429  0.331250  0.025227   \n",
       "3       4     0    0.055151  0.159198  0.640255  0.259493  0.016848   \n",
       "4       5     0    0.046670  0.132980  0.701180  0.218808  0.015350   \n",
       "5       6     0    0.046544  0.081856  0.793531  0.133274  0.015343   \n",
       "6       7     0    0.042350  0.076822  0.802252  0.121926  0.014430   \n",
       "7       8     0    0.041801  0.068393  0.822944  0.109251  0.014549   \n",
       "8       9     0    0.038437  0.060963  0.838129  0.098239  0.014021   \n",
       "9      10     0    0.037768  0.059423  0.841837  0.096557  0.013188   \n",
       "10     11     0    0.037344  0.059671  0.842345  0.098072  0.012431   \n",
       "11     12     0    0.036959  0.058352  0.844836  0.095667  0.012542   \n",
       "12     13     0    0.036706  0.058233  0.845160  0.095445  0.012548   \n",
       "13     14     0    0.036681  0.058124  0.845519  0.095233  0.012649   \n",
       "14     15     0    0.036489  0.058256  0.845202  0.095650  0.012617   \n",
       "15     16     0    0.036589  0.058325  0.845258  0.095791  0.012605   \n",
       "16     17     0    0.036371  0.058204  0.845376  0.095510  0.012744   \n",
       "\n",
       "    intraparenchymal  intraventricular  subarachnoid  subdural  train_sz  \\\n",
       "0           0.119038          0.088011      0.204062  0.221455     17577   \n",
       "1           0.409983          0.280379      0.337232  0.537986     17577   \n",
       "2           0.253565          0.086670      0.267595  0.218730     17577   \n",
       "3           0.186059          0.071142      0.168241  0.153109     17577   \n",
       "4           0.134875          0.061654      0.129225  0.152139     17577   \n",
       "5           0.065370          0.044277      0.083408  0.098045     17577   \n",
       "6           0.076024          0.038554      0.068125  0.096769     17577   \n",
       "7           0.058177          0.035710      0.066958  0.084854     17577   \n",
       "8           0.046781          0.031096      0.059095  0.079272     17577   \n",
       "9           0.045602          0.030030      0.058118  0.075906     17577   \n",
       "10          0.045787          0.029444      0.057719  0.076169     17577   \n",
       "11          0.045106          0.028727      0.056887  0.073870     17577   \n",
       "12          0.045051          0.028569      0.056732  0.073839     17577   \n",
       "13          0.044803          0.028464      0.056771  0.073717     17577   \n",
       "14          0.044774          0.028461      0.056715  0.073924     17577   \n",
       "15          0.044762          0.028428      0.056738  0.074158     17577   \n",
       "16          0.044829          0.028270      0.056709  0.073857     17577   \n",
       "\n",
       "    val_sz   bs  train_time  valid_time      lr      wd  \n",
       "0     2400  100  366.598575   55.069581  0.1000  0.0001  \n",
       "1     2400  100  157.264272   29.822798  0.1000  0.0001  \n",
       "2     2400  100  156.105009   26.639628  0.1000  0.0001  \n",
       "3     2400  100  158.559280   26.512727  0.0200  0.0001  \n",
       "4     2400  100  159.042082   28.155369  0.0200  0.0001  \n",
       "5     2400  100  160.890759   28.551072  0.0200  0.0001  \n",
       "6     2400  100  157.371435   28.277143  0.0200  0.0001  \n",
       "7     2400  100  158.508394   26.840242  0.0200  0.0001  \n",
       "8     2400  100  159.288100   26.728020  0.0020  0.0001  \n",
       "9     2400  100  159.072762   28.434797  0.0020  0.0001  \n",
       "10    2400  100  158.250142   27.065008  0.0020  0.0001  \n",
       "11    2400  100  159.215915   26.805205  0.0002  0.0001  \n",
       "12    2400  100  157.341968   26.664258  0.0002  0.0001  \n",
       "13    2400  100  159.055664   27.114434  0.0002  0.0001  \n",
       "14    2400  100  158.027945   27.053142  0.0002  0.0001  \n",
       "15    2400  100  156.406418   28.478744  0.0002  0.0001  \n",
       "16    2400  100  157.756094   28.209587  0.0002  0.0001  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(PATH_WORK/'stats.f{}.v{}'.format(0,VERSION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08180776, 0.00407849, 0.02802897, 0.02118001, 0.0283712 ,\n",
       "       0.03698493], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.mean((0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f268e57fa90>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD4CAYAAADlwTGnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3dd3iUdbr/8fedTkJIgdBSSAIBiZQgoSlFxQIWwBULFnRFse+6u2fP6p5tx63uHo+6R11FsRdEXYW1YcO2IhCkhhoSIIUSSAKBJKTM/fsjw/6yMcAkJHmm3K/rmiszT8vne2Uy9zzfp3xFVTHGGBN4gpwOYIwxxhlWAIwxJkBZATDGmABlBcAYYwKUFQBjjAlQIU4HaI0ePXpoamqq0zGMMcanrFq1ar+qJjSf7lEBEJEpwCNAMPC0qv6p2fyJwMPAMOBqVX3DPf0c4KEmi57mnv+2iDwHTAIOuufdqKprTpQjNTWVnJwcTyIbY4xxE5GdLU0/aQEQkWDgMeB8oAhYKSKLVXVjk8V2ATcC/9F0XVVdCmS5txMP5AEfNlnkp8eKhTHGmM7lyR7AaCBPVfMBRGQBMB34VwFQ1R3uea4TbGcm8L6qVrU5rTHGmHbjyUHgRKCwyesi97TWuhp4tdm034vIOhF5SETC27BNY4wxbeRJAZAWprXq/hEi0gcYCixpMvk+Go8JjALigZ8dZ925IpIjIjmlpaWt+bXGGGNOwJMCUAQkN3mdBJS08vdcCbylqnXHJqjqbm10FHiWxq6m71DVeaqararZCQnfOYhtjDGmjTwpACuBDBFJE5EwGrtyFrfy98yiWfePe68AERFgBrChlds0xhhzCk5aAFS1HriLxu6bTcBCVc0VkftFZBqAiIwSkSLgCuBJEck9tr6IpNK4B/F5s02/LCLrgfVAD+B3p94cY4wxnhJfuh10dna22nUApjPtO1TD6sIKtuypRICwkCDCQ4IICwkmPCSI3jERpMRH0icmgpBgu7DeeCcRWaWq2c2n+9SVwMZ0tAOHj/LW6mJW76pgTWEFxRXVHq0XEiQkxnUhJT6S0anxTMvqS7/uUR2c1phTY3sAxgANLuWV5Tv5y5ItHKqpJzG2C1kpsYxIjmVEShyn9+1GkAi1DS7q6l3UNriorm2g5GA1hWVV7CqrYldZNfmlh8ktOQTA8KQYLh3el0uH96VXtwiHW2gC2fH2AKwAmID37a5yfrVoAxuKD3Fm/+78ZtrpDOwV3ebtFVdU887aEhavLSG35BAikNo9irjIUOIiw4iLCiMuMpT+CV25aFgfukWEtmNrjPkuKwDGNFN+pJYHPtjMgpWF9OoWzi8vyeTioX1oPDGtfWwvPcw/1paQt+8w5VW1lB+pa/xZVUtNnYuI0CCmDunDFdlJjE3rTlBQ+/1uY46xAmBME6t2lnHny6vZf/goN41P4weTM+ga3nmHxFSV9cUHWZhTyKI1JVTW1JMc34VrRvdjzvg0wkLsgLJpP1YAjKHxg3f+VwX86f3N9I3twuPXnsGQxBhHM9XUNbAkdw+vrSzk6+0HGJYUw1+vHkFqDzuIbNqHFQAT8A5W1/Gfb6xlSe5eLsjsxV+uGE5MF+/qf/9gw25+9uZ66htc3D99CN87I7Fdu6RMYLLTQE1A21hyiNtfXkVxeTW/uHgwc8aneeUH65QhfRiWFMs9r63hJ6+v5Yttpfx2xhA7UGw6hHU0Gr+3vuggV81bxtE6FwvmjuXmCele+eF/TN/YLrx6y1h+cv5A3lm3m4se+ZIPNuzBl/bWjW+wAmD82saSQ1z/zHJiuoTy5h1nkp0a73QkjwQHCXdPzmDhrWOJCA3mtpdWccUTy1i1s9zpaMaPWAEwfmvb3kqum7+cLqHBvHrLWBJjuzgdqdVG9ovngx9O4A+XDWXHgSou/9vX3PHyKgr2H3E6mvEDdhDY+KX80sNcNe8bABbeOo40Pzij5sjRep76Mp95X+RTW+9i9rhUfjg5g5hIOz5gTux4B4FtD8D4nZ0HjnDNU8txuZRXbh7jFx/+AFHhIdxz3kA+++nZXJGdxLNfF3D2/yzlxWU7qG840WisxrTMCoDxK5U1dVw/fwU19Q28dPMYMk7hlg7eqmd0BH/83jDevXsCg3pH88tFuUx95Eu+2Goj5pnWsQJg/MqvFuVSXFHN07OzGdynm9NxOlRm3268estYnrx+JLUNLmY/s4K/LNnsdCzjQ6wAGL+xaE0xb60u5u5zB/jM2T6nSkS48PTefPijiVw9KpnHlm5nYU6h07GMj7ALwYxfKCyr4hdvbSC7Xxx3nTPA6TidLjwkmN/NGEJxRTU///t6kuMiGde/u9OxjJezPQDj8+obXNzz2hoAHroqK2BH5goJDuLRa84gtUcUt71kp4qakwvM/xTjVx5dmseqneX87rIhJMdHOh3HUTFdQnnmhlEEBwk3PbeSiqpapyMZL2YFwPi0VTvL+Osn25iR1ZfpWYlOx/EKKd0jmXf9SIrLq7n9pW+prbdTRE3LrAAYn1VVW889r62hb2wX7p8xxOk4XiU7NZ4HZg5lWf4B/vj+JqfjGC/lUQEQkSkiskVE8kTk3hbmTxSRb0WkXkRmNpvXICJr3I/FTaanichyEdkmIq+JSNipN8cEkqe+KKCwrJr/uWK43S2zBZeNSOK6sSk8//UOtu6tdDqO8UInLQAiEgw8BkwFMoFZIpLZbLFdwI3AKy1solpVs9yPaU2mPwA8pKoZQDkwpw35TYDac7CGJz7fzkVDezM23c52OZ4fnz+IqPAQ/vCe7QWY7/JkD2A0kKeq+apaCywApjddQFV3qOo6wKPORmm8F++5wBvuSc8DMzxObQLe/3y4hQaXcu+UwU5H8WrxUWH84NwMPttSyud2pbBpxpMCkAg0vbKkyD3NUxEikiMi34jIsQ/57kCFqtafbJsiMte9fk5pqb2BDWwoPsib3xbx/bNSSeke2Gf9eGL2mf3o1z2S37+70e4ZZP6NJwWgpZEzWnML0RT3XeiuAR4Wkf6t2aaqzlPVbFXNTkhIaMWvNf5IVbn/nY3ERYZx57mBd8FXW4SHBHPf1NPYuvcwr9lVwqYJTwpAEZDc5HUSUOLpL1DVEvfPfOAzYASwH4gVkWNXIrdqmyZwLcndy4qCMn50/kA78NsKF57em9Gp8fzvh1uprKlzOo7xEp4UgJVAhvusnTDgamDxSdYBQETiRCTc/bwHcBawURsHIVgKHDtj6AZgUWvDm8BytL6BP76/iYyeXZk1KvnkK5h/ERF+cclgDhyp5fHPtjsdx3iJkxYAdz/9XcASYBOwUFVzReR+EZkGICKjRKQIuAJ4UkRy3asPBnJEZC2NH/h/UtWN7nk/A34sInk0HhOY354NM/7nxWU72Xmgiv+6eHDA3u7hVAxLiuV7IxKZ/1UBhWVVTscxXsBGBDM+ofxILRP/spQzUuJ4/qbRTsfxWbsPVnPO/3zG+Zm9+b9ZI5yOYzqJjQhmfNoTX2zn8NF6fn6RnfZ5KvrEdGHO+DTeWVdiF4cZKwDG++2rrOH5r3cwIyuRQb39b4Svznbz+HS6hAbz6Kd5TkcxDrMCYLze40u3U9eg/HByhtNR/EJcVBjXj+vHP9aVsL30sNNxjIOsABivVlJRzSvLdzHzjCRS/WRwd29wy4R0wkOCeGyp7QUEMisAxqv936d5KMrdk+2ir/bUo2s4143px6I1JeywgWMClhUA47V2Haji9ZxCZo1OISnObvnQ3uZOTCckSHj8M9sLCFRWAIzXeuSTbQQHCXcG4Bi/naFntwhmjU7h798W23UBAcoKgPFKefsO89bqImaP60evbhFOx/Fbt05KJ0iEv31uVwcHIisAxis9/PFWIkKDuW1Sf6ej+LU+MV24clQSr+cUUlJR7XQc08msABivs2n3Id5Zt5ubzkqje9dwp+P4vdsm9UcVnrC9gIBjBcB4nQc/3EJ0RAi3TEh3OkpASIqLZObIJBasLGRfZY3TcUwnsgJgvMqqneV8vGkft03qT0yk3e65s9w6qT91DS6e++cOp6OYTmQFwHgNVeUvSzbTo2sY3z8r1ek4ASWtRxRTh/TmxW922ngBAcQKgPEaX+Xt55v8Mu46ZwCRYSEnX8G0q9sm9aeypp5XV+xyOorpJFYAjFdo/Pa/hcTYLswak+J0nIA0LCmWM/t3Z/5XBRytb3A6jukEVgCMV1iSu5d1RQe557wMwkOCnY4TsG6b1J+9h46yaLWN0BoIrAAYxzW4lAc/3EL/hCguG5HodJyANiGjB5l9uvHEF9txuXxnsCjTNlYAjOPeXl3Mtn2H+ckFg2yoR4eJCLdOSie/9AgfbdrrdBzTwey/zTiqtt7FQx9vZWhiDFOH9HY6jgEuHtqH5PguPPH5dnxpyFjTelYAjKNeyymkqLya/7hwECLidBwDhAQHccuEdFbvqmBFQZnTcUwH8qgAiMgUEdkiInkicm8L8yeKyLciUi8iM5tMzxKRZSKSKyLrROSqJvOeE5ECEVnjfmS1T5OMr2hwKU99kc+IlFgmZvRwOo5p4oqRycRHhdntIfzcSQuAiAQDjwFTgUxglohkNltsF3Aj8Eqz6VXAbFU9HZgCPCwisU3m/1RVs9yPNW1sg/FRH23cy66yKuZOSLdv/16mS1gwN56ZytItpWzafcjpOKaDeLIHMBrIU9V8Va0FFgDTmy6gqjtUdR3gajZ9q6pucz8vAfYBCe2S3Pi8+V/lkxzfhQtOt75/b3TDuFS6hofwqA0b6bc8KQCJQGGT10Xuaa0iIqOBMKDpPuXv3V1DD4lIi7d9FJG5IpIjIjmlpaWt/bXGS60prGDljnK+f2YawUH27d8bxUSGMntcP95bv5u8fTZ4vD/ypAC09N/ZqlMDRKQP8CLwfVU9tpdwH3AaMAqIB37W0rqqOk9Vs1U1OyHBdh78xfyvCoiOCOHKUclORzEnMGd8GuEhQTZspJ/ypAAUAU3/S5MAjy8TFJFuwLvAL1T1m2PTVXW3NjoKPEtjV5MJAMUV1by3fjfXjE6ha7jd88ebde8azrXuweN3HbBhI/2NJwVgJZAhImkiEgZcDSz2ZOPu5d8CXlDV15vN6+P+KcAMYENrghvf9dw/CwC44cxUZ4MYj8ydmE5wkPC3z20vwN+ctACoaj1wF7AE2AQsVNVcEblfRKYBiMgoESkCrgCeFJFc9+pXAhOBG1s43fNlEVkPrAd6AL9r15YZr1RZU8eCFYVcPLQPfWO7OB3HeKBXtwiuyk7mjVVFNmykn/Fo/1tV3wPeazbtV02er6Sxa6j5ei8BLx1nm+e2KqnxCwtziqg8Ws/NE9KcjmJa4dZJ6by6YhfzvsjnN9NOdzqOaSd2JbDpNPUNLp75qoDRafEMS4o9+QrGayTFRXL5GUm8umKXDRvpR6wAmE6zJHcvxRXV3Dzevv37otvPbhw28ukvC5yOYtqJFQDTKVSVv32eR2r3SCYP7uV0HNMGqT2imJ6VyEvf7KTsSK3TcUw7sAJgOsVHG/eyofgQd52bYRd++bA7zu5PVW0DL32z0+koph1YATAdzuVSHvp4G2k9opiR1dfpOOYUZPSK5pxBCbywbKcNG+kHrACYDvfhxj1s2n2IH0weYAO++IE549PZf/goi9fYsJG+zv4bTYdyuZSHPtpGekIU04bbcI/+4KwB3TmtdzTzvyqwAWN8nBUA06He27CbLXsr+eFk6/v3FyLCTWelsXlPJcu2H3A6jjkFVgBMh2lwKQ9/vI2Mnl25ZJj1/fuTaVl96dE1jKe/slNCfZkVANNh3llXQt6+w9xz3kD79u9nIkKDuW5sPz7dvI/tpXaraF9lBcB0iPoGF498vI3TekfbYO9+6rqx/QgLCeLZf9pegK+yAmA6xOK1JeTvP8I952UQZN/+/VKPruHMyOrLG6uKKLcLw3ySFQDT7hpcyl8/2cbgPt24INO+/fuzOePTqalz8cqKXU5HMW1gBcC0u3fX72bHgSp+cO4A+/bv5wb1jmZCRg9eWLaD2nrXSZc33sUKgGlXLpfy+NI8BvTsyoU22HtAuGl8GnsPHeXd9XZhmK+xAmDa1Seb97F5TyV3nN3fvv0HiEkZCaT3iOLFZXZ/IF9jBcC0G1Xl0aV5JMV14dLhdt5/oAgKEq4Zk8K3uyrYWHLI6TimFawAmHbz9fYDrC2s4LZJ/Qm1e/4ElJkjkwgPCeKl5bYX4Evsv9S0m0c/zaNndDgzR35ndFDj52Ijw7h0eF/eXl1MZU2d03GMh6wAmHaxamc5y/IPMHdiOhGhwU7HMQ64bmw/qmobeHt1sdNRjIc8KgAiMkVEtohInojc28L8iSLyrYjUi8jMZvNuEJFt7scNTaaPFJH17m3+VUTsiKEPe3xpHrGRocwaneJ0FOOQ4UkxDEnsxkvf7LK7hPqIkxYAEQkGHgOmApnALBHJbLbYLuBG4JVm68YDvwbGAKOBX4tInHv234C5QIb7MaXNrTCO2lhyiE827+Oms9KICg9xOo5xiIhw3Zh+bNlbSc7OcqfjGA94sgcwGshT1XxVrQUWANObLqCqO1R1HdD8SpALgY9UtUxVy4GPgCki0gfopqrLtPGrwgvAjFNtjHHG45/l0TU8hBvGpTodxThsWlZfoiNCbMhIH+FJAUgECpu8LnJP88Tx1k10Pz/pNkVkrojkiEhOaWmph7/WdJbtpYd5d/1urhvbj5jIUKfjGIdFhoVw+RlJvL9+DwcOH3U6jjkJTwpAS33znnbwHW9dj7epqvNUNVtVsxMSEjz8taazPPZpHuEhQdw8Ic3pKMZLXDc2hdoGFwtzik6+sHGUJwWgCEhu8joJ8PSa7+OtW+R+3pZtGi+x88ARFq0t4box/ejRNdzpOMZLDOgZzdj0eF5ZsROXyw4GezNPCsBKIENE0kQkDLgaWOzh9pcAF4hInPvg7wXAElXdDVSKyFj32T+zgUVtyG8c9PjS7QQHCXMnpjsdxXiZ68b2o7Csms+3WbetNztpAVDVeuAuGj/MNwELVTVXRO4XkWkAIjJKRIqAK4AnRSTXvW4Z8Fsai8hK4H73NIDbgaeBPGA78H67tsx0qKLyKt78tohZo5Lp2S3C6TjGy1yQ2ZuE6HBesvsDeTWPztlT1feA95pN+1WT5yv59y6dpss9AzzTwvQcYEhrwhrv8bfPthMkwm1n93c6ivFCYSFBzBqVzP8tzaOwrIrk+EinI5kW2JXAptX2HKzh9ZwiZmYn0Semi9NxjJe6Zkw/gkTslFAvZgXAtNoTn2/Hpcrtk+zbvzm+3jERXHh6L17LKaSmrsHpOKYFVgBMq+yrrOHVFbv43hmJtltvTmr2uFQqqupYvNZO8vNGVgBMqzz1RT51DS7uOHuA01GMDxiTFs/AXl15YdkOuz+QF7ICYDxWfqSWl77ZxYysRFJ7RDkdx/gAEWH2uFQ2FB9idWGF03FMM1YAjMdeX1VIdV0Dt1rfv2mFy0YkEh0ewgtf73A6imnGCoDxiMulvLJ8F6NT4xnUO9rpOMaHRIWHcPnIJN5bv4fSSrs/kDexAmA88vX2A+w4UMW1Y+1+/6b1rhvbj9oGF6+t3OV0FNOEFQDjkZeX7yQ+KowpQ3o7HcX4oAE9uzJ+QA9eXr6L+obmd403TrECYE5q76EaPty4lytGJhEeYsM9mraZPa4fuw/W8PGmvU5HMW5WAMxJLVxZSINLbbhHc0omD+5FYmwXnv/argz2FlYAzAk1uJRXV+xiQkYPO/XTnJLgIOGaMSksyz9Afulhp+MYrACYk/hsyz5KDtZw7Rj79m9O3RXZSYQECa+usIPB3sAKgDmhl5fvIiE6nMmDezkdxfiBntERXHB6L15fVWT3B/ICVgDMcRWVV7F0yz6uHpVMaLC9VUz7uHZMPyqq6vhgwx6nowQ8+682x7VgRSECXG0Hf007GpfendTukby83A4GO80KgGlRXYOLBSsLOWdQTxJj7Z7/pv0EuQ8Gr9xRzta9lU7HCWhWAEyLluTuYf/ho1xjB39NB5g5Mpmw4CBeWW4Hg51kBcB8h6ry1JcFpHaP5OxBPZ2OY/xQfFQYU4f25s1vi6iutYPBTrECYL4jZ2c5awsrmDMhneAgcTqO8VPXjE6hsqaef6yzwWKc4lEBEJEpIrJFRPJE5N4W5oeLyGvu+ctFJNU9/VoRWdPk4RKRLPe8z9zbPDbPvmp6iXlf5BMXGcrMM5KcjmL82Oi0eAb07MrL1g3kmJMWABEJBh4DpgKZwCwRyWy22BygXFUHAA8BDwCo6suqmqWqWcD1wA5VXdNkvWuPzVfVfe3QHnOKCvYf4eNNe7lubD+6hNl9f0zHERGuGZ3C2sIKNhQfdDpOQPJkD2A0kKeq+apaCywApjdbZjrwvPv5G8BkEWnedzALePVUwpqON/+rfEKDgpg9LtXpKCYAXH5GEuEhQbxiVwY7wpMCkAgUNnld5J7W4jKqWg8cBLo3W+YqvlsAnnV3//yyhYJhOlnZkVpezynishGJJESHOx3HBICYyFAuHd6XRauLqaypczpOwPGkALT0wdx8dOcTLiMiY4AqVd3QZP61qjoUmOB+XN/iLxeZKyI5IpJTWlrqQVzTVi99s5Oj9S5unpDmdBQTQK4f248jtQ38/dtip6MEHE8KQBGQ3OR1EtD8sP2/lhGRECAGKGsy/2qafftX1WL3z0rgFRq7mr5DVeeparaqZickJHgQ17RFTV0DLyzbwTmDEsjoZUM+ms4zPDmW4UkxvLBsB6rNv1uajuRJAVgJZIhImoiE0fhhvrjZMouBG9zPZwKfqvsvKSJBwBU0HjvAPS1ERHq4n4cClwAbMI55e3Ux+w/XcsuEdKejmAA0e1wq20uP8PX2A05HCSgnLQDuPv27gCXAJmChquaKyP0iMs292Hygu4jkAT8Gmp4qOhEoUtX8JtPCgSUisg5YAxQDT51ya0ybuFzK018VkNmnG+P6Nz90Y0zHu3hYH+Kjwnhh2Q6nowSUEE8WUtX3gPeaTftVk+c1NH7Lb2ndz4CxzaYdAUa2MqvpIJ9t3UfevsM8fFUWdizeOCEiNJirRiXz5OfbKa6otvtPdRK7Etjw5Of59ImJ4OJhfZyOYgLYsUGHXrG7hHYaKwABbm1hBcsLypgzPs3u+W8clRQXyeTBvXh1RaENFtNJ7D8+wM37Ip/oiBC757/xCrPH9aPsSC3vrd/tdJSAYAUggO08cIT3N+zm2jH96Bru0eEgYzrUWf17kJ4QxQvLrBuoM1gBCGBPf1lASFAQ3z8r1ekoxgCNg8VcP7YfaworWFdU4XQcv2cFIEAdOHyUhTmFzBjRl17dIpyOY8y/XD4yiciwYNsL6ARWAALUC8sab/swd6Jd+GW8S7eIUC4bkcjitSUcOHzU6Th+zQpAAKqubbztw3mDezKgp932wXif75+VSm29i+dtL6BDWQEIQK+vKqS8qo5bJ/V3OooxLRrQM5rzBvfixWU7qKqtdzqO37ICEGDqG1w8/WUBI1Jiye4X53QcY47rtknplFfV8XpOkdNR/JYVgADzQe4edpVVcevEdLvtg/Fq2anxnJESy1Nf5lPf4HI6jl+yAhBAVJWnviwgtXsk52f2djqOMSd166T+FJVX8/6GPU5H8UtWAALIt7vKWVtYwU3j0wgOsm//xvudP7gX6T2iePKL7TZWQAewAhBA5n9VQLeIEC4/I8npKMZ4JChImDsxnQ3Fh2ysgA5gBSBAFJZV8cGGPcwak0KU3fbB+JAZIxLp0TWcJz7f7nQUv2MFIEA8//UORIQbxqU6HcWYVokIDeb7Z6Xy5bb9bCw55HQcv2IFIAAcPlrPaysLuWhoH/raQBvGB103ph9RYcHM+8L2AtqTFYAAsHBlIZVH65kzPs3pKMa0SUxkKLNGp/CPdbspLKtyOo7fsALg5xpcyrNfFzCyXxxZybFOxzGmzeZMSCNI4EnbC2g3VgD83Ecb91JYVm3f/o3P6xPThSuyk1m4sojdB6udjuMXrAD4uWe+KiAxtgsXZPZyOooxp+z2Sf1xqfLEZ7YX0B48KgAiMkVEtohInojc28L8cBF5zT1/uYikuqeniki1iKxxP55oss5IEVnvXuevYvclaHfriw6yYkcZN56ZSoiN92v8QHJ8JDNHJvHqykL2HqpxOo7PO+mngogEA48BU4FMYJaIZDZbbA5QrqoDgIeAB5rM266qWe7HbU2m/w2YC2S4H1Pa3gzTkvlf5RMVFsxVo5OdjmJMu7nj7AE0uJQnP893OorP8+Rr4WggT1XzVbUWWABMb7bMdOB59/M3gMkn+kYvIn2Abqq6TBuv734BmNHq9Oa4SiqqeWfdbq4clUy3iFCn4xjTblK6R3LZiEReXr6TfZW2F3AqPCkAiUBhk9dF7mktLqOq9cBBoLt7XpqIrBaRz0VkQpPlm97jtaVtAiAic0UkR0RySktLPYhrAJ79ZwEK3HSWHfw1/ufOcwZQ5761uWk7TwpAS9/km9+V6XjL7AZSVHUE8GPgFRHp5uE2GyeqzlPVbFXNTkhI8CCuOVRTx6srGi/8So6PdDqOMe0urUcU07MSeXHZThs28hR4UgCKgKadyElAyfGWEZEQIAYoU9WjqnoAQFVXAduBge7lm96RrKVtmjZ6dfkuDh+t51Yb79f4sTvPGUBNfQNPf2V7AW3lSQFYCWSISJqIhAFXA4ubLbMYuMH9fCbwqaqqiCS4DyIjIuk0HuzNV9XdQKWIjHUfK5gNLGqH9gS82noXz/5zB2f2786QxBin4xjTYQb07Molw/rywtc7KD9S63Qcn3TSAuDu078LWAJsAhaqaq6I3C8i09yLzQe6i0gejV09x04VnQisE5G1NB4cvk1Vy9zzbgeeBvJo3DN4v53aFNAWry1hz6Ea5tq3fxMA7j53AFV1DTz+WZ7TUXyS+NIgC9nZ2ZqTk+N0DK+lqkx5+EsAPrhngg35aALCf76xlrdWF/PRjyaR2iPK6TheSURWqWp28+l2dZAf+XxrKVv2VnKLjfdrAsh/XDiIsOAgfv/eJqej+BwrAH5k3hf59O4WwbThfZ2OYkyn6RkdwZ3nDuCjjXv5Z95+p+P4FF9aVfEAABBASURBVCsAfmJD8UG+3n6A75+VSliI/VlNYLnprDSS4rrw23c20uDynW5tp9knhZ+Y90U+XcNDmDUmxekoxnS6iNBgfn7RYDbvqeS1lYUnX8EAVgD8Qm7JQd5ZV8K1Y1Lstg8mYE0d0pvRqfE8+OEWDtXUOR3HJ1gB8HEul/KrRbnERYZxxzkDnI5jjGNEhF9ekklZVS2PfmqnhXrCCoCPe2t1Mat2lvOzKacR08W+/ZvANjQphplnJPHsPwvYsf+I03G8nhUAH3aopo4/vr+ZrORYZo5MOvkKxgSAn7pPC/3duxudjuL1rAD4sIc/2saBI0e5f/rpBAXZef/GAPTsFsHdkzP4eNM+lm7e53Qcr2YFwEdt3nOI55ftYNboFIYl2WDvxjR101lppPeI4r//kcvR+gan43gtKwA+SFX59aJcoiNC+OkFg5yOY4zXCQsJ4jfTTmfHgSobM+AErAD4oMVrS1heUMZPLxxEXFSY03GM8UoTByZw4em9ePTTPEoqqp2O45WsAPiYypo6/vDeJoYkduPqUXbRlzEn8ouLM3Gp8ge7T1CLrAD4mN+/u4nSyqP8bsZQgu3ArzEnlBwfyR1nD+Cddbv5ervdJ6g5KwA+ZOmWfSxYWcjcif3JSrYDv8Z44tZJ6STFdeE3i3Opa3A5HcerWAHwEQer6rj3zXUM7NWVH52f4XQcY3xGRGgwv7okk617D/P81zucjuNVrAD4iPvf2cj+w7U8eEUW4SHBTscxxqecn9mLSQMTeOTjbZRW2iDyx1gB8AEfbdzLm98WcefZ/RmaZOP8GtNaIsKvL82kpr6BP3+w2ek4XsMKgJcrP1LLz99az+A+3bjrXOv6Maat0hO6ctP4NF5fVcTqXeVOx/EKVgC83K8X51J+pJYHrxhuA70Yc4ruPjeDntHh/GZxLi4bOMazAiAiU0Rki4jkici9LcwPF5HX3POXi0iqe/r5IrJKRNa7f57bZJ3P3Ntc4370bK9G+YsPNuxh8doSfjA5g8y+3ZyOY4zP6xoewn0XncbaooO8vsoGjjlpARCRYOAxYCqQCcwSkcxmi80BylV1APAQ8IB7+n7gUlUdCtwAvNhsvWtVNcv9sLs2NVF+pJZfvL2B0/t24/az+zsdxxi/MSMrkex+cfz5gy0crA7sgWM82QMYDeSpar6q1gILgOnNlpkOPO9+/gYwWUREVVeraol7ei4QISLh7RHc393/zkYqqmr5y8zhhAZb148x7UVE+M200ymrquWhj7Y6HcdRnnyyJAJN95WK3NNaXEZV64GDQPdmy1wOrFbVpudgPevu/vmliLR4WauIzBWRHBHJKS0t9SCu7/tk017eWl3MHecMsK4fYzrAkMQYrhmdwovf7GTLnkqn4zjGkwLQ0gdz86MnJ1xGRE6nsVvo1ibzr3V3DU1wP65v6Zer6jxVzVbV7ISEBA/i+raD1XX8/K31nNY7mrtsiEdjOsx/XDCI6IgQfrVoQ8AeEPakABQByU1eJwElx1tGREKAGKDM/ToJeAuYrarbj62gqsXun5XAKzR2NQW837/beMHXX2baWT/GdKS4qDDum3oaywvKeDZArxD25BNmJZAhImkiEgZcDSxutsxiGg/yAswEPlVVFZFY4F3gPlX957GFRSRERHq4n4cClwAbTq0pvu/zraUszCli7sR0u+DLmE5wZXYy5w3uxQMfbA7IrqCTFgB3n/5dwBJgE7BQVXNF5H4RmeZebD7QXUTygB8Dx04VvQsYAPyy2eme4cASEVkHrAGKgafas2G+prKmjvveXMeAnl354WS74MuYziAi/OnyoXSLCOGHC1YH3Ohhouo7fV/Z2dmak5PjdIx2p6r86LU1LFpbwpu3n8kZKXFORzImoHyyaS9zns/h1onp3HfRYKfjtDsRWaWq2c2nWyezF3h5+S7eXlPCj84baB/+xjhg8uBeXDMmhXlf5vNN/gGn43QaKwAOW1dUwf3/2MjZgxLsrB9jHPSLiweT2j2KnyxcGzAXiFkBcFD5kVpuf+lbEqLDeejKLIJshC9jHBMZFsJDV2Wx51ANv160AV/qHm8rKwAOcbmUHy1cQ2nlUR6/9gwb3N0YL5CVHMsPzs3g7TUlPBcAp4ZaAXDIY0vz+GxLKb+8NJPhNryjMV7j7nMHcH5mL377zkaWbvHvW5RZAXDAl9tK+d+PtzI9qy/XjUlxOo4xpomgIOHhq7IY1Lsbd7+ymq17/ff6ACsAnSy35CB3vPQtA3tG84fLhnKcWyAZYxwUFR7C/Buy6RIWzJznV3LgsH8OI2kFoBMVllVx47MriY4I4bmbRhEVHuJ0JGPMcfSN7cLTs7PZd+got764yi8vErMC0En2Hz7K9fOXU1vv4oU5o+kT08XpSMaYkxieHMuDVw4nZ2c59/19vd+dGWRfQTvB4aP13PTcSvYcquHlm8cyoGe005GMMR66ZFhftu87wkMfb6VbRCi/vjTTb7purQB0sNp6F7e/tIrckkPMu34kI/vZlb7G+JofTB7AoZo65n9VQL3Lxf3ThvjFdTtWADpQaeVRfvbmOr7ctp8/zxzG5MG9nI5kjGkDEeEXFw8mJFh48vN8GlzK72cM9fkiYAWgA6gqb68p5r//sZGq2gZ+O/10rsxOPvmKxhivJSLcO+U0QoOCeHRpHvUNyp8uH0awDxcBKwDtbPfBav7rrQ18unkfI/vF8cDlwxjQs6vTsYwx7UBE+MkFAwkJFh7+eBsNLuXPM4cR4qPjdlsBaCeVNXW8uaqIBz/cSr1L+fWlmcwel+rT3w6MMd8lItxz3kCCRXjwo60UV1TzyNUj6B0T4XS0VrMCcApcLmVZ/gFezynkg9w91NS5OLN/d/70vWGkdI90Op4xpgPdPTmDvrFd+OWiDUx95AsevHI4557mW8f5rAC0kqqyeU8l767bzVuriymuqKZbRAgzRyYxc2Qyw5Ni/OYUMWPMiV0+MomslFjuemU1Nz2Xw5zxafxsymk+M563FQAPqCrriw/y3vo9fLBhNzsOVBEkMCEjgXunnsb5mb2ICA12OqYxxgH9E7ry1h1n8sf3NjH/qwJWFJTxx+8NZUii94/rbUNCtqCuwcWm3YdYvauC1bvKWVFQRsnBGoKDhDP7d2fqkD5ccHovenQN7/AsxhjfsSR3D//5xjoOVtcxLr07t0xM4+yBPR0/XfR4Q0IGdAFQVfZVHiVv32G27a0kr/Qwm3dXsr74IEfrXQAkRIczMiWOyYN7cn5mL2Ij7b79xpjjO1hdx4IVu3ju6x3sPlhDekIUc8ancenwvnSLCHUk0ykVABGZAjwCBANPq+qfms0PB14ARgIHgKtUdYd73n3AHKAB+IGqLvFkmy1pawHILz1MfukRisqrKCqvbnxUVLHzQBWVNfX/Wi46IoSBvaLJSo5lREosI1Li6BsTYX36xphWq2tw8d763Tz9ZQHriw8CkBTXhdN6R3Na726c1iealPhIosJDiAoLITI8mMjQ4A45pbTNBUBEgoGtwPlAEbASmKWqG5sscwcwTFVvE5GrgctU9SoRyQReBUYDfYGPgYHu1U64zZa0tQDMfmYFX2wtBSAiNIikuEiS4rqQHBfJgJ5dGdCzKxk9u5IQHW4f9saYdqWqrNpZzvKCMjbtPsSWPZXk7z9Cg6vlz96wkCDCg4MIDQkiLDiIsJDGx/wbsunXPapNGY5XADw5CDwayFPVfPeGFgDTgaYf1tOB37ifvwE8Ko2fpNOBBap6FCgQkTz39vBgm+3mpxcM4sfnDyQprgvdo8LsQ94Y02lEhOzUeLJT4/81raaugbx9h9l9sIaq2nqqahs4crSeI0cbqKqrp7be9f8fDY0/O+JEE08KQCJQ2OR1ETDmeMuoar2IHAS6u6d/02zdRPfzk20TABGZC8wFSElp2+hZQ5O8/2i8MSZwRIQGMyQxxvEzhTzpbGrp63LzfZfjLdPa6d+dqDpPVbNVNTshIeGEQY0xxnjOkwJQBDS9k1kSUHK8ZUQkBIgByk6wrifbNMYY04E8KQArgQwRSRORMOBqYHGzZRYDN7ifzwQ+1cajy4uBq0UkXETSgAxghYfbNMYY04FOegzA3ad/F7CExlM2n1HVXBG5H8hR1cXAfOBF90HeMho/0HEvt5DGg7v1wJ2q2gDQ0jbbv3nGGGOOJ6AvBDPGmEBwvNNAfeOORcYYY9qdFQBjjAlQVgCMMSZA+dQxABEpBXa2cfUewP52jOM0f2qPP7UF/Ks9/tQWCNz29FPV71xI5VMF4FSISE5LB0F8lT+1x5/aAv7VHn9qC1h7mrMuIGOMCVBWAIwxJkAFUgGY53SAduZP7fGntoB/tcef2gLWnn8TMMcAjDHG/LtA2gMwxhjThBUAY4wJUAFRAERkiohsEZE8EbnX6TytISLPiMg+EdnQZFq8iHwkItvcP+OczNgaIpIsIktFZJOI5IrID93Tfa5NIhIhIitEZK27Lf/tnp4mIsvdbXnNfcdbnyEiwSKyWkTecb/2yfaIyA4RWS8ia0Qkxz3N595nx4hIrIi8ISKb3f8/4061PX5fANxjGj8GTAUygVnusYp9xXPAlGbT7gU+UdUM4BP3a19RD/xEVQcDY4E73X8PX2zTUeBcVR0OZAFTRGQs8ADwkLst5cAcBzO2xQ+BTU1e+3J7zlHVrCbnyvvi++yYR4APVPU0YDiNf6NTa4+q+vUDGAcsafL6PuA+p3O1sg2pwIYmr7cAfdzP+wBbnM54Cm1bBJzv620CIoFvaRzadD8Q4p7+b+8/b3/QODjTJ8C5wDs0jt7nk+0BdgA9mk3zyfcZ0A0owH3iTnu1x+/3AGh5TOPE4yzrK3qp6m4A98+eDudpExFJBUYAy/HRNrm7S9YA+4CPgO1AharWuxfxtffbw8B/Ai736+74bnsU+FBEVrnHFgcffZ8B6UAp8Ky7e+5pEYniFNsTCAXA4/GHTecRka7Am8A9qnrI6TxtpaoNqppF4zfn0cDglhbr3FRtIyKXAPtUdVXTyS0s6hPtAc5S1TNo7P69U0QmOh3oFIQAZwB/U9URwBHaofsqEAqAP44/vFdE+gC4f+5zOE+riEgojR/+L6vq392TfbpNqloBfEbjcY1Y99jY4Fvvt7OAaSKyA1hAYzfQw/hoe1S1xP1zH/AWjQXaV99nRUCRqi53v36DxoJwSu0JhALgj+MPNx2D+QYa+9F9gogIjUOIblLV/20yy+faJCIJIhLrft4FOI/GA3NLaRwbG3ykLQCqep+qJqlqKo3/J5+q6rX4YHtEJEpEoo89By4ANuCD7zMAVd0DFIrIIPekyTQOtXtq7XH64EYnHUC5CNhKY//sfzmdp5XZXwV2A3U0fguYQ2O/7CfANvfPeKdztqI942nsQlgHrHE/LvLFNgHDgNXutmwAfuWeng6sAPKA14Fwp7O2oW1nA+/4anvcmde6H7nH/u998X3WpE1ZQI77/fY2EHeq7bFbQRhjTIAKhC4gY4wxLbACYIwxAcoKgDHGBCgrAMYYE6CsABhjTICyAmCMMQHKCoAxxgSo/wdIgWkNRMMbgQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(predictions.mean(0)[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_one(bs = 100, add_seed = 0, fold = 0, anum = 0):\n",
    "    st = time.time()\n",
    "\n",
    "    cur_epoch = getCurrentBatch(fold=fold)\n",
    "    if cur_epoch is None: cur_epoch = 0\n",
    "    print('completed epochs:', cur_epoch)\n",
    "\n",
    "    model = TabularModel(n_cont = len(meta_cols), out_sz=360, layers=[500,200], ps=[0.5,0.5], bn_final=True, \n",
    "                         feat_sz=feat_sz)\n",
    "    \n",
    "    model_file_name = modelFileName(return_last=True, fold=fold)\n",
    "    if model_file_name is not None:\n",
    "        print('loading model', model_file_name)\n",
    "        state_dict = torch.load(PATH_WORK/'models'/model_file_name)\n",
    "        model.load_state_dict(state_dict)\n",
    "    \n",
    "    if (not CLOUD) or CLOUD_SINGLE:\n",
    "        model = model.to(device)\n",
    "    else:\n",
    "        model_parallel = dp.DataParallel(model, device_ids=devices)\n",
    "\n",
    "    setSeeds(SEED + cur_epoch + add_seed)\n",
    "\n",
    "    tst_ds = RSNA_DataSet(test_md, mode='test', bs=bs, fold=fold)\n",
    "    loader_tst = D.DataLoader(tst_ds, num_workers=8 if CLOUD else 0, batch_size=bs, shuffle=False)\n",
    "    print('dataset test:', len(tst_ds), 'loader test:', len(loader_tst))\n",
    "    \n",
    "    tst_ds.setFeats(anum)\n",
    "\n",
    "    loc_data = tst_ds.metadata.copy()\n",
    "    series_counts = loc_data.index.value_counts()\n",
    "\n",
    "    loc_data['orig_idx'] = np.arange(len(loc_data))\n",
    "    loc_data = loc_data.sort_values(['SeriesInstanceUID','pos_idx'])\n",
    "    loc_data['my_order'] = np.arange(len(loc_data))\n",
    "    loc_data = loc_data.sort_values(['orig_idx'])\n",
    "    \n",
    "    if CLOUD and (not CLOUD_SINGLE):\n",
    "        results = model_parallel(test_loop_fn, loader_tst)\n",
    "        predictions = np.concatenate([results[i][0] for i in range(MAX_DEVICES)])\n",
    "        indices = np.concatenate([results[i][1] for i in range(MAX_DEVICES)])\n",
    "        offsets = np.concatenate([results[i][2] for i in range(MAX_DEVICES)])\n",
    "    else:\n",
    "        predictions, indices, offsets = test_loop_fn(model, loader_tst, device)\n",
    "\n",
    "    predictions = predictions[np.argsort(indices)]\n",
    "    offsets = offsets[np.argsort(indices)]\n",
    "    assert len(predictions) == len(test_md.SeriesInstanceUID.unique())\n",
    "    assert np.all(indices[np.argsort(indices)] == np.array(range(len(predictions))))\n",
    "    \n",
    "    val_results = []\n",
    "    for k, series in enumerate(np.sort(loc_data.index.unique())):\n",
    "        cnt = series_counts[series]\n",
    "        assert (offsets[k] + cnt) <= 60\n",
    "        val_results.append(predictions[k,offsets[k]:(offsets[k] + cnt)])\n",
    "\n",
    "    val_results = np.concatenate(val_results)\n",
    "    assert np.isnan(val_results).sum() == 0\n",
    "    val_results = val_results[loc_data.my_order]\n",
    "    assert len(val_results) == len(loc_data)\n",
    "\n",
    "    print('test processing time:', time.time() - st)\n",
    "    \n",
    "    return val_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epochs: 17\n",
      "loading model model.b17.f0.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 0\n",
      "test processing time: 4.716512441635132\n",
      "completed epochs: 17\n",
      "loading model model.b17.f0.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 1\n",
      "test processing time: 4.641012907028198\n",
      "completed epochs: 17\n",
      "loading model model.b17.f0.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 2\n",
      "test processing time: 4.021617650985718\n",
      "completed epochs: 17\n",
      "loading model model.b17.f0.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 3\n",
      "test processing time: 4.19426155090332\n",
      "completed epochs: 17\n",
      "loading model model.b17.f0.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 4\n",
      "test processing time: 4.01205849647522\n",
      "completed epochs: 17\n",
      "loading model model.b17.f0.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 5\n",
      "test processing time: 3.958509683609009\n",
      "completed epochs: 17\n",
      "loading model model.b17.f0.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 6\n",
      "test processing time: 3.920159101486206\n",
      "completed epochs: 17\n",
      "loading model model.b17.f0.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 7\n",
      "test processing time: 3.898054838180542\n",
      "completed epochs: 16\n",
      "loading model model.b16.f1.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 0\n",
      "test processing time: 3.989717721939087\n",
      "completed epochs: 16\n",
      "loading model model.b16.f1.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 1\n",
      "test processing time: 3.92999267578125\n",
      "completed epochs: 16\n",
      "loading model model.b16.f1.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 2\n",
      "test processing time: 3.8665740489959717\n",
      "completed epochs: 16\n",
      "loading model model.b16.f1.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 3\n",
      "test processing time: 3.8939690589904785\n",
      "completed epochs: 16\n",
      "loading model model.b16.f1.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 4\n",
      "test processing time: 3.884352684020996\n",
      "completed epochs: 16\n",
      "loading model model.b16.f1.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 5\n",
      "test processing time: 4.021401882171631\n",
      "completed epochs: 16\n",
      "loading model model.b16.f1.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 6\n",
      "test processing time: 3.8955178260803223\n",
      "completed epochs: 16\n",
      "loading model model.b16.f1.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 7\n",
      "test processing time: 3.875286340713501\n",
      "completed epochs: 17\n",
      "loading model model.b17.f2.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 0\n",
      "test processing time: 3.907681941986084\n",
      "completed epochs: 17\n",
      "loading model model.b17.f2.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 1\n",
      "test processing time: 3.8861751556396484\n",
      "completed epochs: 17\n",
      "loading model model.b17.f2.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 2\n",
      "test processing time: 3.878710985183716\n",
      "completed epochs: 17\n",
      "loading model model.b17.f2.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 3\n",
      "test processing time: 3.9212164878845215\n",
      "completed epochs: 17\n",
      "loading model model.b17.f2.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 4\n",
      "test processing time: 3.9299263954162598\n",
      "completed epochs: 17\n",
      "loading model model.b17.f2.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 5\n",
      "test processing time: 3.9006669521331787\n",
      "completed epochs: 17\n",
      "loading model model.b17.f2.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 6\n",
      "test processing time: 3.877828359603882\n",
      "completed epochs: 17\n",
      "loading model model.b17.f2.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 7\n",
      "test processing time: 3.8699746131896973\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for fold in range(3):\n",
    "    preds2 = []\n",
    "    for anum in range(8):\n",
    "        predictions = inference_one(fold = fold, anum = anum)\n",
    "        preds2.append(predictions)\n",
    "    preds.append(np.stack(preds2))\n",
    "preds = np.stack(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.exp(np.log(preds).mean((0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = 1 / (1 + np.exp(-(np.log(preds/(1-preds)).mean((0,1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12515758, 0.00611261, 0.04398553, 0.03232377, 0.04397949,\n",
       "       0.05341499], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1264937 , 0.00635758, 0.04458478, 0.03210059, 0.0448006 ,\n",
       "       0.05447438], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12421313, 0.00613318, 0.04336379, 0.03132349, 0.04353321,\n",
       "       0.05299766], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_column = np.array([a + '_' + b for a in test_md.SOPInstanceUID for b in all_ich])\n",
    "sub = pd.DataFrame({'ID': id_column, 'Label': predictions.reshape(-1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12515757977962494"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.loc[range(0,len(sub),6), 'Label'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12421312928199768"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.loc[range(0,len(sub),6), 'Label'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sub = pd.read_csv(PATH/'submission_061.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13475628267250275"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_sub.loc[range(0,len(sub),6), 'Label'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(PATH/'sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9878769168485573"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(sub.sort_values('ID').reset_index(drop=True).loc[range(0,len(sub),6), 'Label'], \n",
    "            best_sub.sort_values('ID').reset_index(drop=True).loc[range(0,len(sub),6), 'Label'])[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|| 16.6M/16.6M [00:05<00:00, 3.16MB/s]\n",
      "Successfully submitted to RSNA Intracranial Hemorrhage Detection"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit rsna-intracranial-hemorrhage-detection -f ~/sub.csv -m \"TPU, densenet169, 4aug, 8TTA, 3folds, pre-sigmoid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
