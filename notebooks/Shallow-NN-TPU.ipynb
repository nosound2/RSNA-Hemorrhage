{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conda activate torch-xla-nightly\n",
    "#export XRT_TPU_CONFIG=\"tpu_worker;0;$10.0.101.2:8470\"\n",
    "#git init\n",
    "#git remote add origin https://github.com/nosound2/RSNA-Hemorrhage\n",
    "#git pull origin master\n",
    "#git config remote.origin.push HEAD\n",
    "#gcloud config set compute/zone europe-west4-a\n",
    "#gcloud auth login\n",
    "#gcloud config set project endless-empire-239015\n",
    "#pip install kaggle\n",
    "#mkdir .kaggle\n",
    "#gsutil cp gs://recursion-double-strand/kaggle-keys/kaggle.json ~/.kaggle\n",
    "#chmod 600 /home/zahar_chikishev/.kaggle/kaggle.json\n",
    "#kaggle competitions download rsna-intracranial-hemorrhage-detection -f stage_1_train.csv\n",
    "#sudo apt install unzip\n",
    "#unzip stage_1_train.csv.zip\n",
    "#kaggle kernels output xhlulu/rsna-generate-metadata-csvs -p .\n",
    "#gsutil cp gs://rsna-hemorrhage/yuvals/* .\n",
    "\n",
    "#export XRT_TPU_CONFIG=\"tpu_worker;0;10.0.101.2:8470\"; conda activate torch-xla-nightly; jupyter notebook\n",
    "\n",
    "# 35.204.242.164"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 38\n",
    "CLOUD_SINGLE = False\n",
    "MIXUP = False\n",
    "DATA_SET = 2\n",
    "NO_BLACK_LOSS = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "from matplotlib import patches, patheffects\n",
    "import time\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error,log_loss,roc_auc_score\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "import pdb\n",
    "\n",
    "import scipy as sp\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "\n",
    "CLOUD = not torch.cuda.is_available()\n",
    "\n",
    "if not CLOUD:\n",
    "    torch.cuda.current_device()\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as D\n",
    "import torch.nn.functional as F\n",
    "import torch.utils as U\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "from torchvision import models as M\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if CLOUD:\n",
    "    PATH = Path('/home/zahar_chikishev')\n",
    "    PATH_WORK = Path('/home/zahar_chikishev/running')\n",
    "else:\n",
    "    PATH = Path('C:/StudioProjects/Hemorrhage')\n",
    "    PATH_WORK = Path('C:/StudioProjects/Hemorrhage/running')\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import seaborn as sn\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "\n",
    "all_ich = ['any','epidural','intraparenchymal','intraventricular','subarachnoid','subdural']\n",
    "class_weights = 6.0*np.array([2,1,1,1,1,1])/7.0\n",
    "\n",
    "if CLOUD:\n",
    "    import torch_xla\n",
    "    import torch_xla.distributed.data_parallel as dp\n",
    "    import torch_xla.utils as xu\n",
    "    import torch_xla.core.xla_model as xm\n",
    "    \n",
    "    from typing import Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_black = '006d4432e'\n",
    "\n",
    "if CLOUD:\n",
    "    device = xm.xla_device()\n",
    "    #device = 'cpu'\n",
    "    MAX_DEVICES = 1 if CLOUD_SINGLE else 8\n",
    "    bs = 32\n",
    "else:\n",
    "    device = 'cuda'\n",
    "    #device = 'cpu'\n",
    "    MAX_DEVICES = 1\n",
    "    bs = 16\n",
    "\n",
    "if CLOUD and (not CLOUD_SINGLE):\n",
    "    devices = xm.get_xla_supported_devices(max_devices=MAX_DEVICES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 2351\n",
    "\n",
    "def setSeeds(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "\n",
    "setSeeds(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_cat, cols_float = pickle.load(open(PATH_WORK/'covs','rb'))\n",
    "meta_cols = cols_cat + cols_float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_SET == 1:\n",
    "    if False:\n",
    "        filename = PATH_WORK/'indexes_file.pkl'\n",
    "        all_idx, train_ids, val_ids = pickle.load(open(filename,'rb'))\n",
    "\n",
    "        train_md = pd.read_csv(PATH_WORK/'train_md.csv').sort_values(['SeriesInstanceUID','pos_idx'])\n",
    "        train_md['img_id'] = train_md.SOPInstanceUID.str.split('_').apply(lambda x: x[1])\n",
    "\n",
    "        ids_df = pd.DataFrame(all_idx, columns = ['img_id'])\n",
    "        ids_df = ids_df.join(train_md.set_index('img_id'), on = 'img_id')\n",
    "        \n",
    "        assert len(ids_df.SeriesInstanceUID.unique()) == 19530\n",
    "        \n",
    "        trn_data = ids_df.loc[ids_df.img_id.isin(all_idx[train_ids])].reset_index(drop=True)\n",
    "        val_data = ids_df.loc[ids_df.img_id.isin(all_idx[val_ids])].reset_index(drop=True)\n",
    "\n",
    "        assert len(trn_data.SeriesInstanceUID.unique()) + len(val_data.SeriesInstanceUID.unique()) \\\n",
    "            == len(train_md.SeriesInstanceUID.unique())\n",
    "\n",
    "        assert len(trn_data.PatientID.unique()) + len(val_data.PatientID.unique()) \\\n",
    "            >= len(train_md.PatientID.unique())\n",
    "\n",
    "        pickle.dump((trn_data,val_data), open(PATH_WORK/'train.post.processed.1','wb'))\n",
    "    else:\n",
    "        trn_data,val_data = pickle.load(open(PATH_WORK/'train.post.processed.1','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_SET == 1:\n",
    "    if False:\n",
    "        test_md = pd.read_csv(PATH_WORK/'test_md.csv').sort_values(['SeriesInstanceUID','pos_idx'])\n",
    "        test_md['img_id'] = test_md.SOPInstanceUID.str.split('_').apply(lambda x: x[1])\n",
    "\n",
    "        filename = PATH_WORK/'test_indexes.pkl'\n",
    "        test_ids = pickle.load(open(filename,'rb'))\n",
    "\n",
    "        test_ids_df = pd.DataFrame(test_ids, columns = ['img_id'])\n",
    "        test_md = test_ids_df.join(test_md.set_index('img_id'), on = 'img_id')\n",
    "\n",
    "        assert len(test_md.SeriesInstanceUID.unique()) == 2214\n",
    "\n",
    "        pickle.dump(test_md, open(PATH_WORK/'test.post.processed.1','wb'))\n",
    "    else:\n",
    "        test_md = pickle.load(open(PATH_WORK/'test.post.processed.1','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    print(pd.concat([test_md[meta_cols].mean(0),\n",
    "                     trn_data[meta_cols].mean(0),\n",
    "                     val_data[meta_cols].mean(0)], axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. \n",
    "model_Densenet161_3_vehrsion_basic_classifier_type_features_train_split_2.pkl\n",
    "10/7/19, 4:14:13 PM UTC+3\t\n",
    "model_Densenet161_3_vehrsion_basic_classifier_type_features_test_split_2.pkl\n",
    "10/7/19, 5:05:17 PM UTC+3\t\n",
    "indexes_file.pkl\n",
    "10/7/19, 5:34:42 PM UTC+3\t\n",
    "test_indexes.pkl \n",
    "10/9/19, 6:36:35 PM UTC+3\t\n",
    "\n",
    "2. \n",
    "model_Densenet169_3_vehrsion_basic_classifier_wso_type_features_test_tta_split_2.pkl\n",
    "10/10/19, 6:31:59 PM UTC+3\t\n",
    "model_Densenet169_3_vehrsion_basic_classifier_wso_type_features_train_tta_split_2.pkl\n",
    "10/10/19, 5:56:16 PM UTC+3\t\n",
    "\n",
    "model_Densenet161_3_vehrsion_basic_classifier_wso2_type_features_test_tta_split_2.pkl\n",
    "10/10/19, 3:07:20 PM UTC+3\t\n",
    "model_Densenet161_3_vehrsion_basic_classifier_wso2_type_features_train_tta_split_2.pkl\n",
    "10/10/19, 1:36:36 PM UTC+3\t\n",
    "\n",
    "4.\n",
    "model_Densenet161_3_version_classifier_splits_type_features_test_split_0.pkl\n",
    "10/14/19, 12:53:08 PM UTC+3\t\n",
    "model_Densenet161_3_version_classifier_splits_type_features_test_split_1.pkl\n",
    "10/14/19, 2:09:27 PM UTC+3\t\n",
    "model_Densenet161_3_version_classifier_splits_type_features_test_split_2.pkl\n",
    "10/14/19, 3:17:16 PM UTC+3\t\n",
    "\n",
    "5.\n",
    "train_dedup.csv\n",
    "10/14/19, 11:39:18 AM UTC+3\t\n",
    "model_Densenet169_3_version_classifier_splits_type_features_train_tta_split_2.pkl\n",
    "10/13/19, 2:43:34 PM UTC+3\t\n",
    "model_Densenet169_3_version_classifier_splits_type_features_train_tta_split_0.pkl\n",
    "10/14/19, 6:09:28 PM UTC+3\t\n",
    "model_Densenet169_3_version_classifier_splits_type_features_train_tta_split_1.pkl\n",
    "10/14/19, 8:09:27 PM UTC+3\t\n",
    "PID_splits.pkl\n",
    "10/14/19, 11:34:06 AM UTC+3\t\n",
    "model_Densenet169_3_version_classifier_splits_type_features_test_split_0.pkl\n",
    "10/13/19, 3:34:21 PM UTC+3\t\n",
    "model_Densenet169_3_version_classifier_splits_type_features_test_split_1.pkl\n",
    "10/13/19, 4:36:25 PM UTC+3\t\n",
    "model_Densenet169_3_version_classifier_splits_type_features_test_split_2.pkl\n",
    "10/13/19, 4:05:32 PM UTC+3\t\n",
    "\n",
    "\n",
    "I finished uploading all densenet 169 folds for test and train.\n",
    "(Be aware, fold 0 gives worse scores then 2).\n",
    "I uploaded the train dataset I used (without duplicates) it is called 'train_dedup'\n",
    "Also loaded the splits data into PID_split.\n",
    "It is a tuple. The first variable is a numpy array with the unique PIDs in train_df.\n",
    "The 2nd variable is a double list, with the indices of the train and validation for the 3 splits. \n",
    "The indices are for the PID numpy array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_SET == 1:\n",
    "    feat_sz = 2208\n",
    "elif DATA_SET == 2:\n",
    "    feat_sz = 208\n",
    "else: assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_SET == 2:\n",
    "    if False:\n",
    "        train_dedup = pd.read_csv(PATH_WORK/'yuval'/'train_dedup.csv')\n",
    "        pids, folding = pickle.load(open(PATH_WORK/'yuval'/'PID_splits.pkl','rb'))\n",
    "\n",
    "        assert len(pids) == 17079\n",
    "        assert len(np.unique(pids)) == 17079\n",
    "\n",
    "        for fol in folding:\n",
    "            assert len(fol[0]) + len(fol[1]) == 17079\n",
    "\n",
    "        assert len(folding[0][1]) + len(folding[1][1]) + len(folding[2][1]) == 17079\n",
    "\n",
    "        assert len(train_dedup.PID.unique()) == 17079\n",
    "\n",
    "        train_dedup['fold'] = np.nan\n",
    "\n",
    "        for fold in range(3):\n",
    "            train_dedup.loc[train_dedup.PID.isin(pids[folding[fold][1]]),'fold'] = fold\n",
    "\n",
    "        assert train_dedup.fold.isnull().sum() == 0\n",
    "\n",
    "        train_md = pd.read_csv(PATH_WORK/'train_md.csv').sort_values(['SeriesInstanceUID','pos_idx'])\n",
    "        train_md['img_id'] = train_md.SOPInstanceUID.str.split('_').apply(lambda x: x[1])\n",
    "\n",
    "        ids_df = train_dedup[['fold','PatientID']]\n",
    "        ids_df.columns = ['fold','img_id']\n",
    "\n",
    "        ids_df = ids_df.join(train_md.set_index('img_id'), on = 'img_id')\n",
    "\n",
    "        pickle.dump(ids_df, open(PATH_WORK/'features/densenet169_v3/train/train.ids.df','wb'))\n",
    "\n",
    "        test_md = pickle.load(open(PATH_WORK/'test.post.processed.1','rb'))\n",
    "\n",
    "        pickle.dump(test_md, open(PATH_WORK/'features/densenet169_v3/test/test.ids.df','wb'))\n",
    "\n",
    "        for fold in range(3):\n",
    "            filename = PATH_WORK/'yuval'/\\\n",
    "                'model_Densenet169_3_version_classifier_splits_type_features_train_tta_split_{}.pkl'\\\n",
    "                .format(fold)\n",
    "            feats = pickle.load(open(filename,'rb'))\n",
    "            assert len(feats) == 4*len(ids_df)\n",
    "\n",
    "            for i in range(4):\n",
    "                feats_sub1 = feats[torch.BoolTensor(np.arange(len(feats))%4 == i)]\n",
    "                feats_sub2 = feats_sub1[torch.BoolTensor(train_dedup.fold != fold)]\n",
    "                pickle.dump(feats_sub2, open(PATH_WORK/'features/densenet169_v3/train/train.f{}.a{}'\n",
    "                                             .format(fold,i),'wb'))\n",
    "\n",
    "                feats_sub2 = feats_sub1[torch.BoolTensor(train_dedup.fold == fold)]\n",
    "                pickle.dump(feats_sub2, open(PATH_WORK/'features/densenet169_v3/train/valid.f{}.a{}'\n",
    "                                             .format(fold,i),'wb'))\n",
    "\n",
    "                if i==0:\n",
    "                    black_feats = feats_sub1[torch.BoolTensor(ids_df.img_id == all_black)].squeeze()\n",
    "                    pickle.dump(black_feats, open(PATH_WORK/'features/densenet169_v3/train/black.f{}'\n",
    "                                                  .format(fold),'wb'))\n",
    "\n",
    "        for fold in range(3):\n",
    "            filename = PATH_WORK/'yuval'/\\\n",
    "                'model_Densenet169_3_version_classifier_splits_type_features_test_split_{}.pkl'\\\n",
    "                .format(fold)\n",
    "            feats = pickle.load(open(filename,'rb'))\n",
    "\n",
    "            for i in range(8):\n",
    "                feats_sub = feats[torch.BoolTensor(np.arange(len(feats))%8 == i)]\n",
    "                pickle.dump(feats_sub, open(PATH_WORK/'features/densenet169_v3/test/test.f{}.a{}'\n",
    "                                            .format(fold,i),'wb'))\n",
    "                assert len(feats_sub) == len(test_md)\n",
    "    else:\n",
    "        ids_df = pickle.load(open(PATH_WORK/'features/densenet169_v3/train/train.ids.df','rb'))\n",
    "        test_md = pickle.load(open(PATH_WORK/'features/densenet169_v3/test/test.ids.df','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_SET == 1:\n",
    "    if False:\n",
    "        filename = PATH_WORK/'model_Densenet161_3_vehrsion_basic_classifier_type_features_train_split_2.pkl'\n",
    "        feats = pickle.load(open(filename,'rb'))\n",
    "\n",
    "        for series_id in tqdm(ids_df.SeriesInstanceUID.unique()):\n",
    "            mask = torch.BoolTensor(ids_df.SeriesInstanceUID.values == series_id)\n",
    "            feats_id = feats[mask]\n",
    "            pickle.dump(feats_id, open(PATH_WORK/'features/densenet161_v3/train/{}'.format(series_id),'wb'))\n",
    "\n",
    "\n",
    "        filename = PATH_WORK/'model_Densenet161_3_vehrsion_basic_classifier_type_features_test_split_2.pkl'\n",
    "        feats = pickle.load(open(filename,'rb'))\n",
    "\n",
    "        for series_id in tqdm(test_md.SeriesInstanceUID.unique()):\n",
    "            mask = torch.BoolTensor(test_md.SeriesInstanceUID.values == series_id)\n",
    "            feats_id = feats[mask]\n",
    "            pickle.dump(feats_id, open(PATH_WORK/'features/densenet161_v3/test/{}'.format(series_id),'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#path = PATH_WORK/'features/densenet161_v3/train/ID_000a935543'\n",
    "#feats1 = pickle.load(open(path,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DATA_SET == 1:\n",
    "    path = PATH_WORK/'features/densenet161_v3/train/ID_992b567eb6'\n",
    "    black_feats = pickle.load(open(path,'rb'))[41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSNA_DataSet(D.Dataset):\n",
    "    def __init__(self, metadata, mode='train', bs=None, dataset=DATA_SET, fold=0):\n",
    "        \n",
    "        super(RSNA_DataSet, self).__init__()\n",
    "        \n",
    "        if dataset == 1:\n",
    "            md = metadata.copy()\n",
    "            md = md.reset_index(drop=True)\n",
    "        else:\n",
    "            if mode == 'train':\n",
    "                md = metadata.loc[metadata.fold != fold].copy().reset_index(drop=True)\n",
    "            elif mode == 'valid':\n",
    "                md = metadata.loc[metadata.fold == fold].copy().reset_index(drop=True)\n",
    "            else:\n",
    "                md = metadata.copy().reset_index(drop=True)\n",
    "        \n",
    "        series = np.sort(md.SeriesInstanceUID.unique())\n",
    "        md = md.set_index('SeriesInstanceUID', drop=True)\n",
    "        \n",
    "        samples_add = 0\n",
    "        if (mode != 'train') and not DATA_SMALL:\n",
    "            batch_num = -((-len(series))//(bs*MAX_DEVICES))\n",
    "            samples_add = batch_num*bs*MAX_DEVICES - len(series)\n",
    "            print('adding dummy serieses', samples_add)\n",
    "        \n",
    "        #self.records = df.to_records(index=False)\n",
    "        self.mode = mode\n",
    "        self.real = np.concatenate([np.repeat(True,len(series)),np.repeat(False,samples_add)])\n",
    "        self.series = np.concatenate([series, random.sample(list(series),samples_add)])\n",
    "        self.metadata = md\n",
    "        self.dataset = dataset\n",
    "        self.fold = fold\n",
    "        \n",
    "        print('DataSet', dataset, mode, 'size', len(self.series), 'fold', fold)\n",
    "        \n",
    "        if self.dataset == 2:\n",
    "            path = PATH_WORK/'features/densenet169_v3/train/black.f{}'.format(fold)\n",
    "            self.black_feats = pickle.load(open(path,'rb')).squeeze()\n",
    "            \n",
    "            if mode == 'valid':\n",
    "                self.setFeats(0)\n",
    "            \n",
    "        elif self.dataset == 1:\n",
    "            self.black_feats = black_feats\n",
    "    \n",
    "    def setFeats(self, anum):\n",
    "        if self.dataset == 1: return\n",
    "        print('setFeats, augmentation', anum)\n",
    "        self.anum = anum\n",
    "        folder = 'test' if self.mode == 'test' else 'train'\n",
    "        path = PATH_WORK/'features/densenet169_v3/{}/{}.f{}.a{}'.format(folder,self.mode,self.fold,anum)\n",
    "        feats = pickle.load(open(path,'rb'))\n",
    "        self.feats = feats\n",
    "        assert len(feats) == len(self.metadata)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        series_id = self.series[index]\n",
    "        #df = self.metadata.loc[self.metadata.SeriesInstanceUID == series_id].reset_index(drop=True)\n",
    "        df = self.metadata.loc[series_id].reset_index(drop=True)\n",
    "        \n",
    "        if self.dataset == 1:\n",
    "            folder = 'test' if self.mode == 'test' else 'train'\n",
    "            path = PATH_WORK/'features/densenet161_v3/{}/{}'.format(folder,series_id)\n",
    "            feats = pickle.load(open(path,'rb'))\n",
    "            \n",
    "            if feats.shape[0] > len(df.img_id.unique()):\n",
    "                mask_dup = ~df.img_id.duplicated().values\n",
    "                df = df.loc[mask_dup]\n",
    "                feats = feats[torch.BoolTensor(mask_dup)]\n",
    "            \n",
    "            assert feats.shape[0] == len(df)\n",
    "        elif self.dataset == 2:\n",
    "            feats = self.feats[torch.BoolTensor(self.metadata.index.values == series_id)]\n",
    "        else: assert False\n",
    "        \n",
    "        order = np.argsort(df.pos_idx.values)\n",
    "        df = df.sort_values(['pos_idx'])\n",
    "        feats = feats[torch.LongTensor(order)]\n",
    "        \n",
    "        non_black = torch.ones(len(feats))\n",
    "        feats = torch.cat([feats, torch.Tensor(df[meta_cols].values)], dim=1)\n",
    "        target = torch.Tensor(df[all_ich].values)\n",
    "        \n",
    "        PAD = 4+9\n",
    "        \n",
    "        offset = np.random.randint(0, 61 - feats.shape[0])\n",
    "        #offset = 0\n",
    "        top_pad = PAD + offset\n",
    "        if top_pad > 0:\n",
    "            dummy_row = torch.cat([self.black_feats, torch.Tensor(df.head(1)[meta_cols].values).squeeze()])\n",
    "            feats = torch.cat([dummy_row.repeat(top_pad,1), feats], dim=0)\n",
    "            if offset > 0:\n",
    "                non_black = torch.cat([torch.zeros(offset), non_black])\n",
    "                target = torch.cat([torch.zeros((offset, len(all_ich))), target], dim=0)\n",
    "        bot_pad = 60 - len(df) - offset + PAD\n",
    "        if bot_pad > 0:\n",
    "            dummy_row = torch.cat([self.black_feats, torch.Tensor(df.tail(1)[meta_cols].values).squeeze()])\n",
    "            feats = torch.cat([feats, dummy_row.repeat(bot_pad,1)], dim=0)\n",
    "            if (60 - len(df) - offset) > 0:\n",
    "                non_black = torch.cat([non_black, torch.zeros(60 - len(df) - offset)])\n",
    "                target = torch.cat([target, torch.zeros((60 - len(df) - offset, len(all_ich)))], dim=0)\n",
    "        \n",
    "        assert feats.shape[0] == (60 + 2*PAD)\n",
    "        assert target.shape[0] == 60\n",
    "        \n",
    "        feats = feats.transpose(1,0)\n",
    "        \n",
    "        idx = index\n",
    "        if not self.real[index]: idx = -1\n",
    "        \n",
    "        if self.mode == 'train':\n",
    "            return feats, target, non_black\n",
    "        else:\n",
    "            return feats, target, idx, offset\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.series) if not DATA_SMALL else int(0.01*len(self.series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCurrentBatch(fold=0):\n",
    "    sel_batch = None\n",
    "    for filename in os.listdir(PATH_WORK/'models'):\n",
    "        splits = filename.split('.')\n",
    "        if int(splits[2][1]) != fold: continue\n",
    "        if int(splits[3][1:]) != VERSION: continue\n",
    "        if sel_batch is None:\n",
    "            sel_batch = int(splits[1][1:])\n",
    "        else:\n",
    "            sel_batch = max(sel_batch, int(splits[1][1:]))\n",
    "    return sel_batch\n",
    "\n",
    "def modelFileName(fold=0, batch = 1, return_last = False, return_next = False):\n",
    "    sel_batch = batch\n",
    "    if return_last or return_next:\n",
    "        sel_batch = getCurrentBatch(fold)\n",
    "        if return_last and sel_batch is None:\n",
    "            return None\n",
    "        if return_next:\n",
    "            if sel_batch is None: sel_batch = 1\n",
    "            else: sel_batch += 1\n",
    "    \n",
    "    return 'model.b{}.f{}.v{}'.format(sel_batch, fold, VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BCEWithLogitsLoss(nn.Module):\n",
    "    def __init__(self, weight=None):\n",
    "        super().__init__()\n",
    "        self.weight = weight\n",
    "    \n",
    "    def forward(self, input, target, batch_weights = None):\n",
    "        loss = (torch.log(1+torch.exp(input)) - target*input)*self.weight\n",
    "        if batch_weights is not None:\n",
    "            loss = batch_weights*loss\n",
    "        return loss.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bn_drop_lin(n_in:int, n_out:int, bn:bool=True, p:float=0., actn=None):\n",
    "    \"Sequence of batchnorm (if `bn`), dropout (with `p`) and linear (`n_in`,`n_out`) layers followed by `actn`.\"\n",
    "    layers = [nn.BatchNorm1d(n_in)] if bn else []\n",
    "    if p != 0: layers.append(nn.Dropout(p))\n",
    "    layers.append(nn.Linear(n_in, n_out))\n",
    "    if actn is not None: layers.append(actn)\n",
    "    return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularModel(nn.Module):\n",
    "    \"Basic model for tabular data.\"\n",
    "    def __init__(self, n_cont:int, feat_sz=2208, fc_drop_p=0.3):\n",
    "        super().__init__()\n",
    "        self.bn_cont = nn.BatchNorm1d(feat_sz + n_cont)\n",
    "        self.n_cont = n_cont\n",
    "        self.fc_drop = nn.Dropout(p=fc_drop_p)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2D_1 = nn.Conv2d(1,32,(feat_sz + n_cont,1))\n",
    "        self.conv2D_2 = nn.Conv2d(1,32,(feat_sz + n_cont,5))#,padding=(0,2)\n",
    "        self.bn_cont1 = nn.BatchNorm1d(64)\n",
    "        self.conv1D_1 = nn.Conv1d(64,32,3)#,padding=1\n",
    "        self.conv1D_3 = nn.Conv1d(64,32,5,dilation=5)\n",
    "        self.conv1D_2 = nn.Conv1d(64,6,3)#,padding=1\n",
    "        self.bn_cont2 = nn.BatchNorm1d(64)\n",
    "        self.bn_cont3 = nn.BatchNorm1d(6)\n",
    "\n",
    "    def forward(self, x) -> torch.Tensor:\n",
    "        x = self.bn_cont(x) # bs,2208,60\n",
    "        x = self.fc_drop(x)\n",
    "        x = x.reshape(x.shape[0],1,x.shape[1],x.shape[2]) # bs,1,2208,60\n",
    "        x = torch.cat([self.conv2D_1(x[:,:,:,2:(-2)]).squeeze(), \n",
    "                       self.conv2D_2(x).squeeze()], dim=1) # bs,64,60\n",
    "        x = self.relu(x)\n",
    "        x = self.bn_cont1(x)\n",
    "        x = self.fc_drop(x)\n",
    "        #x = self.conv1D_1(x)\n",
    "        x = torch.cat([self.conv1D_1(x[:,:,9:(-9)]), \n",
    "                       self.conv1D_3(x)], dim=1) # bs,64,60\n",
    "        x = self.relu(x)\n",
    "        x = self.bn_cont2(x)\n",
    "        x = self.fc_drop(x)\n",
    "        x = self.conv1D_2(x)\n",
    "        x = x.transpose(1,2) # bs,60,6\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_fn(model, loader, device, context = None):\n",
    "    \n",
    "    if CLOUD and (not CLOUD_SINGLE):\n",
    "        tlen = len(loader._loader._loader)\n",
    "        OUT_LOSS = 1000\n",
    "        OUT_TIME = 4\n",
    "        generator = loader\n",
    "        device_num = int(str(device)[-1])\n",
    "        dataset = loader._loader._loader.dataset\n",
    "    else:\n",
    "        tlen = len(loader)\n",
    "        OUT_LOSS = 10\n",
    "        OUT_TIME = 1\n",
    "        generator = enumerate(loader)\n",
    "        device_num = 1\n",
    "        dataset = loader.dataset\n",
    "    \n",
    "    #print('Start training {}'.format(device), 'batches', tlen)\n",
    "    \n",
    "    criterion = BCEWithLogitsLoss(weight = torch.Tensor(class_weights).to(device))\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay, betas=(0.9, 0.99))\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    if CLOUD:\n",
    "        tracker = xm.RateTracker()\n",
    "\n",
    "    tloss = 0\n",
    "    tloss_count = 0\n",
    "    \n",
    "    st = time.time()\n",
    "    mixup_collected = False\n",
    "    for i, (x, y, non_black) in generator:\n",
    "        if (not CLOUD) or CLOUD_SINGLE:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            non_black = non_black.to(device)\n",
    "        \n",
    "        if MIXUP:\n",
    "            if mixup_collected:\n",
    "                lambd = np.random.beta(0.4, 0.4, y.size(0))\n",
    "                lambd = torch.Tensor(lambd).to(device)[:,None,None]\n",
    "                #shuffle = torch.randperm(y.size(0)).to(device)\n",
    "                x = lambd * x + (1-lambd) * x_mix #x[shuffle]\n",
    "                mixup_collected = False\n",
    "            else:\n",
    "                x_mix = x\n",
    "                y_mix = y\n",
    "                mixup_collected = True\n",
    "                continue\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(x)\n",
    "        \n",
    "        if MIXUP:\n",
    "            loss = criterion(output, y, lambd) + criterion(output, y_mix, 1-lambd) #y[shuffle]\n",
    "            del x_mix, y_mix\n",
    "        else:\n",
    "            if NO_BLACK_LOSS:\n",
    "                loss = criterion(output, y, non_black[:,:,None])\n",
    "            else:\n",
    "                loss = criterion(output, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        tloss += len(y)*loss.cpu().detach().item()\n",
    "        tloss_count += len(y)\n",
    "        \n",
    "        if CLOUD or CLOUD_SINGLE:\n",
    "            xm.optimizer_step(optimizer)\n",
    "            if CLOUD_SINGLE:\n",
    "                xm.mark_step()\n",
    "        else:\n",
    "            optimizer.step()\n",
    "        \n",
    "        if CLOUD:\n",
    "            tracker.add(len(y))\n",
    "        \n",
    "        st_passed = time.time() - st\n",
    "        if (i+1)%OUT_TIME == 0 and device_num == 1:\n",
    "            #print(torch_xla._XLAC._xla_metrics_report())\n",
    "            print('Batch {} device: {} time passed: {:.3f} time per batch: {:.3f}'\n",
    "                .format(i+1, device, st_passed, st_passed/(i+1)))\n",
    "        \n",
    "        del loss, output, y, x\n",
    "    \n",
    "    return tloss, tloss_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def val_loop_fn(model, loader, device, context = None):\n",
    "    \n",
    "    if CLOUD and (not CLOUD_SINGLE):\n",
    "        tlen = len(loader._loader._loader)\n",
    "        OUT_LOSS = 1000\n",
    "        OUT_TIME = 4\n",
    "        generator = loader\n",
    "        device_num = int(str(device)[-1])\n",
    "    else:\n",
    "        tlen = len(loader)\n",
    "        OUT_LOSS = 1000\n",
    "        OUT_TIME = 10\n",
    "        generator = enumerate(loader)\n",
    "        device_num = 1\n",
    "    \n",
    "    #print('Start validating {}'.format(device), 'batches', tlen)\n",
    "    \n",
    "    st = time.time()\n",
    "    model.eval()\n",
    "    \n",
    "    results = []\n",
    "    indices = []\n",
    "    offsets = []\n",
    "    \n",
    "    for i, (x, y, idx, offset) in generator:\n",
    "        \n",
    "        if (not CLOUD) or CLOUD_SINGLE:\n",
    "            x = x.to(device)\n",
    "        \n",
    "        output = torch.sigmoid(model(x))\n",
    "        \n",
    "        mask = (idx >= 0)\n",
    "        results.append(output[mask].cpu().detach().numpy())\n",
    "        indices.append(idx[mask].cpu().detach().numpy())\n",
    "        offsets.append(offset[mask].cpu().detach().numpy())\n",
    "        \n",
    "        st_passed = time.time() - st\n",
    "        if (i+1)%OUT_TIME == 0 and device_num == 1:\n",
    "            print('Batch {} device: {} time passed: {:.3f} time per batch: {:.3f}'\n",
    "                  .format(i+1, device, st_passed, st_passed/(i+1)))\n",
    "        \n",
    "        del output, y, x, idx, offset\n",
    "    \n",
    "    results = np.concatenate(results)\n",
    "    indices = np.concatenate(indices)\n",
    "    offsets = np.concatenate(offsets)\n",
    "    \n",
    "    return results, indices, offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def test_loop_fn(model, loader, device, context = None):\n",
    "    \n",
    "    if CLOUD and (not CLOUD_SINGLE):\n",
    "        tlen = len(loader._loader._loader)\n",
    "        OUT_LOSS = 1000\n",
    "        OUT_TIME = 100\n",
    "        generator = loader\n",
    "        device_num = int(str(device)[-1])\n",
    "    else:\n",
    "        tlen = len(loader)\n",
    "        OUT_LOSS = 1000\n",
    "        OUT_TIME = 10\n",
    "        generator = enumerate(loader)\n",
    "        device_num = 1\n",
    "    \n",
    "    #print('Start testing {}'.format(device), 'batches', tlen)\n",
    "    \n",
    "    st = time.time()\n",
    "    model.eval()\n",
    "    \n",
    "    results = []\n",
    "    indices = []\n",
    "    offsets = []\n",
    "    \n",
    "    for i, (x, y, idx, offset) in generator:\n",
    "        \n",
    "        if (not CLOUD) or CLOUD_SINGLE:\n",
    "            x = x.to(device)\n",
    "        \n",
    "        output = torch.sigmoid(model(x))\n",
    "        \n",
    "        mask = (idx >= 0)\n",
    "        results.append(output[mask].cpu().detach().numpy())\n",
    "        indices.append(idx[mask].cpu().detach().numpy())\n",
    "        offsets.append(offset[mask].cpu().detach().numpy())\n",
    "        \n",
    "        st_passed = time.time() - st\n",
    "        if (i+1)%OUT_TIME == 0 and device_num == 1:\n",
    "            print('B{} -> time passed: {:.3f} time per batch: {:.3f}'.format(i+1, st_passed, st_passed/(i+1)))\n",
    "        \n",
    "        del output, x, y, idx, offset\n",
    "    \n",
    "    return np.concatenate(results), np.concatenate(indices), np.concatenate(offsets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(weight=None, load_model=True, epochs=1, bs=100, fold=0):\n",
    "    \n",
    "    st0 = time.time()\n",
    "    \n",
    "    cur_epoch = getCurrentBatch(fold=fold)\n",
    "    if cur_epoch is None: cur_epoch = 0\n",
    "    print('completed epochs:', cur_epoch, 'starting now:', epochs)\n",
    "    \n",
    "    setSeeds(SEED + cur_epoch)\n",
    "    \n",
    "    if DATA_SET == 1:\n",
    "        trn_ds = RSNA_DataSet(trn_data, mode='train', bs=bs, fold=fold)\n",
    "        val_ds = RSNA_DataSet(val_data, mode='valid', bs=bs, fold=fold)\n",
    "    elif DATA_SET == 2:\n",
    "        trn_ds = RSNA_DataSet(ids_df, mode='train', bs=bs, fold=fold)\n",
    "        val_ds = RSNA_DataSet(ids_df, mode='valid', bs=bs, fold=fold)\n",
    "    else: assert False\n",
    "    \n",
    "    loader = D.DataLoader(trn_ds, num_workers=16 if (CLOUD and not CLOUD_SINGLE) else 0, batch_size=bs, \n",
    "                          shuffle=True, drop_last=True)\n",
    "    loader_val = D.DataLoader(val_ds, num_workers=16 if (CLOUD and not CLOUD_SINGLE) else 0, batch_size=bs, \n",
    "                              shuffle=True)\n",
    "    print('dataset train:', len(trn_ds), 'valid:', len(val_ds), 'loader train:', len(loader), 'valid:', len(loader_val))\n",
    "    \n",
    "    model = TabularModel(n_cont = len(meta_cols), feat_sz=feat_sz)\n",
    "    \n",
    "    model_file_name = modelFileName(return_last=True, fold=fold)\n",
    "    if model_file_name is not None:\n",
    "        print('loading model', model_file_name)\n",
    "        state_dict = torch.load(PATH_WORK/'models'/model_file_name)\n",
    "        model.load_state_dict(state_dict)\n",
    "    else:\n",
    "        print('starting from scratch')\n",
    "    \n",
    "    if (not CLOUD) or CLOUD_SINGLE:\n",
    "        model = model.to(device)\n",
    "    else:\n",
    "        model_parallel = dp.DataParallel(model, device_ids=devices)\n",
    "        \n",
    "    loc_data = val_ds.metadata.copy()\n",
    "    #if DATA_SET == 1:\n",
    "    #    loc_data = val_data.copy()\n",
    "    #else:\n",
    "    #    loc_data = ids_df.loc[ids_df.fold == fold].copy().reset_index(drop=True)\n",
    "\n",
    "    if DATA_SMALL:\n",
    "        #val_sz = int(0.01*len(loc_data.index.unique()))\n",
    "        val_sz = len(val_ds)\n",
    "        val_series = val_ds.series[:val_sz]\n",
    "        loc_data = loc_data.loc[loc_data.index.isin(val_series)]\n",
    "    \n",
    "    series_counts = loc_data.index.value_counts()\n",
    "    \n",
    "    loc_data['orig_idx'] = np.arange(len(loc_data))\n",
    "    loc_data = loc_data.sort_values(['SeriesInstanceUID','pos_idx'])\n",
    "    loc_data['my_order'] = np.arange(len(loc_data))\n",
    "    loc_data = loc_data.sort_values(['orig_idx'])\n",
    "    \n",
    "    for i in range(cur_epoch+1, cur_epoch+epochs+1):\n",
    "        st = time.time()\n",
    "        \n",
    "        trn_ds.setFeats((i-1) % 4)\n",
    "        \n",
    "        if CLOUD and (not CLOUD_SINGLE):\n",
    "            results = model_parallel(train_loop_fn, loader)\n",
    "            tloss, tloss_count = np.stack(results).sum(0)\n",
    "            state_dict = model_parallel._models[0].state_dict()\n",
    "        else:\n",
    "            tloss, tloss_count = train_loop_fn(model, loader, device)\n",
    "            state_dict = model.state_dict()\n",
    "        \n",
    "        state_dict = {k:v.to('cpu') for k,v in state_dict.items()}\n",
    "        tr_ll = tloss / tloss_count\n",
    "        \n",
    "        train_time = time.time()-st\n",
    "        \n",
    "        model_file_name = modelFileName(return_next=True, fold=fold)\n",
    "        if not DATA_SMALL:\n",
    "            torch.save(state_dict, PATH_WORK/'models'/model_file_name)\n",
    "        \n",
    "        st = time.time()\n",
    "        if CLOUD and (not CLOUD_SINGLE):\n",
    "            results = model_parallel(val_loop_fn, loader_val)\n",
    "            predictions = np.concatenate([results[i][0] for i in range(MAX_DEVICES)])\n",
    "            indices = np.concatenate([results[i][1] for i in range(MAX_DEVICES)])\n",
    "            offsets = np.concatenate([results[i][2] for i in range(MAX_DEVICES)])\n",
    "        else:\n",
    "            predictions, indices, offsets = val_loop_fn(model, loader_val, device)\n",
    "        \n",
    "        predictions = predictions[np.argsort(indices)]\n",
    "        offsets = offsets[np.argsort(indices)]\n",
    "        assert len(predictions) == len(loc_data.index.unique())\n",
    "        assert len(predictions) == len(offsets)\n",
    "        assert np.all(indices[np.argsort(indices)] == np.array(range(len(predictions))))\n",
    "        \n",
    "        #val_results = np.zeros((len(loc_data),6))\n",
    "        val_results = []\n",
    "        for k, series in enumerate(np.sort(loc_data.index.unique())):\n",
    "            cnt = series_counts[series]\n",
    "            #mask = loc_data.SeriesInstanceUID == series\n",
    "            assert (offsets[k] + cnt) <= 60\n",
    "            #val_results[mask] = predictions[k,offsets[k]:(offsets[k] + cnt)]\n",
    "            val_results.append(predictions[k,offsets[k]:(offsets[k] + cnt)])\n",
    "        \n",
    "        val_results = np.concatenate(val_results)\n",
    "        assert np.isnan(val_results).sum() == 0\n",
    "        val_results = val_results[loc_data.my_order]\n",
    "        assert np.isnan(val_results).sum() == 0\n",
    "        assert len(val_results) == len(loc_data)\n",
    "        \n",
    "        lls = [log_loss(loc_data[all_ich[k]].values, val_results[:,k], eps=1e-8, labels=[0,1]) for k in range(6)]\n",
    "        ll = (class_weights * np.array(lls)).mean()\n",
    "        cor = np.corrcoef(loc_data.loc[:,all_ich].values.reshape(-1), val_results.reshape(-1))[0,1]\n",
    "        auc = roc_auc_score(loc_data.loc[:,all_ich].values.reshape(-1), val_results.reshape(-1))\n",
    "\n",
    "        print('ver {}, epoch {}, fold {}, train ll: {:.4f}, val ll: {:.4f}, cor: {:.4f}, auc: {:.4f}, lr: {}'\n",
    "              .format(VERSION, i, fold, tr_ll, ll, cor, auc, learning_rate))\n",
    "        valid_time = time.time()-st\n",
    "\n",
    "        epoch_stats = pd.DataFrame([[i, 0, tr_ll, ll, cor, lls[0], lls[1], lls[2], lls[3], lls[4], lls[5],\n",
    "                                     len(trn_ds), len(val_ds), bs, train_time, valid_time,\n",
    "                                     learning_rate, weight_decay]], \n",
    "                                   columns = \n",
    "                                    ['epoch','fold','train_loss','val_loss','cor',\n",
    "                                     'any','epidural','intraparenchymal','intraventricular','subarachnoid','subdural',\n",
    "                                     'train_sz','val_sz','bs','train_time','valid_time','lr','wd'\n",
    "                                     ])\n",
    "\n",
    "        stats_filename = PATH_WORK/'stats.f{}.v{}'.format(fold,VERSION)\n",
    "        if stats_filename.is_file():\n",
    "            epoch_stats = pd.concat([pd.read_csv(stats_filename), epoch_stats], sort=False)\n",
    "        #if not DATA_SMALL:\n",
    "        epoch_stats.to_csv(stats_filename, index=False)\n",
    "    \n",
    "    print('total running time', time.time() - st0)\n",
    "    \n",
    "    return model, predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Batch 22 device: xla:1 time passed: 277.972 time per batch: 12.635 - 16 cores / 8 workers\n",
    "#Batch 22 device: xla:1 time passed: 209.280 time per batch: 9.513  - 16 cores / 16 workers\n",
    "#Batch 22 device: xla:1 time passed: 213.209 time per batch: 9.691  - 16 cores / 32 workers\n",
    "#Batch 22 device: xla:1 time passed: 275.780 time per batch: 12.535 - 16 cores / 8 workers\n",
    "#Batch 22 device: xla:1 time passed: 208.826 time per batch: 9.492  - 16 cores / 16 workers\n",
    "#Batch 22 device: xla:1 time passed: 245.750 time per batch: 11.170 - 16 cores / 12 workers\n",
    "#Batch 22 device: xla:1 time passed: 374.876 time per batch: 17.040 - 16 cores / 8 workers\n",
    "#Batch 22 device: xla:1 time passed: 400.221 time per batch: 18.192 - 8 cores / 8 workers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bs=16\n",
    "# ver 38, epoch 14, fold 0, train ll: 0.0365, val ll: 0.0694, cor: 0.8330, lr: 0.0002\n",
    "# ver 38, epoch 17, fold 0, train ll: 0.0362, val ll: 0.0684, cor: 0.8346, lr: 0.0001\n",
    "\n",
    "# back to 64, dropped dilated\n",
    "# ver 37, epoch 14, fold 2, train ll: 0.0367, val ll: 0.0641, cor: 0.8360, lr: 0.0002\n",
    "# ver 37, epoch 14, fold 1, train ll: 0.0374, val ll: 0.0665, cor: 0.8324, lr: 0.0002\n",
    "# ver 37, epoch 14, fold 0, train ll: 0.0360, val ll: 0.0676, cor: 0.8346, lr: 0.0002\n",
    "\n",
    "# 3x0.0001\n",
    "# ver 36, epoch 17, fold 2, train ll: 0.0364, val ll: 0.0646, cor: 0.8367\n",
    "# ver 36, epoch 16, fold 1, train ll: 0.0375, val ll: 0.0663, cor: 0.8326\n",
    "# ver 36, epoch 17, fold 0, train ll: 0.0358, val ll: 0.0677, cor: 0.8357\n",
    "\n",
    "# 7x0.02, 4x0.002, 3x0.0002\n",
    "# bs = 32\n",
    "# ver 36, epoch 14, fold 2, train ll: 0.0366, val ll: 0.0647, cor: 0.8362\n",
    "# ver 36, epoch 13, fold 1, train ll: 0.0379, val ll: 0.0663, cor: 0.8327\n",
    "# ver 36, epoch 14, fold 0, train ll: 0.0359, val ll: 0.0674, cor: 0.8354\n",
    "\n",
    "# 10x0.02, 6x0.002, 3x0.0002\n",
    "# bs = 64\n",
    "# ver 34, epoch 19, train ll: 0.0363, val ll: 0.0675, cor: 0.8347\n",
    "\n",
    "#--- dataset 2\n",
    "\n",
    "# 13x0.02, 6x0.002, 3x0.0002\n",
    "# bs = 64\n",
    "# ver 32, epoch 22, train ll: 0.0375, val ll: 0.0582, cor: 0.8480\n",
    "\n",
    "#--- dataset 1\n",
    "\n",
    "# 10x0.02, 8x0.002, 3x0.0002\n",
    "# with augmentations\n",
    "# ver 31, epoch 21, train ll: 0.0866, val ll: 0.1533, cor: 0.5319\n",
    "\n",
    "# 10x0.02, 6x0.002, 3x0.0002\n",
    "# 32 to 64 for conv2D reduce before, w dilated\n",
    "# ver 26, epoch 19, train ll: 0.0867, val ll: 0.1531, cor: 0.5305\n",
    "\n",
    "#--- dataset 2\n",
    "\n",
    "# 10x0.02, 3x0.002, 6x0.0002 (+10x0.0002)\n",
    "# back to orig best\n",
    "# epoch 19, train ll: 0.0389, val ll: 0.0602, cor: 0.8424\n",
    "# epoch 29, train ll: 0.0378, val ll: 0.0593, cor: 0.8448\n",
    "\n",
    "# 3x0.05, 5x0.02, 3x0.002, 6x0.0002\n",
    "# reduced all sizes\n",
    "# epoch 17, train ll: 0.0407, val ll: 0.0620, cor: 0.8371\n",
    "\n",
    "# 3x0.05, 5x0.02, 3x0.002, 6x0.0002\n",
    "# 32 to 64 for conv2D reduce before, w dilated\n",
    "# epoch 17, train ll: 0.0379, val ll: 0.0589, cor: 0.8460\n",
    "\n",
    "# 3x0.1, 5x0.02, 3x0.002, 6x0.0002\n",
    "# dilated, 3xTTA\n",
    "# epoch 17, train ll: 0.0364, val ll: 0.0582, cor: 0.8454, LB 0.066\n",
    "\n",
    "# train ll: 0.0354, val ll: 0.0577, cor: 0.8462, LB 0.065"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-cycle\n",
    "# copy latest model to GS code\n",
    "# improve black image meta data\n",
    "# freeze bias approach?\n",
    "# pseudo-labelling?\n",
    "# normalize metadata outliers\n",
    "# try GCP fast guide connecting\n",
    "# 32 TTA\n",
    "# add AUC metric, check mixup on it\n",
    "# no auc\n",
    "# no initial bn for feats\n",
    "# no loss for black slices\n",
    "\n",
    "# Yuval: zoom in, squish, perspective wraping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epochs: 0 starting now: 1\n",
      "DataSet 2 train size 13042 fold 0\n",
      "DataSet 2 valid size 6488 fold 0\n",
      "setFeats, augmentation 0\n",
      "dataset train: 130 valid: 64 loader train: 8 valid: 4\n",
      "starting from scratch\n",
      "setFeats, augmentation 0\n",
      "Batch 1 device: cuda time passed: 0.731 time per batch: 0.731\n",
      "Batch 2 device: cuda time passed: 1.462 time per batch: 0.731\n",
      "Batch 3 device: cuda time passed: 2.175 time per batch: 0.725\n",
      "Batch 4 device: cuda time passed: 2.887 time per batch: 0.722\n",
      "Batch 5 device: cuda time passed: 3.606 time per batch: 0.721\n",
      "Batch 6 device: cuda time passed: 4.326 time per batch: 0.721\n",
      "Batch 7 device: cuda time passed: 5.039 time per batch: 0.720\n",
      "Batch 8 device: cuda time passed: 5.753 time per batch: 0.719\n",
      "ver 38, epoch 1, fold 0, train ll: 0.1695, val ll: 0.4171, cor: 0.2315, auc: 0.8260, lr: 0.02\n",
      "total running time 10.096197128295898\n"
     ]
    }
   ],
   "source": [
    "DATA_SMALL = True\n",
    "MIXUP = False\n",
    "learning_rate = 0.02\n",
    "weight_decay = 1e-3\n",
    "model, predictions = train_one(epochs=1, bs=16, fold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>fold</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>cor</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "      <th>train_sz</th>\n",
       "      <th>val_sz</th>\n",
       "      <th>bs</th>\n",
       "      <th>train_time</th>\n",
       "      <th>valid_time</th>\n",
       "      <th>lr</th>\n",
       "      <th>wd</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.495845</td>\n",
       "      <td>0.222767</td>\n",
       "      <td>0.262498</td>\n",
       "      <td>0.372876</td>\n",
       "      <td>0.077706</td>\n",
       "      <td>0.184072</td>\n",
       "      <td>0.147871</td>\n",
       "      <td>0.184957</td>\n",
       "      <td>0.219007</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>80.560221</td>\n",
       "      <td>140.067585</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.139651</td>\n",
       "      <td>0.212226</td>\n",
       "      <td>0.341014</td>\n",
       "      <td>0.371492</td>\n",
       "      <td>0.032217</td>\n",
       "      <td>0.182368</td>\n",
       "      <td>0.141474</td>\n",
       "      <td>0.176040</td>\n",
       "      <td>0.210502</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>80.586969</td>\n",
       "      <td>143.552010</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.116839</td>\n",
       "      <td>0.206248</td>\n",
       "      <td>0.416058</td>\n",
       "      <td>0.357151</td>\n",
       "      <td>0.030161</td>\n",
       "      <td>0.177022</td>\n",
       "      <td>0.138181</td>\n",
       "      <td>0.175438</td>\n",
       "      <td>0.208632</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>81.362263</td>\n",
       "      <td>142.334380</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0.100432</td>\n",
       "      <td>0.248836</td>\n",
       "      <td>0.337192</td>\n",
       "      <td>0.446849</td>\n",
       "      <td>0.031468</td>\n",
       "      <td>0.198398</td>\n",
       "      <td>0.151840</td>\n",
       "      <td>0.208802</td>\n",
       "      <td>0.257649</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>80.487220</td>\n",
       "      <td>142.352772</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0.101299</td>\n",
       "      <td>0.198162</td>\n",
       "      <td>0.448064</td>\n",
       "      <td>0.339129</td>\n",
       "      <td>0.027744</td>\n",
       "      <td>0.173676</td>\n",
       "      <td>0.136346</td>\n",
       "      <td>0.179236</td>\n",
       "      <td>0.191874</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>84.477289</td>\n",
       "      <td>146.857369</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0.097309</td>\n",
       "      <td>0.190345</td>\n",
       "      <td>0.473876</td>\n",
       "      <td>0.327777</td>\n",
       "      <td>0.027687</td>\n",
       "      <td>0.166361</td>\n",
       "      <td>0.123394</td>\n",
       "      <td>0.165293</td>\n",
       "      <td>0.194129</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>81.727369</td>\n",
       "      <td>143.910612</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0.095991</td>\n",
       "      <td>0.179739</td>\n",
       "      <td>0.478224</td>\n",
       "      <td>0.308505</td>\n",
       "      <td>0.027745</td>\n",
       "      <td>0.164267</td>\n",
       "      <td>0.121925</td>\n",
       "      <td>0.155056</td>\n",
       "      <td>0.172170</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>81.609755</td>\n",
       "      <td>140.314667</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.105157</td>\n",
       "      <td>0.162818</td>\n",
       "      <td>0.491558</td>\n",
       "      <td>0.280636</td>\n",
       "      <td>0.027615</td>\n",
       "      <td>0.150090</td>\n",
       "      <td>0.105528</td>\n",
       "      <td>0.135659</td>\n",
       "      <td>0.159565</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>81.324294</td>\n",
       "      <td>141.004881</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0.101255</td>\n",
       "      <td>0.182301</td>\n",
       "      <td>0.492721</td>\n",
       "      <td>0.322835</td>\n",
       "      <td>0.027490</td>\n",
       "      <td>0.159502</td>\n",
       "      <td>0.115876</td>\n",
       "      <td>0.152459</td>\n",
       "      <td>0.175106</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>81.279920</td>\n",
       "      <td>140.633701</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0.103850</td>\n",
       "      <td>0.161570</td>\n",
       "      <td>0.499187</td>\n",
       "      <td>0.276134</td>\n",
       "      <td>0.029684</td>\n",
       "      <td>0.149958</td>\n",
       "      <td>0.109171</td>\n",
       "      <td>0.133094</td>\n",
       "      <td>0.156818</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>81.465451</td>\n",
       "      <td>142.004258</td>\n",
       "      <td>0.0200</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0.089629</td>\n",
       "      <td>0.163397</td>\n",
       "      <td>0.510048</td>\n",
       "      <td>0.281652</td>\n",
       "      <td>0.028058</td>\n",
       "      <td>0.149294</td>\n",
       "      <td>0.107586</td>\n",
       "      <td>0.132725</td>\n",
       "      <td>0.162813</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>79.851575</td>\n",
       "      <td>141.318649</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "      <td>0.089327</td>\n",
       "      <td>0.160301</td>\n",
       "      <td>0.517028</td>\n",
       "      <td>0.276732</td>\n",
       "      <td>0.027610</td>\n",
       "      <td>0.147871</td>\n",
       "      <td>0.105576</td>\n",
       "      <td>0.131298</td>\n",
       "      <td>0.156291</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>80.063437</td>\n",
       "      <td>140.657684</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0.088809</td>\n",
       "      <td>0.158927</td>\n",
       "      <td>0.515110</td>\n",
       "      <td>0.274000</td>\n",
       "      <td>0.027109</td>\n",
       "      <td>0.145623</td>\n",
       "      <td>0.105583</td>\n",
       "      <td>0.130697</td>\n",
       "      <td>0.155481</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>80.327655</td>\n",
       "      <td>142.896167</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0.088837</td>\n",
       "      <td>0.155999</td>\n",
       "      <td>0.521167</td>\n",
       "      <td>0.270748</td>\n",
       "      <td>0.026655</td>\n",
       "      <td>0.143846</td>\n",
       "      <td>0.101793</td>\n",
       "      <td>0.126280</td>\n",
       "      <td>0.151921</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>80.514647</td>\n",
       "      <td>144.235245</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0.088136</td>\n",
       "      <td>0.157832</td>\n",
       "      <td>0.523962</td>\n",
       "      <td>0.274537</td>\n",
       "      <td>0.026952</td>\n",
       "      <td>0.144709</td>\n",
       "      <td>0.101704</td>\n",
       "      <td>0.127705</td>\n",
       "      <td>0.154680</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>80.382700</td>\n",
       "      <td>143.656591</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.088043</td>\n",
       "      <td>0.155825</td>\n",
       "      <td>0.526298</td>\n",
       "      <td>0.272011</td>\n",
       "      <td>0.026415</td>\n",
       "      <td>0.142138</td>\n",
       "      <td>0.099895</td>\n",
       "      <td>0.126003</td>\n",
       "      <td>0.152305</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>80.809602</td>\n",
       "      <td>141.776692</td>\n",
       "      <td>0.0020</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>0.086902</td>\n",
       "      <td>0.154525</td>\n",
       "      <td>0.529515</td>\n",
       "      <td>0.269470</td>\n",
       "      <td>0.026794</td>\n",
       "      <td>0.141182</td>\n",
       "      <td>0.098794</td>\n",
       "      <td>0.123728</td>\n",
       "      <td>0.152238</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>80.541241</td>\n",
       "      <td>141.074992</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>0.086735</td>\n",
       "      <td>0.154822</td>\n",
       "      <td>0.530893</td>\n",
       "      <td>0.270364</td>\n",
       "      <td>0.026630</td>\n",
       "      <td>0.141084</td>\n",
       "      <td>0.098386</td>\n",
       "      <td>0.123932</td>\n",
       "      <td>0.152994</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>80.734518</td>\n",
       "      <td>142.519661</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.086563</td>\n",
       "      <td>0.153274</td>\n",
       "      <td>0.531932</td>\n",
       "      <td>0.267862</td>\n",
       "      <td>0.026561</td>\n",
       "      <td>0.140366</td>\n",
       "      <td>0.097915</td>\n",
       "      <td>0.123196</td>\n",
       "      <td>0.149155</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>80.243078</td>\n",
       "      <td>143.743049</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0.086106</td>\n",
       "      <td>0.364768</td>\n",
       "      <td>0.062663</td>\n",
       "      <td>0.698073</td>\n",
       "      <td>0.033167</td>\n",
       "      <td>0.267723</td>\n",
       "      <td>0.216273</td>\n",
       "      <td>0.286145</td>\n",
       "      <td>0.353920</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>64.752618</td>\n",
       "      <td>22.980388</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>26</td>\n",
       "      <td>0</td>\n",
       "      <td>0.138757</td>\n",
       "      <td>0.242859</td>\n",
       "      <td>0.235416</td>\n",
       "      <td>0.429052</td>\n",
       "      <td>0.034606</td>\n",
       "      <td>0.201127</td>\n",
       "      <td>0.150016</td>\n",
       "      <td>0.216166</td>\n",
       "      <td>0.239994</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>47.874474</td>\n",
       "      <td>16.058632</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>0.121541</td>\n",
       "      <td>0.212181</td>\n",
       "      <td>0.299105</td>\n",
       "      <td>0.364818</td>\n",
       "      <td>0.031306</td>\n",
       "      <td>0.177752</td>\n",
       "      <td>0.139025</td>\n",
       "      <td>0.188272</td>\n",
       "      <td>0.219280</td>\n",
       "      <td>13042</td>\n",
       "      <td>7200</td>\n",
       "      <td>100</td>\n",
       "      <td>48.058998</td>\n",
       "      <td>16.376153</td>\n",
       "      <td>0.0002</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  fold  train_loss  val_loss       cor       any  epidural  \\\n",
       "0       1     0    0.495845  0.222767  0.262498  0.372876  0.077706   \n",
       "1       2     0    0.139651  0.212226  0.341014  0.371492  0.032217   \n",
       "2       3     0    0.116839  0.206248  0.416058  0.357151  0.030161   \n",
       "3       4     0    0.100432  0.248836  0.337192  0.446849  0.031468   \n",
       "4       5     0    0.101299  0.198162  0.448064  0.339129  0.027744   \n",
       "5       6     0    0.097309  0.190345  0.473876  0.327777  0.027687   \n",
       "6       7     0    0.095991  0.179739  0.478224  0.308505  0.027745   \n",
       "7       8     0    0.105157  0.162818  0.491558  0.280636  0.027615   \n",
       "8       9     0    0.101255  0.182301  0.492721  0.322835  0.027490   \n",
       "9      10     0    0.103850  0.161570  0.499187  0.276134  0.029684   \n",
       "10     13     0    0.089629  0.163397  0.510048  0.281652  0.028058   \n",
       "11     14     0    0.089327  0.160301  0.517028  0.276732  0.027610   \n",
       "12     15     0    0.088809  0.158927  0.515110  0.274000  0.027109   \n",
       "13     16     0    0.088837  0.155999  0.521167  0.270748  0.026655   \n",
       "14     17     0    0.088136  0.157832  0.523962  0.274537  0.026952   \n",
       "15     18     0    0.088043  0.155825  0.526298  0.272011  0.026415   \n",
       "16     19     0    0.086902  0.154525  0.529515  0.269470  0.026794   \n",
       "17     20     0    0.086735  0.154822  0.530893  0.270364  0.026630   \n",
       "18     21     0    0.086563  0.153274  0.531932  0.267862  0.026561   \n",
       "19     25     0    0.086106  0.364768  0.062663  0.698073  0.033167   \n",
       "20     26     0    0.138757  0.242859  0.235416  0.429052  0.034606   \n",
       "21     27     0    0.121541  0.212181  0.299105  0.364818  0.031306   \n",
       "\n",
       "    intraparenchymal  intraventricular  subarachnoid  subdural  train_sz  \\\n",
       "0           0.184072          0.147871      0.184957  0.219007     13042   \n",
       "1           0.182368          0.141474      0.176040  0.210502     13042   \n",
       "2           0.177022          0.138181      0.175438  0.208632     13042   \n",
       "3           0.198398          0.151840      0.208802  0.257649     13042   \n",
       "4           0.173676          0.136346      0.179236  0.191874     13042   \n",
       "5           0.166361          0.123394      0.165293  0.194129     13042   \n",
       "6           0.164267          0.121925      0.155056  0.172170     13042   \n",
       "7           0.150090          0.105528      0.135659  0.159565     13042   \n",
       "8           0.159502          0.115876      0.152459  0.175106     13042   \n",
       "9           0.149958          0.109171      0.133094  0.156818     13042   \n",
       "10          0.149294          0.107586      0.132725  0.162813     13042   \n",
       "11          0.147871          0.105576      0.131298  0.156291     13042   \n",
       "12          0.145623          0.105583      0.130697  0.155481     13042   \n",
       "13          0.143846          0.101793      0.126280  0.151921     13042   \n",
       "14          0.144709          0.101704      0.127705  0.154680     13042   \n",
       "15          0.142138          0.099895      0.126003  0.152305     13042   \n",
       "16          0.141182          0.098794      0.123728  0.152238     13042   \n",
       "17          0.141084          0.098386      0.123932  0.152994     13042   \n",
       "18          0.140366          0.097915      0.123196  0.149155     13042   \n",
       "19          0.267723          0.216273      0.286145  0.353920     13042   \n",
       "20          0.201127          0.150016      0.216166  0.239994     13042   \n",
       "21          0.177752          0.139025      0.188272  0.219280     13042   \n",
       "\n",
       "    val_sz   bs  train_time  valid_time      lr     wd  \n",
       "0     7200  100   80.560221  140.067585  0.0200  0.001  \n",
       "1     7200  100   80.586969  143.552010  0.0200  0.001  \n",
       "2     7200  100   81.362263  142.334380  0.0200  0.001  \n",
       "3     7200  100   80.487220  142.352772  0.0200  0.001  \n",
       "4     7200  100   84.477289  146.857369  0.0200  0.001  \n",
       "5     7200  100   81.727369  143.910612  0.0200  0.001  \n",
       "6     7200  100   81.609755  140.314667  0.0200  0.001  \n",
       "7     7200  100   81.324294  141.004881  0.0200  0.001  \n",
       "8     7200  100   81.279920  140.633701  0.0200  0.001  \n",
       "9     7200  100   81.465451  142.004258  0.0200  0.001  \n",
       "10    7200  100   79.851575  141.318649  0.0020  0.001  \n",
       "11    7200  100   80.063437  140.657684  0.0020  0.001  \n",
       "12    7200  100   80.327655  142.896167  0.0020  0.001  \n",
       "13    7200  100   80.514647  144.235245  0.0020  0.001  \n",
       "14    7200  100   80.382700  143.656591  0.0020  0.001  \n",
       "15    7200  100   80.809602  141.776692  0.0020  0.001  \n",
       "16    7200  100   80.541241  141.074992  0.0002  0.001  \n",
       "17    7200  100   80.734518  142.519661  0.0002  0.001  \n",
       "18    7200  100   80.243078  143.743049  0.0002  0.001  \n",
       "19    7200  100   64.752618   22.980388  0.0002  0.001  \n",
       "20    7200  100   47.874474   16.058632  0.0002  0.001  \n",
       "21    7200  100   48.058998   16.376153  0.0002  0.001  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv(PATH_WORK/'stats.f{}.v{}'.format(0,VERSION))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.08180776, 0.00407849, 0.02802897, 0.02118001, 0.0283712 ,\n",
       "       0.03698493], dtype=float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.mean((0,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f268e57fa90>]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(predictions.mean(0)[:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_one(bs = 100, add_seed = 0, fold = 0, anum = 0):\n",
    "    st = time.time()\n",
    "\n",
    "    cur_epoch = getCurrentBatch(fold=fold)\n",
    "    if cur_epoch is None: cur_epoch = 0\n",
    "    print('completed epochs:', cur_epoch)\n",
    "\n",
    "    model = TabularModel(n_cont = len(meta_cols), feat_sz=feat_sz)\n",
    "    \n",
    "    model_file_name = modelFileName(return_last=True, fold=fold)\n",
    "    if model_file_name is not None:\n",
    "        print('loading model', model_file_name)\n",
    "        state_dict = torch.load(PATH_WORK/'models'/model_file_name)\n",
    "        model.load_state_dict(state_dict)\n",
    "    \n",
    "    if (not CLOUD) or CLOUD_SINGLE:\n",
    "        model = model.to(device)\n",
    "    else:\n",
    "        model_parallel = dp.DataParallel(model, device_ids=devices)\n",
    "\n",
    "    setSeeds(SEED + cur_epoch + add_seed)\n",
    "\n",
    "    tst_ds = RSNA_DataSet(test_md, mode='test', bs=bs, fold=fold)\n",
    "    loader_tst = D.DataLoader(tst_ds, num_workers=8 if CLOUD else 0, batch_size=bs, shuffle=False)\n",
    "    print('dataset test:', len(tst_ds), 'loader test:', len(loader_tst))\n",
    "    \n",
    "    tst_ds.setFeats(anum)\n",
    "\n",
    "    loc_data = tst_ds.metadata.copy()\n",
    "    series_counts = loc_data.index.value_counts()\n",
    "\n",
    "    loc_data['orig_idx'] = np.arange(len(loc_data))\n",
    "    loc_data = loc_data.sort_values(['SeriesInstanceUID','pos_idx'])\n",
    "    loc_data['my_order'] = np.arange(len(loc_data))\n",
    "    loc_data = loc_data.sort_values(['orig_idx'])\n",
    "    \n",
    "    if CLOUD and (not CLOUD_SINGLE):\n",
    "        results = model_parallel(test_loop_fn, loader_tst)\n",
    "        predictions = np.concatenate([results[i][0] for i in range(MAX_DEVICES)])\n",
    "        indices = np.concatenate([results[i][1] for i in range(MAX_DEVICES)])\n",
    "        offsets = np.concatenate([results[i][2] for i in range(MAX_DEVICES)])\n",
    "    else:\n",
    "        predictions, indices, offsets = test_loop_fn(model, loader_tst, device)\n",
    "\n",
    "    predictions = predictions[np.argsort(indices)]\n",
    "    offsets = offsets[np.argsort(indices)]\n",
    "    assert len(predictions) == len(test_md.SeriesInstanceUID.unique())\n",
    "    assert np.all(indices[np.argsort(indices)] == np.array(range(len(predictions))))\n",
    "    \n",
    "    val_results = []\n",
    "    for k, series in enumerate(np.sort(loc_data.index.unique())):\n",
    "        cnt = series_counts[series]\n",
    "        assert (offsets[k] + cnt) <= 60\n",
    "        val_results.append(predictions[k,offsets[k]:(offsets[k] + cnt)])\n",
    "\n",
    "    val_results = np.concatenate(val_results)\n",
    "    assert np.isnan(val_results).sum() == 0\n",
    "    val_results = val_results[loc_data.my_order]\n",
    "    assert len(val_results) == len(loc_data)\n",
    "\n",
    "    print('test processing time:', time.time() - st)\n",
    "    \n",
    "    return val_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERSION = 36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed epochs: 17\n",
      "loading model model.b17.f0.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 0\n",
      "test processing time: 4.716512441635132\n",
      "completed epochs: 17\n",
      "loading model model.b17.f0.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 1\n",
      "test processing time: 4.641012907028198\n",
      "completed epochs: 17\n",
      "loading model model.b17.f0.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 2\n",
      "test processing time: 4.021617650985718\n",
      "completed epochs: 17\n",
      "loading model model.b17.f0.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 3\n",
      "test processing time: 4.19426155090332\n",
      "completed epochs: 17\n",
      "loading model model.b17.f0.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 4\n",
      "test processing time: 4.01205849647522\n",
      "completed epochs: 17\n",
      "loading model model.b17.f0.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 5\n",
      "test processing time: 3.958509683609009\n",
      "completed epochs: 17\n",
      "loading model model.b17.f0.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 6\n",
      "test processing time: 3.920159101486206\n",
      "completed epochs: 17\n",
      "loading model model.b17.f0.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 0\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 7\n",
      "test processing time: 3.898054838180542\n",
      "completed epochs: 16\n",
      "loading model model.b16.f1.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 0\n",
      "test processing time: 3.989717721939087\n",
      "completed epochs: 16\n",
      "loading model model.b16.f1.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 1\n",
      "test processing time: 3.92999267578125\n",
      "completed epochs: 16\n",
      "loading model model.b16.f1.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 2\n",
      "test processing time: 3.8665740489959717\n",
      "completed epochs: 16\n",
      "loading model model.b16.f1.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 3\n",
      "test processing time: 3.8939690589904785\n",
      "completed epochs: 16\n",
      "loading model model.b16.f1.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 4\n",
      "test processing time: 3.884352684020996\n",
      "completed epochs: 16\n",
      "loading model model.b16.f1.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 5\n",
      "test processing time: 4.021401882171631\n",
      "completed epochs: 16\n",
      "loading model model.b16.f1.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 6\n",
      "test processing time: 3.8955178260803223\n",
      "completed epochs: 16\n",
      "loading model model.b16.f1.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 1\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 7\n",
      "test processing time: 3.875286340713501\n",
      "completed epochs: 17\n",
      "loading model model.b17.f2.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 0\n",
      "test processing time: 3.907681941986084\n",
      "completed epochs: 17\n",
      "loading model model.b17.f2.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 1\n",
      "test processing time: 3.8861751556396484\n",
      "completed epochs: 17\n",
      "loading model model.b17.f2.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 2\n",
      "test processing time: 3.878710985183716\n",
      "completed epochs: 17\n",
      "loading model model.b17.f2.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 3\n",
      "test processing time: 3.9212164878845215\n",
      "completed epochs: 17\n",
      "loading model model.b17.f2.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 4\n",
      "test processing time: 3.9299263954162598\n",
      "completed epochs: 17\n",
      "loading model model.b17.f2.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 5\n",
      "test processing time: 3.9006669521331787\n",
      "completed epochs: 17\n",
      "loading model model.b17.f2.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 6\n",
      "test processing time: 3.877828359603882\n",
      "completed epochs: 17\n",
      "loading model model.b17.f2.v36\n",
      "adding dummy serieses 186\n",
      "DataSet 2 test size 2400 fold 2\n",
      "dataset test: 2400 loader test: 24\n",
      "setFeats, augmentation 7\n",
      "test processing time: 3.8699746131896973\n"
     ]
    }
   ],
   "source": [
    "preds = []\n",
    "for fold in range(3):\n",
    "    preds2 = []\n",
    "    for anum in range(8):\n",
    "        predictions = inference_one(fold = fold, anum = anum)\n",
    "        preds2.append(predictions)\n",
    "    preds.append(np.stack(preds2))\n",
    "preds = np.stack(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = np.exp(np.log(preds).mean((0,1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = 1 / (1 + np.exp(-(np.log(preds/(1-preds)).mean((0,1)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12515758, 0.00611261, 0.04398553, 0.03232377, 0.04397949,\n",
       "       0.05341499], dtype=float32)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1264937 , 0.00635758, 0.04458478, 0.03210059, 0.0448006 ,\n",
       "       0.05447438], dtype=float32)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.12421313, 0.00613318, 0.04336379, 0.03132349, 0.04353321,\n",
       "       0.05299766], dtype=float32)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_column = np.array([a + '_' + b for a in test_md.SOPInstanceUID for b in all_ich])\n",
    "sub = pd.DataFrame({'ID': id_column, 'Label': predictions.reshape(-1)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12515757977962494"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.loc[range(0,len(sub),6), 'Label'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12421312928199768"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sub.loc[range(0,len(sub),6), 'Label'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_sub = pd.read_csv(PATH/'submission_061.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13475628267250275"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_sub.loc[range(0,len(sub),6), 'Label'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.to_csv(PATH/'sub.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9878769168485573"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.corrcoef(sub.sort_values('ID').reset_index(drop=True).loc[range(0,len(sub),6), 'Label'], \n",
    "            best_sub.sort_values('ID').reset_index(drop=True).loc[range(0,len(sub),6), 'Label'])[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████| 16.6M/16.6M [00:05<00:00, 3.16MB/s]\n",
      "Successfully submitted to RSNA Intracranial Hemorrhage Detection"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit rsna-intracranial-hemorrhage-detection -f ~/sub.csv -m \"TPU, densenet169, 4aug, 8TTA, 3folds, pre-sigmoid\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
