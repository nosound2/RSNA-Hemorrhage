{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "from matplotlib import patches, patheffects\n",
    "import time\n",
    "from random import randint\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error,log_loss\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "import pdb\n",
    "\n",
    "import scipy as sp\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import torch\n",
    "torch.cuda.current_device()\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as D\n",
    "import torch.nn.functional as F\n",
    "import torch.utils as U\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms as T\n",
    "from torchvision import models as M\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "PATH = Path('C:/StudioProjects/Hemorrhage')\n",
    "PATH_WORK = Path('C:/StudioProjects/Hemorrhage/running')\n",
    "\n",
    "import lightgbm as lgb\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "import seaborn as sn\n",
    "from eli5.permutation_importance import get_score_importances\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "\n",
    "all_ich = ['any','epidural','intraparenchymal','intraventricular','subarachnoid','subdural']\n",
    "\n",
    "sys.path.insert(0, \"C:\\\\fastai\")\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from fastai.tabular import *\n",
    "from fastprogress import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SMALL = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_cat, cols_float = pickle.load(open(PATH_WORK/'covs','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = PATH_WORK/'yuvals_indexes_file.pkl'\n",
    "all_idx, train_ids, val_ids = pickle.load(open(filename,'rb'))\n",
    "\n",
    "train_md = pd.read_csv(PATH_WORK/'train_md.csv').sort_values(['SeriesInstanceUID','pos_idx'])\n",
    "train_md['img_id'] = train_md.SOPInstanceUID.str.split('_').apply(lambda x: x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "trn_data = train_md.loc[train_md.img_id.isin(all_idx[train_ids])]\n",
    "val_data = train_md.loc[train_md.img_id.isin(all_idx[val_ids])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(trn_data.SeriesInstanceUID.unique()) + len(val_data.SeriesInstanceUID.unique()) \\\n",
    "    == len(train_md.SeriesInstanceUID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(trn_data.PatientID.unique()) + len(val_data.PatientID.unique()) \\\n",
    "    >= len(train_md.PatientID.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids_df = pd.DataFrame(all_idx, columns = ['img_id'])\n",
    "ids_df = ids_df.join(train_md[['img_id','SeriesInstanceUID','pos_idx']].set_index('img_id'), on = 'img_id')\n",
    "\n",
    "assert len(ids_df.SeriesInstanceUID.unique()) == 19530"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "filename = PATH_WORK/'yuvals_model_Densenet161_3_vehrsion_basic_classifier_type_features_train_split_2.pkl'\n",
    "feats = pickle.load(open(filename,'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 19530/19530 [25:29<00:00, 12.77it/s]\n"
     ]
    }
   ],
   "source": [
    "for series_id in tqdm(ids_df.SeriesInstanceUID.unique()):\n",
    "    mask = torch.BoolTensor(ids_df.SeriesInstanceUID.values == series_id)\n",
    "    feats_id = feats[mask]\n",
    "    pickle.dump(feats_id, open(PATH_WORK/'features/densenet161_v3/{}'.format(series_id),'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RSNA_DataSet(D.Dataset):\n",
    "    def __init__(self, metadata, ids_df, mode='train'):\n",
    "        \n",
    "        super(RSNA_DataSet, self).__init__()\n",
    "        \n",
    "        #self.records = df.to_records(index=False)\n",
    "        self.mode = mode\n",
    "        self.series = metadata.SeriesInstanceUID.unique()\n",
    "        self.metadata = metadata\n",
    "        self.ids_df = ids_df\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        series_id = self.series[index]\n",
    "        df = self.metadata.loc[self.metadata.SeriesInstanceUID == series_id]\n",
    "        \n",
    "        path = PATH_WORK/'features/densenet161_v3/{}'.format(series_id)\n",
    "        feats = pickle.load(open(path,'rb'))\n",
    "        ids_df_sub = ids_df.loc[ids_df.SeriesInstanceUID.values == series_id]\n",
    "        \n",
    "        if feats.shape[0] > len(df):\n",
    "            mask_dup = ~ids_df_sub.img_id.duplicated().values\n",
    "            ids_df_sub = ids_df_sub.loc[mask_dup]\n",
    "            feats = feats[torch.BoolTensor(mask_dup)]\n",
    "        \n",
    "        assert feats.shape[0] == len(df)\n",
    "        assert len(ids_df_sub) == len(df)\n",
    "        assert np.all(ids_df_sub.img_id.isin(df.img_id).values)\n",
    "        order = np.argsort(ids_df_sub.pos_idx.values)\n",
    "        assert np.all(ids_df_sub.img_id.values[order] == df.img_id.values)\n",
    "        feats = feats[torch.LongTensor(order)]\n",
    "        \n",
    "        feats = torch.cat([feats, torch.Tensor(df[cols_cat + cols_float].values)], dim=1)\n",
    "        feats = torch.cat([feats, torch.zeros((60 - feats.shape[0], feats.shape[1]))], dim=0)\n",
    "        \n",
    "        #target = torch.cat([torch.IntTensor(df[all_ich].values), \n",
    "        #                    torch.zeros((60 - len(df), len(all_ich)), dtype=torch.int32)], dim=0)\n",
    "        target = torch.cat([torch.Tensor(df[all_ich].values), \n",
    "                            torch.zeros((60 - len(df), len(all_ich)))], dim=0)\n",
    "        \n",
    "        return feats.transpose(1,0), target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.series) if not DATA_SMALL else int(0.01*len(self.series))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _Loss(Module):\n",
    "    def __init__(self, size_average=None, reduce=None, reduction='mean'):\n",
    "        super(_Loss, self).__init__()\n",
    "        if size_average is not None or reduce is not None:\n",
    "            self.reduction = _Reduction.legacy_get_string(size_average, reduce)\n",
    "        else:\n",
    "            self.reduction = reduction\n",
    "\n",
    "class BCEWithLogitsLoss(_Loss):\n",
    "    __constants__ = ['weight', 'pos_weight', 'reduction']\n",
    "\n",
    "    def __init__(self, weight=None, size_average=None, reduce=None, reduction='mean', pos_weight=None):\n",
    "        super(BCEWithLogitsLoss, self).__init__(size_average, reduce, reduction)\n",
    "        self.register_buffer('weight', weight)\n",
    "        self.register_buffer('pos_weight', pos_weight)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return F.binary_cross_entropy_with_logits(input.squeeze(), target,\n",
    "                                                  self.weight,\n",
    "                                                  pos_weight=self.pos_weight,\n",
    "                                                  reduction=self.reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatProduct(nn.Module):\n",
    "    def __init__(self, in_feature, out_feature):\n",
    "        super(FeatProduct, self).__init__()\n",
    "        self.in_feature = in_feature\n",
    "        self.out_feature = out_feature\n",
    "        self.weight = nn.Parameter(torch.Tensor(out_feature, in_feature))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = F.linear(x, self.weight)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularModel(Module):\n",
    "    \"Basic model for tabular data.\"\n",
    "    def __init__(self, n_cont:int, out_sz:int, layers:Collection[int], ps:Collection[float]=None,\n",
    "                 emb_drop:float=0., use_bn:bool=True, bn_final:bool=False, feat_sz=2208):\n",
    "        super().__init__()\n",
    "        ps = ifnone(ps, [0]*len(layers))\n",
    "        ps = listify(ps, layers)\n",
    "        self.bn_cont = nn.BatchNorm1d(feat_sz + n_cont)\n",
    "        self.n_cont = n_cont\n",
    "        sizes = self.get_sizes(layers, out_sz)\n",
    "        actns = [nn.ReLU(inplace=True) for _ in range(len(sizes)-2)] + [None]\n",
    "        layers = []\n",
    "        for i,(n_in,n_out,dp,act) in enumerate(zip(sizes[:-1],sizes[1:],[0.]+ps,actns)):\n",
    "            layers += bn_drop_lin(n_in, n_out, bn=use_bn and i!=0, p=dp, actn=act)\n",
    "        if bn_final: layers.append(nn.BatchNorm1d(sizes[-1]))\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "        self.feat_product = FeatProduct(feat_sz + n_cont, 20)\n",
    "\n",
    "    def get_sizes(self, layers, out_sz):\n",
    "        return [1200] + layers + [out_sz]\n",
    "\n",
    "    def forward(self, x) -> Tensor:\n",
    "        x = self.bn_cont(x)\n",
    "        x = x.transpose(1,2)\n",
    "        x = self.feat_product(x)\n",
    "        x = x.reshape(x.shape[0],-1)\n",
    "        x = self.layers(x)\n",
    "        x = x.reshape(x.shape[0],60,6)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(weight = None):\n",
    "    \n",
    "    sampler = None\n",
    "    if False:\n",
    "        ww = torch.DoubleTensor(wt)\n",
    "        sampler = torch.utils.data.sampler.WeightedRandomSampler(ww, len(ww), replacement=True)\n",
    "    \n",
    "    trn_ds = RSNA_DataSet(trn_data, ids_df, mode='train')\n",
    "    val_ds = RSNA_DataSet(val_data, ids_df, mode='valid')\n",
    "    df = DataBunch.create(train_ds=trn_ds, valid_ds=val_ds, bs=100, num_workers=0)\n",
    "    \n",
    "    tab_model = TabularModel(n_cont = len(cols_float) + len(cols_cat), out_sz=360, \\\n",
    "                             layers=[500,200], ps=[0.5,0.5], bn_final=True)\n",
    "    model = Learner(df, tab_model, path=PATH_WORK, loss_func=BCEWithLogitsLoss())#.mixup()\n",
    "    model.fit(2, 1e-1, wd=5e-3)\n",
    "\n",
    "    predictions = np.array(model.get_preds(ds_type=DatasetType.Valid)[0])\n",
    "    #model.data.add_test(df_test)\n",
    "    #predictions[data_filt['fold'] == i] = model.get_preds(ds_type=DatasetType.Test)[0].reshape(-1)\n",
    "    \n",
    "    val_sz = len(val_data.SeriesInstanceUID.unique())\n",
    "    if DATA_SMALL: val_sz = int(0.01*val_sz)\n",
    "    val_series = val_data.SeriesInstanceUID.unique()[:val_sz]\n",
    "    val_target = np.zeros((val_sz,60,6))\n",
    "    for i, series in enumerate(val_series):\n",
    "        mask = val_data.SeriesInstanceUID == series\n",
    "        val_target[i,:mask.sum()] = val_data.loc[mask, all_ich]\n",
    "    \n",
    "    print('Log-loss', log_loss(val_target.reshape(-1),predictions.reshape(-1),eps=1e-6))\n",
    "    print('correlation', np.corrcoef(val_target.reshape(-1),predictions.reshape(-1))[0,1])\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='2', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/2 00:00<00:00]\n",
       "    </div>\n",
       "    \n",
       "<table style='width:300px; margin-bottom:10px'>\n",
       "  <tr>\n",
       "    <th>epoch</th>\n",
       "    <th>train_loss</th>\n",
       "    <th>valid_loss</th>\n",
       "    <th>time</th>\n",
       "  </tr>\n",
       "</table>\n",
       "\n",
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1', style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_one()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
