{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/reina/anaconda3/envs/RSNA/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n",
      "/home/reina/anaconda3/envs/RSNA/lib/python3.6/importlib/_bootstrap.py:219: ImportWarning: can't resolve package from __spec__ or __package__, falling back on __name__ and __path__\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import os\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "import pydicom\n",
    "import time\n",
    "import gc\n",
    "import operator \n",
    "from apex import amp \n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as D\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import KFold\n",
    "from tqdm import tqdm, tqdm_notebook\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "import warnings\n",
    "warnings.filterwarnings(action='once')\n",
    "import pickle\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "from skimage.io import imread,imshow\n",
    "from helper import *\n",
    "from apex import amp\n",
    "import helper\n",
    "import torchvision.models as models\n",
    "from torch.optim import Adam\n",
    "from defenitions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 22428429\n",
    "device=device_by_name(\"Tesla\")\n",
    "#device=device_by_name(\"RTX\")\n",
    "#device = \"cpu\"\n",
    "sendmeemail=Email_Progress(my_gmail,my_pass,to_email,'Densenet161-Copy2-3 results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_submission(test_df,pred):\n",
    "    epidural_df=pd.DataFrame(data={'ID':'ID_'+test_df.PatientID.values+'_epidural','Label':torch.sigmoid(pred[:,0])})\n",
    "    intraparenchymal_df=pd.DataFrame(data={'ID':'ID_'+test_df.PatientID.values+'_intraparenchymal','Label':torch.sigmoid(pred[:,1])})\n",
    "    intraventricular_df=pd.DataFrame(data={'ID':'ID_'+test_df.PatientID.values+'_intraventricular','Label':torch.sigmoid(pred[:,2])})\n",
    "    subarachnoid_df=pd.DataFrame(data={'ID':'ID_'+test_df.PatientID.values+'_subarachnoid','Label':torch.sigmoid(pred[:,3])})\n",
    "    subdural_df=pd.DataFrame(data={'ID':'ID_'+test_df.PatientID.values+'_subdural','Label':torch.sigmoid(pred[:,4])})\n",
    "    any_df=pd.DataFrame(data={'ID':'ID_'+test_df.PatientID.values+'_any','Label':torch.sigmoid(pred[:,5])}) \n",
    "    return pd.concat([epidural_df,\n",
    "                        intraparenchymal_df,\n",
    "                        intraventricular_df,\n",
    "                        subarachnoid_df,\n",
    "                        subdural_df,\n",
    "                        any_df]).sort_values('ID').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PatientID</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "      <th>any</th>\n",
       "      <th>PID</th>\n",
       "      <th>StudyI</th>\n",
       "      <th>SeriesI</th>\n",
       "      <th>WindowCenter</th>\n",
       "      <th>WindowWidth</th>\n",
       "      <th>ImagePositionZ</th>\n",
       "      <th>ImagePositionX</th>\n",
       "      <th>ImagePositionY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>63eb1e259</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>a449357f</td>\n",
       "      <td>62d125e5b2</td>\n",
       "      <td>0be5c0d1b3</td>\n",
       "      <td>['00036', '00036']</td>\n",
       "      <td>['00080', '00080']</td>\n",
       "      <td>180.199951</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>-8.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2669954a7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>363d5865</td>\n",
       "      <td>a20b80c7bf</td>\n",
       "      <td>3564d584db</td>\n",
       "      <td>['00047', '00047']</td>\n",
       "      <td>['00080', '00080']</td>\n",
       "      <td>922.530821</td>\n",
       "      <td>-156.0</td>\n",
       "      <td>45.572849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>52c9913b1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9c2b4bd7</td>\n",
       "      <td>3e3634f8cf</td>\n",
       "      <td>973274ffc9</td>\n",
       "      <td>40</td>\n",
       "      <td>150</td>\n",
       "      <td>4.455000</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>-115.063000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4e6ff6126</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3ae81c2d</td>\n",
       "      <td>a1390c15c2</td>\n",
       "      <td>e5ccad8244</td>\n",
       "      <td>['00036', '00036']</td>\n",
       "      <td>['00080', '00080']</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>-99.5</td>\n",
       "      <td>28.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7858edd88</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>c1867feb</td>\n",
       "      <td>c73e81ed3a</td>\n",
       "      <td>28e0531b3a</td>\n",
       "      <td>40</td>\n",
       "      <td>100</td>\n",
       "      <td>145.793000</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>-132.190000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PatientID  epidural  intraparenchymal  intraventricular  subarachnoid  \\\n",
       "0  63eb1e259         0                 0                 0             0   \n",
       "1  2669954a7         0                 0                 0             0   \n",
       "2  52c9913b1         0                 0                 0             0   \n",
       "3  4e6ff6126         0                 0                 0             0   \n",
       "4  7858edd88         0                 0                 0             0   \n",
       "\n",
       "   subdural  any       PID      StudyI     SeriesI        WindowCenter  \\\n",
       "0         0    0  a449357f  62d125e5b2  0be5c0d1b3  ['00036', '00036']   \n",
       "1         0    0  363d5865  a20b80c7bf  3564d584db  ['00047', '00047']   \n",
       "2         0    0  9c2b4bd7  3e3634f8cf  973274ffc9                  40   \n",
       "3         0    0  3ae81c2d  a1390c15c2  e5ccad8244  ['00036', '00036']   \n",
       "4         0    0  c1867feb  c73e81ed3a  28e0531b3a                  40   \n",
       "\n",
       "          WindowWidth  ImagePositionZ  ImagePositionX  ImagePositionY  \n",
       "0  ['00080', '00080']      180.199951          -125.0       -8.000000  \n",
       "1  ['00080', '00080']      922.530821          -156.0       45.572849  \n",
       "2                 150        4.455000          -125.0     -115.063000  \n",
       "3  ['00080', '00080']      100.000000           -99.5       28.500000  \n",
       "4                 100      145.793000          -125.0     -132.190000  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(data_dir+'train.csv')\n",
    "train_df=train_df[~train_df.PatientID.isin(bad_images)].reset_index(drop=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17079,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(19530,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.PID.unique().shape\n",
    "train_df.StudyI.unique().shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PatientID</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "      <th>any</th>\n",
       "      <th>SeriesI</th>\n",
       "      <th>PID</th>\n",
       "      <th>StudyI</th>\n",
       "      <th>WindowCenter</th>\n",
       "      <th>WindowWidth</th>\n",
       "      <th>ImagePositionZ</th>\n",
       "      <th>ImagePositionX</th>\n",
       "      <th>ImagePositionY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28fbab7eb</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>ebfd7e4506</td>\n",
       "      <td>cf1b6b11</td>\n",
       "      <td>93407cadbb</td>\n",
       "      <td>30</td>\n",
       "      <td>80</td>\n",
       "      <td>158.458000</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>-135.598000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>877923b8b</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>6d95084e15</td>\n",
       "      <td>ad8ea58f</td>\n",
       "      <td>a337baa067</td>\n",
       "      <td>30</td>\n",
       "      <td>80</td>\n",
       "      <td>138.729050</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>-101.797981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a591477cb</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>8e06b2c9e0</td>\n",
       "      <td>ecfb278b</td>\n",
       "      <td>0cfe838d54</td>\n",
       "      <td>30</td>\n",
       "      <td>80</td>\n",
       "      <td>60.830002</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>-133.300003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42217c898</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>e800f419cf</td>\n",
       "      <td>e96e31f4</td>\n",
       "      <td>c497ac5bad</td>\n",
       "      <td>30</td>\n",
       "      <td>80</td>\n",
       "      <td>55.388000</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>-146.081000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a130c4d2f</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>faeb7454f3</td>\n",
       "      <td>69affa42</td>\n",
       "      <td>854e4fbc01</td>\n",
       "      <td>30</td>\n",
       "      <td>80</td>\n",
       "      <td>33.516888</td>\n",
       "      <td>-125.0</td>\n",
       "      <td>-118.689819</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PatientID  epidural  intraparenchymal  intraventricular  subarachnoid  \\\n",
       "0  28fbab7eb       0.5               0.5               0.5           0.5   \n",
       "1  877923b8b       0.5               0.5               0.5           0.5   \n",
       "2  a591477cb       0.5               0.5               0.5           0.5   \n",
       "3  42217c898       0.5               0.5               0.5           0.5   \n",
       "4  a130c4d2f       0.5               0.5               0.5           0.5   \n",
       "\n",
       "   subdural  any     SeriesI       PID      StudyI WindowCenter WindowWidth  \\\n",
       "0       0.5  0.5  ebfd7e4506  cf1b6b11  93407cadbb           30          80   \n",
       "1       0.5  0.5  6d95084e15  ad8ea58f  a337baa067           30          80   \n",
       "2       0.5  0.5  8e06b2c9e0  ecfb278b  0cfe838d54           30          80   \n",
       "3       0.5  0.5  e800f419cf  e96e31f4  c497ac5bad           30          80   \n",
       "4       0.5  0.5  faeb7454f3  69affa42  854e4fbc01           30          80   \n",
       "\n",
       "   ImagePositionZ  ImagePositionX  ImagePositionY  \n",
       "0      158.458000          -125.0     -135.598000  \n",
       "1      138.729050          -125.0     -101.797981  \n",
       "2       60.830002          -125.0     -133.300003  \n",
       "3       55.388000          -125.0     -146.081000  \n",
       "4       33.516888          -125.0     -118.689819  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = pd.read_csv(data_dir+'test.csv')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.13292910447761194"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(train_df.PID.unique()).intersection(set(test_df.PID.unique())))/test_df.PID.unique().shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-79f2b4a770c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msplit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeriesI\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSEED\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msplit_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0msplit_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "split = train_df.SeriesI.unique()\n",
    "np.random.seed(SEED)\n",
    "np.random.shuffle(split)\n",
    "split_train=split[:int(0.9*split.shape[0])]\n",
    "split_val=split[int(0.9*split.shape[0]):]\n",
    "\n",
    "idx_train = train_df[train_df.SeriesI.isin(set(split_train))].index.values\n",
    "idx_validate = train_df[train_df.SeriesI.isin(set(split_val))].index.values\n",
    "idx_train.shape\n",
    "idx_validate.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_loss(y_pred,y_true,weights):\n",
    "    if len(y_pred.shape)==len(y_true.shape):\n",
    "        loss = F.binary_cross_entropy_with_logits(y_pred,y_true,weights.repeat(y_pred.shape[0],1))\n",
    "    else:\n",
    "        loss0 = F.binary_cross_entropy_with_logits(y_pred,y_true[...,0],weights.repeat(y_pred.shape[0],1),reduction='none')\n",
    "        loss1 = F.binary_cross_entropy_with_logits(y_pred,y_true[...,1],weights.repeat(y_pred.shape[0],1),reduction='none')\n",
    "        loss = (y_true[...,2]*loss0+(1.0-y_true[...,2])*loss1).mean() \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class parameter_scheduler():\n",
    "    def __init__(self,model,do_first=['classifier'],num_epoch=1):\n",
    "        self.model=model\n",
    "        self.do_first = do_first\n",
    "        self.num_epoch=num_epoch\n",
    "    def __call__(self,epoch):\n",
    "        if epoch>=self.num_epoch:\n",
    "            for n,p in self.model.named_parameters():\n",
    "                p.requires_grad=True\n",
    "        else:\n",
    "            for n,p in self.model.named_parameters():\n",
    "                p.requires_grad= any(nd in n for nd in self.do_first)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_optimizer_parameters(model,klr):\n",
    "    zero_layer=['conv0','norm0','ws_norm']\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    num_blocks=4\n",
    "    no_decay=['bias']\n",
    "    optimizer_grouped_parameters=[\n",
    "        {'params': [p for n, p in param_optimizer if (not any(nd in n for nd in no_decay) and any(nd in n for nd in zero_layer))], 'lr':klr*2e-5,'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)  and any(nd in n for nd in zero_layer)], 'lr':klr*2e-5, 'weight_decay': 0.0}\n",
    "        ]\n",
    "    optimizer_grouped_parameters.extend([\n",
    "        {'params': [p for n, p in param_optimizer if (not any(nd in n for nd in no_decay) and ('wso' in n))], 'lr':klr*1e-5,'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)  and ('wso' in n)], 'lr':klr*1e-5, 'weight_decay': 0.0}\n",
    "        ])\n",
    "    optimizer_grouped_parameters.extend([\n",
    "        {'params': [p for n, p in param_optimizer if (not any(nd in n for nd in no_decay) and ('classifier' in n))], 'lr':klr*1e-3,'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)  and ('classifier' in n)], 'lr':klr*1e-3, 'weight_decay': 0.0}\n",
    "        ])\n",
    "    for i in range(num_blocks):\n",
    "        optimizer_grouped_parameters.extend([\n",
    "        {'params': [p for n, p in param_optimizer if (not any(nd in n for nd in no_decay) and ('denseblock{}'.format(i+1) in n))], 'lr':klr*(2.0**i)*2e-5,'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)  and ('denseblock{}'.format(i+1) in n)], 'lr':klr*(2.0**i)*2e-5, 'weight_decay': 0.0}\n",
    "        ])\n",
    "    optimizer_grouped_parameters.extend([\n",
    "        {'params': [p for n, p in param_optimizer if (not any(nd in n for nd in no_decay) and ('norm5' in n))], 'lr':klr*1e-4,'weight_decay': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)  and ('norm5' in n)], 'lr':klr*1e-4, 'weight_decay': 0.0}\n",
    "        ])\n",
    "    return(optimizer_grouped_parameters)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "\n",
    "num_split=2\n",
    "np.random.seed(SEED+num_split)\n",
    "torch.manual_seed(SEED+num_split)\n",
    "torch.cuda.manual_seed(SEED+num_split)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "klr=1\n",
    "batch_size=32\n",
    "num_workers=12\n",
    "num_epochs=2\n",
    "model_name,version = 'Densenet161_3' , 'basic_wso'\n",
    "model = MyDenseNet(models.densenet161(pretrained=True),\n",
    "                   len(hemorrhage_types),\n",
    "                   num_channels=3,\n",
    "                   drop_out=0.2,\n",
    "                   wso=((40,80),(80,200),(600,2800)),\n",
    "                   strategy='none',\n",
    "                   dont_do_grad=[],\n",
    "                   pool_type='max',\n",
    "                   extra_pool=4,\n",
    "                   )\n",
    "\n",
    "_=model.to(device)\n",
    "weights = torch.tensor([1.,1.,1.,1.,1.,2.],device=device)\n",
    "loss_func=my_loss\n",
    "targets_dataset=D.TensorDataset(torch.tensor(train_df[hemorrhage_types].values,dtype=torch.float))\n",
    "transform=MyTransform(flip=True,zoom=0.05,rotate=15,out_size=512,shift=40)\n",
    "imagedataset = ImageDataset(train_df,transform=transform.random,base_path=train_images_dir,\n",
    "                           window_eq=False,equalize=False,rescale=True)\n",
    "transform_val=MyTransform(out_size=512)\n",
    "imagedataset_val = ImageDataset(train_df,transform=transform_val.random,base_path=train_images_dir,\n",
    "                               window_eq=False,equalize=False,rescale=True)\n",
    "combined_dataset=DatasetCat([imagedataset,targets_dataset])\n",
    "combined_dataset_val=DatasetCat([imagedataset_val,targets_dataset])\n",
    "#param_s=parameter_scheduler(model,num_epoch=0)\n",
    "optimizer_grouped_parameters=get_optimizer_parameters(model,klr)\n",
    "sampling=sampler(train_df[hemorrhage_types].values[idx_train],0.2,[10,1,1,1,1,0])\n",
    "sample_ratio=1.02*float(sampling().shape[0])/idx_train.shape[0]\n",
    "train_dataset=D.Subset(combined_dataset,idx_train)\n",
    "validate_dataset=D.Subset(combined_dataset_val,idx_validate)\n",
    "num_train_optimization_steps = num_epochs*(sample_ratio*len(train_dataset)//batch_size+int(len(train_dataset)%batch_size>0))\n",
    "fig,ax = plt.subplots(figsize=(10,7))\n",
    "gr=loss_graph(fig,ax,num_epochs,int(num_train_optimization_steps/num_epochs)+1,limits=[0.05,0.4])\n",
    "sched=WarmupExpCosineWithWarmupRestartsSchedule( t_total=num_train_optimization_steps, cycles=num_epochs//2,tau=1)\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,lr=klr*1e-3,schedule=sched)\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\n",
    "history,best_model= model_train(model,\n",
    "                                optimizer,\n",
    "                                train_dataset,\n",
    "                                batch_size,\n",
    "                                num_epochs,\n",
    "                                loss_func,\n",
    "                                weights=weights,\n",
    "                                do_apex=False,\n",
    "                                model_apexed=True,\n",
    "                                validate_dataset=validate_dataset,\n",
    "                                param_schedualer=None,\n",
    "                                weights_data=None,\n",
    "                                metric=None,\n",
    "                                return_model=True,\n",
    "                                num_workers=num_workers,\n",
    "                                sampler=sampling,\n",
    "                                graph=gr,\n",
    "                                call_progress=sendmeemail)\n",
    "torch.save(model.state_dict(), models_dir+models_format.format(model_name,version,num_split))\n",
    "torch.save(best_model.state_dict(), models_dir+models_format.format(model_name,version+'_best',num_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "\n",
    "num_split=2\n",
    "np.random.seed(SEED+num_split)\n",
    "torch.manual_seed(SEED+num_split)\n",
    "torch.cuda.manual_seed(SEED+num_split)\n",
    "#torch.backends.cudnn.deterministic = True\n",
    "klr=1\n",
    "batch_size=32\n",
    "num_workers=12\n",
    "num_epochs=8\n",
    "model_name,version = 'Densenet161_3' , 'basic_classifier_wso2'\n",
    "#model1 = MyDenseNet(models.densenet161(pretrained=True),len(hemorrhage_types),num_channels=3)\n",
    "model = MyDenseNet(models.densenet161(pretrained=True),\n",
    "                   len(hemorrhage_types),\n",
    "                   num_channels=3,\n",
    "                   drop_out=0.2,\n",
    "                   wso=((40,80),(80,200),(600,2800)),\n",
    "                   strategy='none',\n",
    "                   dont_do_grad=[],\n",
    "                   extra_pool=4,\n",
    "                   pool_type='max'\n",
    "                   )\n",
    "model.load_state_dict(torch.load(models_dir+models_format.format(model_name,'basic_wso',num_split),map_location=torch.device(device)))\n",
    "_=model.to(device)\n",
    "weights = torch.tensor([1.,1.,1.,1.,1.,2.],device=device)\n",
    "loss_func=my_loss\n",
    "targets_dataset=D.TensorDataset(torch.tensor(train_df[hemorrhage_types].values,dtype=torch.float))\n",
    "transform=MyTransform(flip=True,zoom=0.05,rotate=15,out_size=512,shift=40)\n",
    "imagedataset = ImageDataset(train_df,transform=transform.random,base_path=train_images_dir,\n",
    "                           window_eq=False,equalize=False,rescale=True)\n",
    "transform_val=MyTransform(out_size=512)\n",
    "imagedataset_val = ImageDataset(train_df,transform=transform_val.random,base_path=train_images_dir,\n",
    "                               window_eq=False,equalize=False,rescale=True)\n",
    "combined_dataset=DatasetCat([imagedataset,targets_dataset])\n",
    "combined_dataset_val=DatasetCat([imagedataset_val,targets_dataset])\n",
    "#param_s=parameter_scheduler(model,num_epoch=0)\n",
    "pre_proc = Mixup(0.4,device=device)\n",
    "optimizer_grouped_parameters=get_optimizer_parameters(model,klr)\n",
    "sampling=simple_sampler(train_df[hemorrhage_types].values[idx_train],0.25)\n",
    "#sampling=sampler(train_df[hemorrhage_types].values[idx_train],5,[0,0,0,0,0,2],train_df.SeriesI.values[idx_train])\n",
    "sample_ratio= 0.25 #1.003*float(sampling().shape[0])/idx_train.shape[0]\n",
    "train_dataset=D.Subset(combined_dataset,idx_train)\n",
    "validate_dataset=D.Subset(combined_dataset_val,idx_validate)\n",
    "num_train_optimization_steps = num_epochs*(sample_ratio*len(train_dataset)//batch_size+int(len(train_dataset)%batch_size>0))\n",
    "fig,ax = plt.subplots(figsize=(10,7))\n",
    "gr=loss_graph(fig,ax,num_epochs,int(num_train_optimization_steps/num_epochs)+1,limits=(0.05,0.2))\n",
    "sched=WarmupExpCosineWithWarmupRestartsSchedule( t_total=num_train_optimization_steps, cycles=num_epochs//2,tau=1)\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,lr=klr*1e-3,schedule=sched)\n",
    "model, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\",verbosity=0)\n",
    "history,best_model= model_train(model,\n",
    "                                optimizer,\n",
    "                                train_dataset,\n",
    "                                batch_size,\n",
    "                                num_epochs,\n",
    "                                loss_func,\n",
    "                                weights=weights,\n",
    "                                do_apex=False,\n",
    "                                model_apexed=True,\n",
    "                                validate_dataset=validate_dataset,\n",
    "                                param_schedualer=None,\n",
    "                                weights_data=None,\n",
    "                                metric=None,\n",
    "                                return_model=True,\n",
    "                                num_workers=num_workers,\n",
    "                                sampler=sampling,\n",
    "                                pre_process = None,\n",
    "                                graph=gr,\n",
    "                                call_progress=sendmeemail)\n",
    "\n",
    "torch.save(model.state_dict(), models_dir+models_format.format(model_name,version,num_split))\n",
    "torch.save(best_model.state_dict(), models_dir+models_format.format(model_name,version+'_best',num_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name,version, num_split = 'Densenet161_3' , 'basic_classifier_wso2',2\n",
    "model = MyDenseNet(models.densenet161(pretrained=True),\n",
    "                   len(hemorrhage_types),\n",
    "                   num_channels=3,\n",
    "                   drop_out=0.2,\n",
    "                   wso=((40,80),(80,200),(600,2800)),\n",
    "                   strategy='none',\n",
    "                   dont_do_grad=[],\n",
    "                   extra_pool=4,\n",
    "                   pool_type='max',\n",
    "                   return_features=True\n",
    "                   )\n",
    "model.load_state_dict(torch.load(models_dir+models_format.format(model_name,version,num_split),map_location=torch.device(device)))\n",
    "_=model.to(device)\n",
    "transform=MyTransform(flip=True,zoom=0.05,rotate=15,out_size=512,shift=40)\n",
    "transform_val=MyTransform(out_size=512)\n",
    "indexes=np.arange(train_df.shape[0]).repeat(4)\n",
    "train_dataset=D.Subset(ImageDataset(train_df,transform=transform.random,base_path=train_images_dir,\n",
    "                          window_eq=False,equalize=False,rescale=True),indexes)\n",
    "pred,features = model_run(model,train_dataset,do_apex=True,batch_size=96,num_workers=14)\n",
    "\n",
    "pickle_file=open(outputs_dir+outputs_format.format(model_name,version,'features_train_tta',num_split),'wb')\n",
    "pickle.dump(features,pickle_file,protocol=4)\n",
    "pickle_file.close()\n",
    "\n",
    "pickle_file=open(outputs_dir+outputs_format.format(model_name,version,'predictions_train_tta',num_split),'wb')\n",
    "pickle.dump(pred,pickle_file,protocol=4)\n",
    "pickle_file.close()\n",
    "\n",
    "\n",
    "my_loss(pred[(idx_validate*4+np.arange(4)[:,None]).transpose(1,0)].mean(1),\n",
    "        torch.tensor(train_df[hemorrhage_types].values[idx_validate],dtype=torch.float),\n",
    "        torch.tensor([1.,1.,1.,1.,1.,2.]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name,version, num_split = 'Densenet161_3' , 'basic_classifier_wso2',2\n",
    "model = MyDenseNet(models.densenet161(pretrained=True),\n",
    "                   len(hemorrhage_types),\n",
    "                   num_channels=3,\n",
    "                   drop_out=0.2,\n",
    "                   wso=((40,80),(80,200),(600,2800)),\n",
    "                   strategy='none',\n",
    "                   dont_do_grad=[],\n",
    "                   extra_pool=4,\n",
    "                   pool_type='max',\n",
    "                   return_features=True\n",
    "                   )\n",
    "model.load_state_dict(torch.load(models_dir+models_format.format(model_name,version,num_split),map_location=torch.device(device)))\n",
    "_=model.to(device)\n",
    "transform=MyTransform(flip=True,zoom=0.05,rotate=15,out_size=512,shift=40)\n",
    "transform_val=MyTransform(out_size=512)\n",
    "indexes=np.arange(test_df.shape[0]).repeat(8)\n",
    "imagedataset_test=D.Subset(ImageDataset(test_df,transform=transform.random,base_path=test_images_dir,\n",
    "                              window_eq=False,equalize=False,rescale=True),indexes)\n",
    "pred,features = model_run(model,imagedataset_test,do_apex=True,batch_size=96,num_workers=18)\n",
    "pickle_file=open(outputs_dir+outputs_format.format(model_name,version,'features_test',num_split),'wb')\n",
    "pickle.dump(features,pickle_file,protocol=4)\n",
    "pickle_file.close()\n",
    "pickle_file=open(outputs_dir+outputs_format.format(model_name,version,'predictions_test',num_split),'wb')\n",
    "pickle.dump(pred,pickle_file,protocol=4)\n",
    "pickle_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred.shape[0]/8\n",
    "pred[(np.arange(pred.shape[0]).reshape(pred.shape[0]//8,8))].mean(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred[(np.arange(pred.shape[0]).reshape()).transpose(1,0)].mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df=get_submission(test_df,pred[(np.arange(pred.shape[0]).reshape(pred.shape[0]//8,8))].mean(1))\n",
    "submission_df.head(12)\n",
    "submission_df.shape\n",
    "sub_num=24\n",
    "submission_df.to_csv('/media/hd/notebooks/data/RSNA/submissions/submission{}.csv'.format(sub_num),\n",
    "                                                                  index=False, columns=['ID','Label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-f38b9348fd14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_neighbors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Densenet161_3'\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0;34m'basic_classifier'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mpickle_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs_dir\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0moutputs_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mversion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'features_train'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_split\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpickle_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'outputs_dir' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib nbagg\n",
    "num_neighbors=2\n",
    "model_name,version, num_split = 'Densenet161_3' , 'basic_classifier',2\n",
    "pickle_file=open(outputs_dir+outputs_format.format(model_name,version,'features_train',num_split),'rb')\n",
    "features=pickle.load(pickle_file)\n",
    "pickle_file.close()\n",
    "np.random.seed(SEED+num_split)\n",
    "torch.manual_seed(SEED+num_split)\n",
    "torch.cuda.manual_seed(SEED+num_split)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "klr=1\n",
    "batch_size=64\n",
    "num_workers=1\n",
    "num_epochs=1\n",
    "model0 = MyDenseNet(models.densenet161(pretrained=True),len(hemorrhage_types),num_channels=3)\n",
    "model0.load_state_dict(torch.load(models_dir+models_format.format(model_name,version,num_split),map_location=torch.device(device)))\n",
    "_=model0.to('cpu')\n",
    "model=NeighborsNet(len(hemorrhage_types),\n",
    "                   num_neighbors=num_neighbors,\n",
    "                   num_features=model0.classifier.in_features,\n",
    "                   intermidiate=128, \n",
    "                   dropout=0.5)\n",
    "\n",
    "\n",
    "version=version+'_neighbors{}'.format(num_neighbors)\n",
    "_=model.to(device)\n",
    "weights = torch.tensor([1.,1.,1.,1.,1.,2.],device=device)\n",
    "loss_func=my_loss\n",
    "param_optimizer = model.parameters()\n",
    "features_dataset=FeatursDataset(train_df,features,num_neighbors,'SeriesI','ImagePositionZ',hemorrhage_types)\n",
    "train_dataset=D.Subset(features_dataset,idx_train)\n",
    "validate_dataset=D.Subset(features_dataset,idx_validate)\n",
    "fig,ax = plt.subplots(figsize=(10,7))\n",
    "gr=loss_graph(fig,ax,num_epochs,len(train_dataset)//batch_size+1)\n",
    "#num_train_optimization_steps = num_epochs*(sample_ratio*len(train_dataset)//batch_size+int(len(train_dataset)%batch_size>0))\n",
    "#sched=WarmupExpCosineWithWarmupRestartsSchedule( t_total=num_train_optimization_steps, cycles=num_epochs,tau=0)\n",
    "optimizer = torch.optim.Adam(param_optimizer, lr=0.001)\n",
    "history,best_model= model_train(model,\n",
    "                                optimizer,\n",
    "                                train_dataset,\n",
    "                                batch_size,\n",
    "                                num_epochs,\n",
    "                                loss_func,\n",
    "                                weights=weights,\n",
    "                                do_apex=False,\n",
    "                                validate_dataset=validate_dataset,\n",
    "                                param_schedualer=None,\n",
    "                                weights_data=None,\n",
    "                                metric=None,\n",
    "                                return_model=True,\n",
    "                                num_workers=num_workers,\n",
    "                                sampler=None,\n",
    "                                graph=gr)\n",
    "torch.save(best_model.state_dict(), models_dir+models_format.format(model_name,version,num_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neighbors=2\n",
    "model_name,version, num_split = 'Densenet161_3' , 'basic_classifier',2\n",
    "pickle_file=open(outputs_dir+outputs_format.format(model_name,version,'features_test',num_split),'rb')\n",
    "features=pickle.load(pickle_file)\n",
    "pickle_file.close()\n",
    "model0 = MyDenseNet(models.densenet161(pretrained=True),len(hemorrhage_types),num_channels=3)\n",
    "_=model0.to('cpu')\n",
    "model=NeighborsNet(len(hemorrhage_types),num_neighbors=num_neighbors,\n",
    "                   classifier_layer=model0.classifier,intermidiate=128)\n",
    "version=version+'_neighbors{}'.format(num_neighbors)\n",
    "model.load_state_dict(torch.load(models_dir+models_format.format(model_name,version,num_split),map_location=torch.device(device)))\n",
    "test_dataset=FeatursDataset(test_df,features,num_neighbors,'SeriesI','ImagePositionZ')\n",
    "pred = model_run(model,test_dataset,do_apex=False,batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df=get_submission(test_df,pred)\n",
    "submission_df.head(12)\n",
    "submission_df.shape\n",
    "sub_num=19\n",
    "submission_df.to_csv('/media/hd/notebooks/data/RSNA/submissions/submission{}.csv'.format(sub_num),\n",
    "                                                                  index=False, columns=['ID','Label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_neighbors=2\n",
    "model_name,version, num_split = 'Densenet161_3' , 'basic_classifier',2\n",
    "pickle_file=open(outputs_dir+outputs_format.format(model_name,version,'features_test',num_split),'rb')\n",
    "features=pickle.load(pickle_file)\n",
    "pickle_file.close()\n",
    "model0 = MyDenseNet(models.densenet161(pretrained=True),len(hemorrhage_types),num_channels=3)\n",
    "_=model0.to('cpu')\n",
    "model=NeighborsNet(len(hemorrhage_types),num_neighbors=num_neighbors,\n",
    "                   classifier_layer=model0.classifier,intermidiate=128)\n",
    "version=version+'_neighbors{}'.format(num_neighbors)\n",
    "model.load_state_dict(torch.load(models_dir+models_format.format(model_name,version,num_split),map_location=torch.device(device)))\n",
    "test_dataset=FeatursDataset(test_df,features,num_neighbors,'SeriesI','ImagePositionZ')\n",
    "pred1 = model_run(model,test_dataset,do_apex=False,batch_size=512)\n",
    "\n",
    "num_neighbors=2\n",
    "model_name,version, num_split = 'Densenet161' , 'basic_more',1\n",
    "pickle_file=open(outputs_dir+outputs_format.format(model_name,version,'features_test',num_split),'rb')\n",
    "features=pickle.load(pickle_file)\n",
    "pickle_file.close()\n",
    "model0 = MyDenseNet(models.densenet161(pretrained=True),len(hemorrhage_types),num_channels=3)\n",
    "_=model0.to('cpu')\n",
    "model=NeighborsNet(len(hemorrhage_types),num_neighbors=num_neighbors,\n",
    "                   classifier_layer=model0.classifier,intermidiate=128)\n",
    "version=version+'_neighbors{}'.format(num_neighbors)\n",
    "model.load_state_dict(torch.load(models_dir+models_format.format(model_name,version,num_split),map_location=torch.device(device)))\n",
    "test_dataset=FeatursDataset(test_df,features,num_neighbors,'SeriesI','ImagePositionZ')\n",
    "pred2 = model_run(model,test_dataset,do_apex=False,batch_size=512)\n",
    "\n",
    "num_neighbors=2\n",
    "model_name,version, num_split = 'Densenet161' , 'basic_more',0\n",
    "pickle_file=open(outputs_dir+outputs_format.format(model_name,version,'features_test',num_split),'rb')\n",
    "features=pickle.load(pickle_file)\n",
    "pickle_file.close()\n",
    "model0 = MyDenseNet(models.densenet161(pretrained=True),len(hemorrhage_types),num_channels=3)\n",
    "_=model0.to('cpu')\n",
    "model=NeighborsNet(len(hemorrhage_types),num_neighbors=num_neighbors,\n",
    "                   classifier_layer=model0.classifier,intermidiate=128)\n",
    "version=version+'_neighbors{}in'.format(num_neighbors)\n",
    "model.load_state_dict(torch.load(models_dir+models_format.format(model_name,version,num_split),map_location=torch.device(device)))\n",
    "test_dataset=FeatursDataset(test_df,features,num_neighbors,'SeriesI','ImagePositionZ')\n",
    "pred3 = model_run(model,test_dataset,do_apex=False,batch_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=(pred1+(pred2+pred3)/2)/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df=get_submission(test_df,pred)\n",
    "submission_df.head(12)\n",
    "submission_df.shape\n",
    "sub_num=20\n",
    "submission_df.to_csv('/media/hd/notebooks/data/RSNA/submissions/submission{}.csv'.format(sub_num),\n",
    "                                                                  index=False, columns=['ID','Label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib nbagg\n",
    "num_neighbors=3\n",
    "model_name,version, num_split = 'Densenet161_3' , 'basic_classifier',2\n",
    "pickle_file=open(outputs_dir+outputs_format.format(model_name,version,'features_train',num_split),'rb')\n",
    "features=pickle.load(pickle_file)\n",
    "pickle_file.close()\n",
    "np.random.seed(SEED+num_split)\n",
    "torch.manual_seed(SEED+num_split)\n",
    "torch.cuda.manual_seed(SEED+num_split)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "klr=1\n",
    "batch_size=64\n",
    "num_workers=1\n",
    "num_epochs=1\n",
    "model0 = MyDenseNet(models.densenet161(pretrained=True),len(hemorrhage_types),num_channels=3)\n",
    "model0.load_state_dict(torch.load(models_dir+models_format.format(model_name,version,num_split),map_location=torch.device(device)))\n",
    "_=model0.to('cpu')\n",
    "model=NeighborsNet(len(hemorrhage_types),\n",
    "                   num_neighbors=num_neighbors,\n",
    "                   num_features=model0.classifier.in_features,\n",
    "                   intermidiate=128, \n",
    "                   dropout=0.5)\n",
    "\n",
    "\n",
    "version=version+'_neighbors{}cor'.format(num_neighbors)\n",
    "_=model.to(device)\n",
    "weights = torch.tensor([1.,1.,1.,1.,1.,2.],device=device)\n",
    "loss_func=my_loss\n",
    "param_optimizer = model.parameters()\n",
    "features_dataset=FeatursDatasetCor(train_df,features,num_neighbors,'SeriesI',hemorrhage_types)\n",
    "train_dataset=D.Subset(features_dataset,idx_train)\n",
    "validate_dataset=D.Subset(features_dataset,idx_validate)\n",
    "fig,ax = plt.subplots(figsize=(10,7))\n",
    "gr=loss_graph(fig,ax,num_epochs,len(train_dataset)//batch_size+1)\n",
    "#num_train_optimization_steps = num_epochs*(sample_ratio*len(train_dataset)//batch_size+int(len(train_dataset)%batch_size>0))\n",
    "#sched=WarmupExpCosineWithWarmupRestartsSchedule( t_total=num_train_optimization_steps, cycles=num_epochs,tau=0)\n",
    "optimizer = torch.optim.Adam(param_optimizer, lr=0.001)\n",
    "history,best_model= model_train(model,\n",
    "                                optimizer,\n",
    "                                train_dataset,\n",
    "                                batch_size,\n",
    "                                num_epochs,\n",
    "                                loss_func,\n",
    "                                weights=weights,\n",
    "                                do_apex=False,\n",
    "                                validate_dataset=validate_dataset,\n",
    "                                param_schedualer=None,\n",
    "                                weights_data=None,\n",
    "                                metric=None,\n",
    "                                return_model=True,\n",
    "                                num_workers=num_workers,\n",
    "                                sampler=None,\n",
    "                                graph=gr)\n",
    "torch.save(best_model.state_dict(), models_dir+models_format.format(model_name,version,num_split))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
