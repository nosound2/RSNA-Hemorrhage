# RSNA Intracranial Hemorrhage Detection challenge

Kaggle competition [RSNA Intracranial Hemorrhage Detection](https://www.kaggle.com/c/rsna-intracranial-hemorrhage-detection/overview)

Team "Mind Blowers":

* [Yuval Reina](https://www.kaggle.com/yuval6967)
* [Zahar Chikishev](https://www.kaggle.com/zaharch)

The solution consists of the following components, run consecutively

* Prepare metadata
* Training features generating neural networks
* Training shallow neural networks based on the features and metadata
   * By Yuval
   * By Zahar
* Ensembling


## Prepare metadata

`notebooks/DICOM_metadata_to_CSV.ipynb` - traverses DICOM files and extracts metadata into a dataframe. Produces three dataframes, one for the train images and two for the stage 1&2 test images.

`notebooks/Metadata.ipynb` - gets the output of the previous notebook and post-processes the collected metadata. Prepares metadata features for training, will be used as an input to Zahar's shallow NNs. Specifically, outputs two dataframes saved in `train_md.csv` and `test_md.csv` with the metadata features. 

The last section of the notebook also prepares weights for the training images. The weights are selected to simulate the distribution to that we encounter in the test images. 

`Production/Prepare.ipynb`is used to prepare the train.csv and test.csv for the base mosels and yuval's Sallow NN
## Features generating neural networks
The features generating training notebooks are :
* `Production/Densenet161-folds.ipynb`
* `Production/Densenet169-folds.ipynb`
* `Production/Densenet201-folds.ipynb`
* `Production/se_resnet101.ipynb`
* `Production/se_resnext101_32x4d + prepare densenet features.ipynb`
* `se_resnext101_32x4d-new_folds.ipynb` - 5 folds for se-resnext101 and se-resnet101 and focal loss retraining

These notebooks also run the models on the train and test data to extract the features (4 augmented samples for train and 8 for test)

The MD5SUM of the current trained models before fine tunning on stage1 test:

* 23261b8e24d70d538695685ef18abf74  model_.zip   (all non-focal loss models)
* 1ca9069e66ce993dff762534b96da551  model_se_resnext101_32x4d_focal.zip (se_resnext101_32x4d focal loss)
* 598f2cf2d8d4c3c29f1da4ee7193ebba  model_se_resnet101_version_new_splits_focal_split_.zip (se_resnet101 focal loss)

## Shallow NN by Yuval
These networks are taking features for a full series. 
* `Post Full Head Models Train.ipynb`
* `Post Full Head Models Train - new split.ipynb`
* `Post Full Head Models Train-focal features.ipynb` shallow nets for focal loss output very drafty)

The network that was used at the end is ResModelPoll and all use weighted BCE as loss

The inference is finally done using  `prepare_ensembling.ipynb` which use the features from the base models and the Shallow NN to create predictions that are then ensembled by Zahar.

## Shallow NN by Zahar

`notebooks/Training.ipynb` - trains a shallow neural network based on the generated features and the metadata. 

2 different models are trained on 3 folds. The difference between the models is in input features. The input features for 4 different models are generated by the following backbones (see section above "Features generating neural networks")

* Densenet161
* Se-Resnext101-32x4d

3 models are trained on 5 folds:

* Se-Resnext101-32x4d
* Se-resnet101
* Se-resnet101 with focal loss fine-tuning

All of the above models are fine-tuned after a regular training step. The fine tuning is different in that it uses weighted random sampling, with weights defined by `notebooks/Metadata.ipynb`. 

## Ensembling

`notebooks/Ensembling.ipynb` - ensembles the results from all shallow NNs into final predictions and prepares the final submissions.

The two final submissions are obtained by running this notebook and the difference is the following:

* **Safe submission** ensembles regular Zahar and Yuval's models.
* **Risky submission** ensembles weighted Zahar's models and regular Yuval's models, while the ensembling uses by-sample weighted log-loss with the same weights as defined before.
